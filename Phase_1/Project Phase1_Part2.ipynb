{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5d94be5efc67499992c6379478ddb138": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_33a0f342383e499ba1f13d57257e8154",
              "IPY_MODEL_a538498abe1e4623a1d6751c13e98d54",
              "IPY_MODEL_b188430c03f543449ab2326c390d6524"
            ],
            "layout": "IPY_MODEL_21c02e06791c4253ab50015f87078ebc"
          }
        },
        "33a0f342383e499ba1f13d57257e8154": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ffecabd5bec24ab6a55e9eeb227f5a37",
            "placeholder": "​",
            "style": "IPY_MODEL_a8b1dc3cc965436499399cba3fc2aa98",
            "value": "Epoch: 100%"
          }
        },
        "a538498abe1e4623a1d6751c13e98d54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3cedcc4dae4d435989fe6d85a32bfd7f",
            "max": 20,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f001d6a8d0c84ebb91717442d260f326",
            "value": 20
          }
        },
        "b188430c03f543449ab2326c390d6524": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ea9daa0d0b948ee8abffb1d050644a5",
            "placeholder": "​",
            "style": "IPY_MODEL_3ed6c1199c264da99bd78959f01dc380",
            "value": " 20/20 [04:24&lt;00:00, 13.73s/epoch]"
          }
        },
        "21c02e06791c4253ab50015f87078ebc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ffecabd5bec24ab6a55e9eeb227f5a37": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8b1dc3cc965436499399cba3fc2aa98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3cedcc4dae4d435989fe6d85a32bfd7f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f001d6a8d0c84ebb91717442d260f326": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0ea9daa0d0b948ee8abffb1d050644a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ed6c1199c264da99bd78959f01dc380": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Private Training"
      ],
      "metadata": {
        "id": "MvFE0ZzCZbAa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q4"
      ],
      "metadata": {
        "id": "do0GRfbjhWO2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import the Class"
      ],
      "metadata": {
        "id": "XKeTSZlHdOZs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import model\n",
        "from model import CIFAR10Classifier"
      ],
      "metadata": {
        "id": "Xxg2iVAEYdOr"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Import Required Libraries"
      ],
      "metadata": {
        "id": "2hgJzfcUdRD9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, random_split"
      ],
      "metadata": {
        "id": "vmwrzPtnZKB0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loading and Transformation"
      ],
      "metadata": {
        "id": "DQ-gUnWTdV-2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform and normalize data\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)"
      ],
      "metadata": {
        "id": "V0FgN5tpZLnF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e175c360-335c-4d77-c8b9-50fa701230dd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split the Dataset"
      ],
      "metadata": {
        "id": "Z3R6s46_daV_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 512\n",
        "# Split the trainset into 80% train and 20% validation\n",
        "train_size = int(0.8 * len(trainset))\n",
        "val_size = len(trainset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(trainset, [train_size, val_size])\n",
        "\n",
        "train_loader_baseline = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader_baseline = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader_baseline = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False)\n"
      ],
      "metadata": {
        "id": "7Kp10br7ZNgq"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_size);"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rzaHjwmfARo",
        "outputId": "fd4fcba5-cd7f-48b1-e815-a2e6c156de7b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "40000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize Model, Loss Function, and Optimizer"
      ],
      "metadata": {
        "id": "LdDDwdsodfZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = CIFAR10Classifier().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
      ],
      "metadata": {
        "id": "ouf_p6-yZQ5l"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the Model"
      ],
      "metadata": {
        "id": "TVlWaBYKdknQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(20):  # number of epochs\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_loader_baseline, 0):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward + backward + optimize\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Print training progress\n",
        "        if i % 100 == 99:\n",
        "            print(f\"[Epoch {epoch + 1}, Batch {i + 1}] Loss: {running_loss / 100:.4f}\")\n",
        "            running_loss = 0.0\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in val_loader_baseline:\n",
        "            images, labels = data\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    val_loss /= len(val_loader_baseline)\n",
        "    val_accuracy = 100 * correct / total\n",
        "    print(f\"[Epoch {epoch + 1}] Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "print('Finished Training')\n"
      ],
      "metadata": {
        "id": "WoaY1tq3ZRqx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a3e61b2-ce77-40e2-966c-aa7bc13cb162"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:1374: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1] Validation Loss: 1.5112, Validation Accuracy: 47.60%\n",
            "[Epoch 2] Validation Loss: 1.3566, Validation Accuracy: 52.73%\n",
            "[Epoch 3] Validation Loss: 1.2634, Validation Accuracy: 56.23%\n",
            "[Epoch 4] Validation Loss: 1.1845, Validation Accuracy: 59.53%\n",
            "[Epoch 5] Validation Loss: 1.1355, Validation Accuracy: 60.68%\n",
            "[Epoch 6] Validation Loss: 1.1072, Validation Accuracy: 62.14%\n",
            "[Epoch 7] Validation Loss: 1.0703, Validation Accuracy: 63.18%\n",
            "[Epoch 8] Validation Loss: 1.0358, Validation Accuracy: 64.05%\n",
            "[Epoch 9] Validation Loss: 1.0188, Validation Accuracy: 64.59%\n",
            "[Epoch 10] Validation Loss: 1.0018, Validation Accuracy: 65.31%\n",
            "[Epoch 11] Validation Loss: 0.9923, Validation Accuracy: 65.81%\n",
            "[Epoch 12] Validation Loss: 0.9804, Validation Accuracy: 66.01%\n",
            "[Epoch 13] Validation Loss: 0.9781, Validation Accuracy: 65.45%\n",
            "[Epoch 14] Validation Loss: 0.9701, Validation Accuracy: 66.21%\n",
            "[Epoch 15] Validation Loss: 0.9716, Validation Accuracy: 66.36%\n",
            "[Epoch 16] Validation Loss: 0.9686, Validation Accuracy: 66.68%\n",
            "[Epoch 17] Validation Loss: 0.9565, Validation Accuracy: 66.66%\n",
            "[Epoch 18] Validation Loss: 0.9496, Validation Accuracy: 66.86%\n",
            "[Epoch 19] Validation Loss: 0.9530, Validation Accuracy: 67.05%\n",
            "[Epoch 20] Validation Loss: 0.9458, Validation Accuracy: 67.25%\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save the Trained Model"
      ],
      "metadata": {
        "id": "VgB-qcXAdoij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = './baseline_model.pth'\n",
        "torch.save(model.state_dict(), PATH)\n"
      ],
      "metadata": {
        "id": "R4vb0FooZVYA"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Evaluate the Model"
      ],
      "metadata": {
        "id": "m_Cl2dC4dqqv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in test_loader_baseline:\n",
        "        images, labels = data\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Accuracy of the network on the 10000 test images: {100 * correct / total:.2f}%\")\n"
      ],
      "metadata": {
        "id": "fnVo9Y2AZYLM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "371f1b66-64cd-43ec-cd58-a0b9521e0dd3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 10000 test images: 67.05%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q5"
      ],
      "metadata": {
        "id": "Q64z_kO1hUZq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using opacus (Implemented Code)"
      ],
      "metadata": {
        "id": "6NXtTbiD2EBO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://opacus.ai/tutorials/building_image_classifier"
      ],
      "metadata": {
        "id": "Doh4jP8UBuOJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opacus\n"
      ],
      "metadata": {
        "id": "zs_kFxYpIcSg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "ee18d44f-8263-4f7f-8e68-44f4313121f0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opacus\n",
            "  Downloading opacus-1.4.1-py3-none-any.whl (226 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/226.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m225.3/226.7 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.7/226.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.10/dist-packages (from opacus) (1.25.2)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.10/dist-packages (from opacus) (2.3.0+cu121)\n",
            "Requirement already satisfied: scipy>=1.2 in /usr/local/lib/python3.10/dist-packages (from opacus) (1.11.4)\n",
            "Requirement already satisfied: opt-einsum>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from opacus) (3.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=2.0->opacus)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=2.0->opacus)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=2.0->opacus)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=2.0->opacus)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=2.0->opacus)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=2.0->opacus)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=2.0->opacus)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=2.0->opacus)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=2.0->opacus)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=2.0->opacus)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=2.0->opacus)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0->opacus)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0->opacus) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.0->opacus) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, opacus\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 opacus-1.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyper-parameters"
      ],
      "metadata": {
        "id": "BmafYXL4B4X7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.simplefilter(\"ignore\")\n",
        "\n",
        "MAX_GRAD_NORM = 10\n",
        "EPSILON = 50.0\n",
        "DELTA = 1e-5\n",
        "EPOCHS = 20\n",
        "\n",
        "LR = 1e-3\n",
        "\n",
        "BATCH_SIZE = 512\n",
        "MAX_PHYSICAL_BATCH_SIZE = 128"
      ],
      "metadata": {
        "id": "veoo9lZ6nFfC"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data"
      ],
      "metadata": {
        "id": "tlRESoG_B9PD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "# Transform and normalize data\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "])\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Split the trainset into 80% train and 20% validation\n",
        "train_size = int(0.8 * len(trainset))\n",
        "val_size = len(trainset) - train_size\n",
        "train_dataset, val_dataset = random_split(trainset, [train_size, val_size])\n",
        "\n",
        "train_loader_modified = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader_modified  = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader_modified  = DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kboMYTz3nJo6",
        "outputId": "6aaa465b-29a9-4ed1-bacc-2d0d73ad8648"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model"
      ],
      "metadata": {
        "id": "o7CnKq7jCCfE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import model\n",
        "from model import CIFAR10Classifier\n",
        "\n",
        "model = CIFAR10Classifier()"
      ],
      "metadata": {
        "id": "VVimpQe5nTrJ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from opacus.validators import ModuleValidator\n",
        "\n",
        "model.train()\n",
        "\n",
        "errors = ModuleValidator.validate(model, strict=False)\n",
        "errors[-5:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXcpGR19nax2",
        "outputId": "a7bfbdd1-d543-481a-c0de-dfbaa57011c7"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = ModuleValidator.fix(model)\n",
        "ModuleValidator.validate(model, strict=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwdISkM9nisX",
        "outputId": "be105674-a411-4cd1-9026-ab6950ae35da"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") #defining device\n",
        "\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "r0NZh9H1nt7Z"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "#optimizer = optim.RMSprop(model.parameters(), lr=LR)"
      ],
      "metadata": {
        "id": "I5l7Jh1Gnv9S"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare for Training"
      ],
      "metadata": {
        "id": "ex6LYDoWCXST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(preds, labels):\n",
        "    return (preds == labels).mean()"
      ],
      "metadata": {
        "id": "dzaryz37nxes"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from opacus import PrivacyEngine\n",
        "\n",
        "privacy_engine = PrivacyEngine()\n",
        "\n",
        "model, optimizer, train_loader_modified = privacy_engine.make_private_with_epsilon(\n",
        "    module=model,\n",
        "    optimizer=optimizer,\n",
        "    data_loader=train_loader_modified,\n",
        "    epochs=EPOCHS,\n",
        "    target_epsilon=EPSILON,\n",
        "    target_delta=DELTA,\n",
        "    max_grad_norm=MAX_GRAD_NORM,\n",
        ")\n",
        "\n",
        "print(f\"Using sigma={optimizer.noise_multiplier} and C={MAX_GRAD_NORM}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NuzpPyPnzVM",
        "outputId": "78e7c675-80e4-409d-edcf-f401809729e9"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using sigma=0.37944793701171875 and C=10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from opacus.utils.batch_memory_manager import BatchMemoryManager\n",
        "\n",
        "\n",
        "def train(model, train_loader, optimizer, epoch, device):\n",
        "    model.train()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    losses = []\n",
        "    top1_acc = []\n",
        "\n",
        "    with BatchMemoryManager(\n",
        "        data_loader=train_loader,\n",
        "        max_physical_batch_size=MAX_PHYSICAL_BATCH_SIZE,\n",
        "        optimizer=optimizer\n",
        "    ) as memory_safe_data_loader:\n",
        "\n",
        "        for i, (images, target) in enumerate(memory_safe_data_loader):\n",
        "            optimizer.zero_grad()\n",
        "            images = images.to(device)\n",
        "            target = target.to(device)\n",
        "\n",
        "            # compute output\n",
        "            output = model(images)\n",
        "            loss = criterion(output, target)\n",
        "\n",
        "            preds = np.argmax(output.detach().cpu().numpy(), axis=1)\n",
        "            labels = target.detach().cpu().numpy()\n",
        "\n",
        "            # measure accuracy and record loss\n",
        "            acc = accuracy(preds, labels)\n",
        "\n",
        "            losses.append(loss.item())\n",
        "            top1_acc.append(acc)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if (i+1) % 200 == 0:\n",
        "                epsilon = privacy_engine.get_epsilon(DELTA)\n",
        "                print(\n",
        "                    f\"\\tTrain Epoch: {epoch} \\t\"\n",
        "                    f\"Loss: {np.mean(losses):.6f} \"\n",
        "                    f\"Acc@1: {np.mean(top1_acc) * 100:.6f} \"\n",
        "                    f\"(ε = {epsilon:.2f}, δ = {DELTA})\"\n",
        "                )"
      ],
      "metadata": {
        "id": "7P5Ck1p-n2SC"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, test_loader, device):\n",
        "    model.eval()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    losses = []\n",
        "    top1_acc = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, target in test_loader:\n",
        "            images = images.to(device)\n",
        "            target = target.to(device)\n",
        "\n",
        "            output = model(images)\n",
        "            loss = criterion(output, target)\n",
        "            preds = np.argmax(output.detach().cpu().numpy(), axis=1)\n",
        "            labels = target.detach().cpu().numpy()\n",
        "            acc = accuracy(preds, labels)\n",
        "\n",
        "            losses.append(loss.item())\n",
        "            top1_acc.append(acc)\n",
        "\n",
        "    top1_avg = np.mean(top1_acc)\n",
        "\n",
        "    print(\n",
        "        f\"\\tTest set:\"\n",
        "        f\"Loss: {np.mean(losses):.6f} \"\n",
        "        f\"Acc: {top1_avg * 100:.6f} \"\n",
        "    )\n",
        "    return np.mean(top1_acc)"
      ],
      "metadata": {
        "id": "m_IOXqdjn6pJ"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validation(model, val_loader, device):\n",
        "    model.eval()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    losses = []\n",
        "    top1_acc = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, target in val_loader:\n",
        "            images = images.to(device)\n",
        "            target = target.to(device)\n",
        "\n",
        "            output = model(images)\n",
        "            loss = criterion(output, target)\n",
        "            preds = np.argmax(output.detach().cpu().numpy(), axis=1)\n",
        "            labels = target.detach().cpu().numpy()\n",
        "            acc = accuracy(preds, labels)\n",
        "\n",
        "            losses.append(loss.item())\n",
        "            top1_acc.append(acc)\n",
        "\n",
        "    top1_avg = np.mean(top1_acc)\n",
        "\n",
        "    print(\n",
        "        f\"\\tValidation set:\"\n",
        "        f\"Loss: {np.mean(losses):.6f} \"\n",
        "        f\"Acc: {top1_avg * 100:.6f} \"\n",
        "    )\n",
        "    return np.mean(top1_acc)"
      ],
      "metadata": {
        "id": "Y4zMHtq8TH9r"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train The Network"
      ],
      "metadata": {
        "id": "0rn3JJBUChVm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.notebook import tqdm\n",
        "\n",
        "for epoch in tqdm(range(EPOCHS), desc=\"Epoch\", unit=\"epoch\"):\n",
        "    train(model, train_loader_modified, optimizer, epoch + 1, device)"
      ],
      "metadata": {
        "id": "Wbv0Yf_1n9Ga",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406,
          "referenced_widgets": [
            "5d94be5efc67499992c6379478ddb138",
            "33a0f342383e499ba1f13d57257e8154",
            "a538498abe1e4623a1d6751c13e98d54",
            "b188430c03f543449ab2326c390d6524",
            "21c02e06791c4253ab50015f87078ebc",
            "ffecabd5bec24ab6a55e9eeb227f5a37",
            "a8b1dc3cc965436499399cba3fc2aa98",
            "3cedcc4dae4d435989fe6d85a32bfd7f",
            "f001d6a8d0c84ebb91717442d260f326",
            "0ea9daa0d0b948ee8abffb1d050644a5",
            "3ed6c1199c264da99bd78959f01dc380"
          ]
        },
        "outputId": "5cf308df-3286-4f23-d9ae-a2bf03abd33f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch:   0%|          | 0/20 [00:00<?, ?epoch/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5d94be5efc67499992c6379478ddb138"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Epoch: 1 \tLoss: 2.178985 Acc@1: 19.898451 (ε = 12.89, δ = 1e-05)\n",
            "\tTrain Epoch: 2 \tLoss: 1.973132 Acc@1: 30.473708 (ε = 17.15, δ = 1e-05)\n",
            "\tTrain Epoch: 3 \tLoss: 1.899869 Acc@1: 33.989458 (ε = 20.18, δ = 1e-05)\n",
            "\tTrain Epoch: 4 \tLoss: 1.846271 Acc@1: 36.255564 (ε = 22.80, δ = 1e-05)\n",
            "\tTrain Epoch: 5 \tLoss: 1.834799 Acc@1: 37.975965 (ε = 25.05, δ = 1e-05)\n",
            "\tTrain Epoch: 6 \tLoss: 1.807479 Acc@1: 39.592424 (ε = 27.20, δ = 1e-05)\n",
            "\tTrain Epoch: 7 \tLoss: 1.784235 Acc@1: 40.696626 (ε = 29.18, δ = 1e-05)\n",
            "\tTrain Epoch: 8 \tLoss: 1.755678 Acc@1: 41.537582 (ε = 31.05, δ = 1e-05)\n",
            "\tTrain Epoch: 9 \tLoss: 1.761812 Acc@1: 41.750261 (ε = 32.84, δ = 1e-05)\n",
            "\tTrain Epoch: 10 \tLoss: 1.736863 Acc@1: 43.359000 (ε = 34.56, δ = 1e-05)\n",
            "\tTrain Epoch: 11 \tLoss: 1.716502 Acc@1: 43.688296 (ε = 36.22, δ = 1e-05)\n",
            "\tTrain Epoch: 12 \tLoss: 1.704348 Acc@1: 43.960949 (ε = 37.86, δ = 1e-05)\n",
            "\tTrain Epoch: 13 \tLoss: 1.711286 Acc@1: 44.481772 (ε = 39.38, δ = 1e-05)\n",
            "\tTrain Epoch: 14 \tLoss: 1.699056 Acc@1: 44.702703 (ε = 40.91, δ = 1e-05)\n",
            "\tTrain Epoch: 15 \tLoss: 1.697359 Acc@1: 45.201041 (ε = 42.39, δ = 1e-05)\n",
            "\tTrain Epoch: 16 \tLoss: 1.676325 Acc@1: 45.893734 (ε = 43.87, δ = 1e-05)\n",
            "\tTrain Epoch: 17 \tLoss: 1.685303 Acc@1: 45.981080 (ε = 45.28, δ = 1e-05)\n",
            "\tTrain Epoch: 18 \tLoss: 1.668644 Acc@1: 46.391319 (ε = 46.70, δ = 1e-05)\n",
            "\tTrain Epoch: 19 \tLoss: 1.666370 Acc@1: 46.216335 (ε = 48.05, δ = 1e-05)\n",
            "\tTrain Epoch: 20 \tLoss: 1.656989 Acc@1: 46.953617 (ε = 49.42, δ = 1e-05)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saving the model"
      ],
      "metadata": {
        "id": "cyN4HlxDCmiW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = './modified_model.pth'\n",
        "torch.save(model.state_dict(), PATH)"
      ],
      "metadata": {
        "id": "-xocQD55UT-m"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test the network"
      ],
      "metadata": {
        "id": "cdF5IV8KCpK2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "validation(model, val_loader_modified, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cin0EVrNTKRl",
        "outputId": "c29a4e01-e022-4061-ceb1-05573bf380a6"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tValidation set:Loss: 1.336747 Acc: 54.375574 \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5437557444852941"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top1_acc = test(model, test_loader_modified, device)"
      ],
      "metadata": {
        "id": "mLAeNlrKoAOv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8efbd910-b33a-4371-b29c-574b242a8aaf"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTest set:Loss: 1.340004 Acc: 53.816636 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q6 & Q7"
      ],
      "metadata": {
        "id": "NyoiSB_yYOsr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using MLP model for Attacker Model"
      ],
      "metadata": {
        "id": "0dIIHR7cYOKV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MLP with 4 Hidden Layers  :\n",
        "\n",
        "> Learning Rate (in training shadow models) : 0.001\n",
        "\n",
        "> Learning Rate (in training attacking models) : 0.001\n",
        "\n",
        "> Epochs (in training shadow models) : 10\n",
        "\n",
        "> Epochs (in training attacking models) : 10\n",
        "\n",
        "> Batch Size (in training shadow models) : 64\n",
        "\n",
        "> Batch Size (in training attacking models) : 64\n",
        "\n",
        "> num_shadow_models = 100 ---> The Article\n",
        "\n",
        "> Optimizer (in training attacking models ) : Adam"
      ],
      "metadata": {
        "id": "FUpnXK9wZCEV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Subset, ConcatDataset\n",
        "from model import CIFAR10Classifier\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class AttackModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AttackModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(10, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 32)\n",
        "        self.fc4 = nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = torch.sigmoid(self.fc4(x))\n",
        "        return x\n",
        "\n",
        "class MembershipInferenceAttackNoPrivacy:\n",
        "    def __init__(self, shadow_model_class, attack_model_class, device='cpu'):\n",
        "        self.shadow_model_class = shadow_model_class\n",
        "        self.attack_model_class = attack_model_class\n",
        "        self.device = device\n",
        "        self.attack_models = {}\n",
        "\n",
        "    def train_shadow_models(self, seen_loaders, num_epochs=10, lr=1e-3):\n",
        "        self.shadow_models = [self.shadow_model_class().to(self.device) for _ in range(len(seen_loaders))]\n",
        "\n",
        "        for i, (shadow_model, seen_loader) in enumerate(zip(self.shadow_models, seen_loaders)):\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "            optimizer = optim.Adam(shadow_model.parameters(), lr=lr)\n",
        "            self._train_model(shadow_model, seen_loader, criterion, optimizer, num_epochs)\n",
        "            print(f'Shadow model {i+1} trained.')\n",
        "\n",
        "    def _train_model(self, model, dataloader, criterion, optimizer, num_epochs):\n",
        "        model.train()\n",
        "        for epoch in range(num_epochs):\n",
        "            running_loss = 0.0\n",
        "            for inputs, labels in dataloader:\n",
        "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                running_loss += loss.item()\n",
        "            print(f'Epoch {epoch+1}, Loss: {running_loss/len(dataloader):.6f}')\n",
        "\n",
        "    def collect_outputs(self, seen_loaders, unseen_loaders):\n",
        "        self.attack_data = []\n",
        "        self.attack_labels = []\n",
        "\n",
        "        for shadow_model, seen_loader, unseen_loader in zip(self.shadow_models, seen_loaders, unseen_loaders):\n",
        "            self._collect_shadow_model_outputs(shadow_model, seen_loader, label=1)  # in\n",
        "            self._collect_shadow_model_outputs(shadow_model, unseen_loader, label=0)  # out\n",
        "\n",
        "        self.attack_data = torch.cat(self.attack_data).to(self.device)\n",
        "        self.attack_labels = torch.cat(self.attack_labels).to(self.device)\n",
        "\n",
        "    def _collect_shadow_model_outputs(self, model, dataloader, label):\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in dataloader:\n",
        "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
        "                outputs = model(inputs)\n",
        "                probabilities = F.softmax(outputs, dim=1)\n",
        "                self.attack_data.append(torch.cat([probabilities, labels.unsqueeze(1).float()], dim=1))\n",
        "                self.attack_labels.append(torch.full((outputs.size(0),), label, dtype=torch.float).to(self.device))\n",
        "\n",
        "    def train_attack_models(self, num_epochs=10, lr=0.001):\n",
        "        attack_dataset = torch.utils.data.TensorDataset(self.attack_data, self.attack_labels)\n",
        "        for class_label in range(10):\n",
        "            class_indices = (self.attack_data[:, -1] == class_label).nonzero().squeeze()\n",
        "            class_data = self.attack_data[class_indices][:, :-1]\n",
        "            class_labels = self.attack_labels[class_indices].view(-1, 1)\n",
        "\n",
        "\n",
        "            attack_dataset = torch.utils.data.TensorDataset(class_data, class_labels)\n",
        "            attack_loader = DataLoader(attack_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "            attack_model = self.attack_model_class().to(self.device)\n",
        "            criterion = nn.BCELoss()\n",
        "            optimizer = optim.Adam(attack_model.parameters(), lr=lr)\n",
        "\n",
        "            self._train_model(attack_model, attack_loader, criterion, optimizer, num_epochs)\n",
        "            self.attack_models[class_label] = attack_model\n",
        "\n",
        "            print(f'Attack model for class {class_label} trained.')\n",
        "\n",
        "    def save_attack_models(self, path):\n",
        "        for class_label, model in self.attack_models.items():\n",
        "            torch.save(model.state_dict(), f'{path}_class_{class_label}.pth')\n",
        "            print(f'Attack model for class {class_label} saved to {path}_class_{class_label}.pth')\n",
        "\n",
        "    def load_attack_models(self, path):\n",
        "        for class_label in range(10):\n",
        "            model = self.attack_model_class().to(self.device)\n",
        "            model.load_state_dict(torch.load(f'{path}_class_{class_label}.pth', map_location=self.device))\n",
        "            self.attack_models[class_label] = model\n",
        "            print(f'Attack model for class {class_label} loaded from {path}_class_{class_label}.pth')\n",
        "\n",
        "    def infer_membership(self, model, seen_loader, unseen_loader , seen_outputs , unseen_outputs , labels):\n",
        "\n",
        "        model_outputs = torch.cat([seen_outputs, unseen_outputs]).to(self.device)\n",
        "\n",
        "        labels = labels.to(self.device)\n",
        "\n",
        "        memberships = []\n",
        "        for output, label in zip(model_outputs, labels):\n",
        "            class_label = label.item()\n",
        "            attack_model = self.attack_models[class_label]\n",
        "            membership_pred = attack_model(output.unsqueeze(0)).item()\n",
        "            memberships.append(membership_pred)\n",
        "        return torch.tensor(memberships, device=self.device)\n",
        "\n",
        "    def evaluate_attack_model(self, seen_loader, unseen_loader, target_model):\n",
        "        seen_outputs , lables_seen= self._get_model_outputs(target_model, seen_loader)\n",
        "        unseen_outputs , labels_unseen = self._get_model_outputs(target_model, unseen_loader)\n",
        "\n",
        "        attack_data = torch.cat([seen_outputs, unseen_outputs]).to(self.device)\n",
        "        attack_labels = torch.cat([torch.ones(len(seen_outputs)), torch.zeros(len(unseen_outputs))]).to(self.device)\n",
        "\n",
        "        labels = torch.cat([lables_seen, labels_unseen]).to(self.device)\n",
        "\n",
        "        memberships = self.infer_membership(target_model, seen_loader, unseen_loader , seen_outputs , unseen_outputs , labels)\n",
        "        membership_preds = (memberships > 0.5).float()\n",
        "        accuracy = (membership_preds == attack_labels).float().mean().item()\n",
        "        return accuracy\n",
        "\n",
        "    def _get_model_outputs(self, model, dataloader):\n",
        "        model.eval()\n",
        "        outputs_list = []\n",
        "        labels_list = []\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in dataloader:\n",
        "                inputs = inputs.to(self.device)\n",
        "                outputs = model(inputs)\n",
        "                probabilities = F.softmax(outputs, dim=1)\n",
        "                outputs_list.append(probabilities)\n",
        "                labels_list.append(labels) # Convert labels to list for easy concatenation\n",
        "        return torch.cat(outputs_list), torch.cat(labels_list)"
      ],
      "metadata": {
        "id": "4bfh5WeRYWrZ"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "WV1CN1uUY7p7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
        "train_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_size = int(0.8 * len(train_set))\n",
        "remaining_size = len(train_set) - train_size\n",
        "train_subset, remaining_subset = torch.utils.data.random_split(train_set, [train_size, remaining_size])\n",
        "\n",
        "\n",
        "num_shadow_models = 100\n",
        "seen_size_per_model = train_size // num_shadow_models\n",
        "seen_loaders = []\n",
        "\n",
        "for i in range(num_shadow_models):\n",
        "    start_idx = i * seen_size_per_model\n",
        "    end_idx = (i + 1) * seen_size_per_model\n",
        "    seen_indices = torch.arange(start_idx, end_idx)\n",
        "    seen_train_set = Subset(train_subset, seen_indices)\n",
        "    seen_loader = DataLoader(seen_train_set, batch_size=64, shuffle=True)\n",
        "    seen_loaders.append(seen_loader)\n",
        "\n",
        "unseen_dataset = ConcatDataset([remaining_subset, test_set])\n",
        "unseen_size_per_model = len(unseen_dataset) // num_shadow_models\n",
        "unseen_loaders = []\n",
        "\n",
        "for i in range(num_shadow_models):\n",
        "    start_idx = i * unseen_size_per_model\n",
        "    end_idx = (i + 1) * unseen_size_per_model\n",
        "    unseen_indices = torch.arange(start_idx, end_idx)\n",
        "    unseen_subset = Subset(unseen_dataset, unseen_indices)\n",
        "    unseen_loader = DataLoader(unseen_subset, batch_size=64, shuffle=False)\n",
        "    unseen_loaders.append(unseen_loader)\n",
        "\n",
        "test_loader = DataLoader(test_set, batch_size=64, shuffle=False)\n",
        "\n",
        "# Initialize MembershipInferenceAttackNoPrivacy\n",
        "mia = MembershipInferenceAttackNoPrivacy(CIFAR10Classifier, AttackModel, device)\n",
        "\n",
        "# Train shadow models without differential privacy\n",
        "mia.train_shadow_models(seen_loaders, num_epochs=10)\n",
        "\n",
        "# Collect outputs for attack model\n",
        "mia.collect_outputs(seen_loaders, unseen_loaders)\n",
        "\n",
        "# Train attack models\n",
        "mia.train_attack_models(num_epochs=10)\n",
        "\n",
        "# Save the attack models\n",
        "mia.save_attack_models('attack_model')\n",
        "\n",
        "# Load the attack models (for future use)\n",
        "mia.load_attack_models('attack_model')"
      ],
      "metadata": {
        "id": "MRIjTrnMYczY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d629affa-0c04-4a81-8dca-d0df806405bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch 1, Loss: 2.278183\n",
            "Epoch 2, Loss: 2.141670\n",
            "Epoch 3, Loss: 2.035657\n",
            "Epoch 4, Loss: 1.948037\n",
            "Epoch 5, Loss: 1.827686\n",
            "Epoch 6, Loss: 1.767027\n",
            "Epoch 7, Loss: 1.657067\n",
            "Epoch 8, Loss: 1.562772\n",
            "Epoch 9, Loss: 1.393725\n",
            "Epoch 10, Loss: 1.383874\n",
            "Shadow model 1 trained.\n",
            "Epoch 1, Loss: 2.333053\n",
            "Epoch 2, Loss: 2.211608\n",
            "Epoch 3, Loss: 2.103024\n",
            "Epoch 4, Loss: 2.021191\n",
            "Epoch 5, Loss: 1.933874\n",
            "Epoch 6, Loss: 1.873432\n",
            "Epoch 7, Loss: 1.723550\n",
            "Epoch 8, Loss: 1.580324\n",
            "Epoch 9, Loss: 1.479427\n",
            "Epoch 10, Loss: 1.407574\n",
            "Shadow model 2 trained.\n",
            "Epoch 1, Loss: 2.326754\n",
            "Epoch 2, Loss: 2.228614\n",
            "Epoch 3, Loss: 2.177272\n",
            "Epoch 4, Loss: 2.108865\n",
            "Epoch 5, Loss: 2.043911\n",
            "Epoch 6, Loss: 1.893782\n",
            "Epoch 7, Loss: 1.860719\n",
            "Epoch 8, Loss: 1.795353\n",
            "Epoch 9, Loss: 1.687164\n",
            "Epoch 10, Loss: 1.643388\n",
            "Shadow model 3 trained.\n",
            "Epoch 1, Loss: 2.290504\n",
            "Epoch 2, Loss: 2.199087\n",
            "Epoch 3, Loss: 2.077600\n",
            "Epoch 4, Loss: 2.003038\n",
            "Epoch 5, Loss: 1.921396\n",
            "Epoch 6, Loss: 1.838039\n",
            "Epoch 7, Loss: 1.742831\n",
            "Epoch 8, Loss: 1.696112\n",
            "Epoch 9, Loss: 1.638949\n",
            "Epoch 10, Loss: 1.555054\n",
            "Shadow model 4 trained.\n",
            "Epoch 1, Loss: 2.299843\n",
            "Epoch 2, Loss: 2.124530\n",
            "Epoch 3, Loss: 2.119181\n",
            "Epoch 4, Loss: 1.963845\n",
            "Epoch 5, Loss: 1.805853\n",
            "Epoch 6, Loss: 1.687003\n",
            "Epoch 7, Loss: 1.621715\n",
            "Epoch 8, Loss: 1.549351\n",
            "Epoch 9, Loss: 1.423782\n",
            "Epoch 10, Loss: 1.362608\n",
            "Shadow model 5 trained.\n",
            "Epoch 1, Loss: 2.330350\n",
            "Epoch 2, Loss: 2.189879\n",
            "Epoch 3, Loss: 2.150589\n",
            "Epoch 4, Loss: 2.091677\n",
            "Epoch 5, Loss: 1.982412\n",
            "Epoch 6, Loss: 1.932097\n",
            "Epoch 7, Loss: 1.872928\n",
            "Epoch 8, Loss: 1.789363\n",
            "Epoch 9, Loss: 1.725380\n",
            "Epoch 10, Loss: 1.691762\n",
            "Shadow model 6 trained.\n",
            "Epoch 1, Loss: 2.290319\n",
            "Epoch 2, Loss: 2.164351\n",
            "Epoch 3, Loss: 2.134650\n",
            "Epoch 4, Loss: 1.989053\n",
            "Epoch 5, Loss: 1.924650\n",
            "Epoch 6, Loss: 1.934540\n",
            "Epoch 7, Loss: 1.832889\n",
            "Epoch 8, Loss: 1.770496\n",
            "Epoch 9, Loss: 1.615993\n",
            "Epoch 10, Loss: 1.685588\n",
            "Shadow model 7 trained.\n",
            "Epoch 1, Loss: 2.307012\n",
            "Epoch 2, Loss: 2.223793\n",
            "Epoch 3, Loss: 2.096441\n",
            "Epoch 4, Loss: 1.957058\n",
            "Epoch 5, Loss: 1.951712\n",
            "Epoch 6, Loss: 1.856694\n",
            "Epoch 7, Loss: 1.744138\n",
            "Epoch 8, Loss: 1.674575\n",
            "Epoch 9, Loss: 1.565593\n",
            "Epoch 10, Loss: 1.492602\n",
            "Shadow model 8 trained.\n",
            "Epoch 1, Loss: 2.332264\n",
            "Epoch 2, Loss: 2.191101\n",
            "Epoch 3, Loss: 2.097904\n",
            "Epoch 4, Loss: 1.950528\n",
            "Epoch 5, Loss: 1.934664\n",
            "Epoch 6, Loss: 1.791361\n",
            "Epoch 7, Loss: 1.690552\n",
            "Epoch 8, Loss: 1.577210\n",
            "Epoch 9, Loss: 1.526015\n",
            "Epoch 10, Loss: 1.425531\n",
            "Shadow model 9 trained.\n",
            "Epoch 1, Loss: 2.287453\n",
            "Epoch 2, Loss: 2.144388\n",
            "Epoch 3, Loss: 1.962586\n",
            "Epoch 4, Loss: 1.982860\n",
            "Epoch 5, Loss: 1.858883\n",
            "Epoch 6, Loss: 1.785325\n",
            "Epoch 7, Loss: 1.720772\n",
            "Epoch 8, Loss: 1.592509\n",
            "Epoch 9, Loss: 1.574316\n",
            "Epoch 10, Loss: 1.517118\n",
            "Shadow model 10 trained.\n",
            "Epoch 1, Loss: 2.314902\n",
            "Epoch 2, Loss: 2.208903\n",
            "Epoch 3, Loss: 2.086264\n",
            "Epoch 4, Loss: 2.005649\n",
            "Epoch 5, Loss: 1.872085\n",
            "Epoch 6, Loss: 1.774582\n",
            "Epoch 7, Loss: 1.714712\n",
            "Epoch 8, Loss: 1.684254\n",
            "Epoch 9, Loss: 1.501419\n",
            "Epoch 10, Loss: 1.438556\n",
            "Shadow model 11 trained.\n",
            "Epoch 1, Loss: 2.317933\n",
            "Epoch 2, Loss: 2.223469\n",
            "Epoch 3, Loss: 2.131528\n",
            "Epoch 4, Loss: 2.023561\n",
            "Epoch 5, Loss: 1.927639\n",
            "Epoch 6, Loss: 1.844556\n",
            "Epoch 7, Loss: 1.767172\n",
            "Epoch 8, Loss: 1.668623\n",
            "Epoch 9, Loss: 1.588680\n",
            "Epoch 10, Loss: 1.567401\n",
            "Shadow model 12 trained.\n",
            "Epoch 1, Loss: 2.290602\n",
            "Epoch 2, Loss: 2.107984\n",
            "Epoch 3, Loss: 1.993025\n",
            "Epoch 4, Loss: 1.852706\n",
            "Epoch 5, Loss: 1.797569\n",
            "Epoch 6, Loss: 1.649508\n",
            "Epoch 7, Loss: 1.632416\n",
            "Epoch 8, Loss: 1.461483\n",
            "Epoch 9, Loss: 1.389703\n",
            "Epoch 10, Loss: 1.264266\n",
            "Shadow model 13 trained.\n",
            "Epoch 1, Loss: 2.319783\n",
            "Epoch 2, Loss: 2.194231\n",
            "Epoch 3, Loss: 2.109389\n",
            "Epoch 4, Loss: 2.053510\n",
            "Epoch 5, Loss: 1.949626\n",
            "Epoch 6, Loss: 1.873472\n",
            "Epoch 7, Loss: 1.796323\n",
            "Epoch 8, Loss: 1.703143\n",
            "Epoch 9, Loss: 1.673870\n",
            "Epoch 10, Loss: 1.533347\n",
            "Shadow model 14 trained.\n",
            "Epoch 1, Loss: 2.307336\n",
            "Epoch 2, Loss: 2.150859\n",
            "Epoch 3, Loss: 2.055137\n",
            "Epoch 4, Loss: 1.933675\n",
            "Epoch 5, Loss: 1.896999\n",
            "Epoch 6, Loss: 1.781991\n",
            "Epoch 7, Loss: 1.703516\n",
            "Epoch 8, Loss: 1.679069\n",
            "Epoch 9, Loss: 1.666971\n",
            "Epoch 10, Loss: 1.560839\n",
            "Shadow model 15 trained.\n",
            "Epoch 1, Loss: 2.296183\n",
            "Epoch 2, Loss: 2.194730\n",
            "Epoch 3, Loss: 2.127539\n",
            "Epoch 4, Loss: 2.028648\n",
            "Epoch 5, Loss: 1.958074\n",
            "Epoch 6, Loss: 1.862728\n",
            "Epoch 7, Loss: 1.819899\n",
            "Epoch 8, Loss: 1.716385\n",
            "Epoch 9, Loss: 1.696084\n",
            "Epoch 10, Loss: 1.577089\n",
            "Shadow model 16 trained.\n",
            "Epoch 1, Loss: 2.349062\n",
            "Epoch 2, Loss: 2.223693\n",
            "Epoch 3, Loss: 2.170028\n",
            "Epoch 4, Loss: 2.048040\n",
            "Epoch 5, Loss: 2.004541\n",
            "Epoch 6, Loss: 1.950207\n",
            "Epoch 7, Loss: 1.902023\n",
            "Epoch 8, Loss: 1.824648\n",
            "Epoch 9, Loss: 1.741658\n",
            "Epoch 10, Loss: 1.688821\n",
            "Shadow model 17 trained.\n",
            "Epoch 1, Loss: 2.319572\n",
            "Epoch 2, Loss: 2.186488\n",
            "Epoch 3, Loss: 2.125301\n",
            "Epoch 4, Loss: 2.040591\n",
            "Epoch 5, Loss: 1.933587\n",
            "Epoch 6, Loss: 1.919663\n",
            "Epoch 7, Loss: 1.754909\n",
            "Epoch 8, Loss: 1.714957\n",
            "Epoch 9, Loss: 1.661857\n",
            "Epoch 10, Loss: 1.635944\n",
            "Shadow model 18 trained.\n",
            "Epoch 1, Loss: 2.319639\n",
            "Epoch 2, Loss: 2.161044\n",
            "Epoch 3, Loss: 2.054942\n",
            "Epoch 4, Loss: 1.961167\n",
            "Epoch 5, Loss: 1.828305\n",
            "Epoch 6, Loss: 1.776615\n",
            "Epoch 7, Loss: 1.666760\n",
            "Epoch 8, Loss: 1.606021\n",
            "Epoch 9, Loss: 1.570449\n",
            "Epoch 10, Loss: 1.430888\n",
            "Shadow model 19 trained.\n",
            "Epoch 1, Loss: 2.305564\n",
            "Epoch 2, Loss: 2.204110\n",
            "Epoch 3, Loss: 2.071929\n",
            "Epoch 4, Loss: 2.016843\n",
            "Epoch 5, Loss: 1.917330\n",
            "Epoch 6, Loss: 1.822851\n",
            "Epoch 7, Loss: 1.691182\n",
            "Epoch 8, Loss: 1.601545\n",
            "Epoch 9, Loss: 1.546460\n",
            "Epoch 10, Loss: 1.403289\n",
            "Shadow model 20 trained.\n",
            "Epoch 1, Loss: 2.280123\n",
            "Epoch 2, Loss: 2.192609\n",
            "Epoch 3, Loss: 2.067081\n",
            "Epoch 4, Loss: 1.963757\n",
            "Epoch 5, Loss: 1.858186\n",
            "Epoch 6, Loss: 1.756217\n",
            "Epoch 7, Loss: 1.696454\n",
            "Epoch 8, Loss: 1.617699\n",
            "Epoch 9, Loss: 1.531529\n",
            "Epoch 10, Loss: 1.470884\n",
            "Shadow model 21 trained.\n",
            "Epoch 1, Loss: 2.333810\n",
            "Epoch 2, Loss: 2.221933\n",
            "Epoch 3, Loss: 2.171625\n",
            "Epoch 4, Loss: 2.098363\n",
            "Epoch 5, Loss: 2.078836\n",
            "Epoch 6, Loss: 1.954084\n",
            "Epoch 7, Loss: 1.904985\n",
            "Epoch 8, Loss: 1.775864\n",
            "Epoch 9, Loss: 1.711754\n",
            "Epoch 10, Loss: 1.785045\n",
            "Shadow model 22 trained.\n",
            "Epoch 1, Loss: 2.305727\n",
            "Epoch 2, Loss: 2.199273\n",
            "Epoch 3, Loss: 2.028649\n",
            "Epoch 4, Loss: 2.032788\n",
            "Epoch 5, Loss: 1.970467\n",
            "Epoch 6, Loss: 1.849257\n",
            "Epoch 7, Loss: 1.810719\n",
            "Epoch 8, Loss: 1.715436\n",
            "Epoch 9, Loss: 1.663444\n",
            "Epoch 10, Loss: 1.673118\n",
            "Shadow model 23 trained.\n",
            "Epoch 1, Loss: 2.324259\n",
            "Epoch 2, Loss: 2.262848\n",
            "Epoch 3, Loss: 2.187951\n",
            "Epoch 4, Loss: 2.107921\n",
            "Epoch 5, Loss: 2.062569\n",
            "Epoch 6, Loss: 1.951217\n",
            "Epoch 7, Loss: 1.877997\n",
            "Epoch 8, Loss: 1.824582\n",
            "Epoch 9, Loss: 1.730590\n",
            "Epoch 10, Loss: 1.626956\n",
            "Shadow model 24 trained.\n",
            "Epoch 1, Loss: 2.301108\n",
            "Epoch 2, Loss: 2.179533\n",
            "Epoch 3, Loss: 2.079924\n",
            "Epoch 4, Loss: 2.000580\n",
            "Epoch 5, Loss: 1.863395\n",
            "Epoch 6, Loss: 1.825924\n",
            "Epoch 7, Loss: 1.737544\n",
            "Epoch 8, Loss: 1.602455\n",
            "Epoch 9, Loss: 1.591629\n",
            "Epoch 10, Loss: 1.469070\n",
            "Shadow model 25 trained.\n",
            "Epoch 1, Loss: 2.281888\n",
            "Epoch 2, Loss: 2.201553\n",
            "Epoch 3, Loss: 2.152905\n",
            "Epoch 4, Loss: 2.007228\n",
            "Epoch 5, Loss: 1.906390\n",
            "Epoch 6, Loss: 1.819834\n",
            "Epoch 7, Loss: 1.753040\n",
            "Epoch 8, Loss: 1.671253\n",
            "Epoch 9, Loss: 1.528980\n",
            "Epoch 10, Loss: 1.480917\n",
            "Shadow model 26 trained.\n",
            "Epoch 1, Loss: 2.302412\n",
            "Epoch 2, Loss: 2.186318\n",
            "Epoch 3, Loss: 2.123110\n",
            "Epoch 4, Loss: 2.050374\n",
            "Epoch 5, Loss: 1.948670\n",
            "Epoch 6, Loss: 1.820428\n",
            "Epoch 7, Loss: 1.889304\n",
            "Epoch 8, Loss: 1.755866\n",
            "Epoch 9, Loss: 1.605270\n",
            "Epoch 10, Loss: 1.584537\n",
            "Shadow model 27 trained.\n",
            "Epoch 1, Loss: 2.310790\n",
            "Epoch 2, Loss: 2.249955\n",
            "Epoch 3, Loss: 2.180061\n",
            "Epoch 4, Loss: 2.148167\n",
            "Epoch 5, Loss: 1.986778\n",
            "Epoch 6, Loss: 1.956624\n",
            "Epoch 7, Loss: 1.886928\n",
            "Epoch 8, Loss: 1.793500\n",
            "Epoch 9, Loss: 1.705669\n",
            "Epoch 10, Loss: 1.669881\n",
            "Shadow model 28 trained.\n",
            "Epoch 1, Loss: 2.312262\n",
            "Epoch 2, Loss: 2.158343\n",
            "Epoch 3, Loss: 2.044042\n",
            "Epoch 4, Loss: 1.972291\n",
            "Epoch 5, Loss: 1.985969\n",
            "Epoch 6, Loss: 1.866684\n",
            "Epoch 7, Loss: 1.821442\n",
            "Epoch 8, Loss: 1.717876\n",
            "Epoch 9, Loss: 1.666667\n",
            "Epoch 10, Loss: 1.533513\n",
            "Shadow model 29 trained.\n",
            "Epoch 1, Loss: 2.311394\n",
            "Epoch 2, Loss: 2.201785\n",
            "Epoch 3, Loss: 2.148725\n",
            "Epoch 4, Loss: 2.073173\n",
            "Epoch 5, Loss: 2.008064\n",
            "Epoch 6, Loss: 1.932932\n",
            "Epoch 7, Loss: 1.923477\n",
            "Epoch 8, Loss: 1.681771\n",
            "Epoch 9, Loss: 1.697948\n",
            "Epoch 10, Loss: 1.614170\n",
            "Shadow model 30 trained.\n",
            "Epoch 1, Loss: 2.277795\n",
            "Epoch 2, Loss: 2.178171\n",
            "Epoch 3, Loss: 2.026509\n",
            "Epoch 4, Loss: 1.944619\n",
            "Epoch 5, Loss: 1.860357\n",
            "Epoch 6, Loss: 1.714295\n",
            "Epoch 7, Loss: 1.599136\n",
            "Epoch 8, Loss: 1.623645\n",
            "Epoch 9, Loss: 1.428226\n",
            "Epoch 10, Loss: 1.373879\n",
            "Shadow model 31 trained.\n",
            "Epoch 1, Loss: 2.319868\n",
            "Epoch 2, Loss: 2.176729\n",
            "Epoch 3, Loss: 2.114350\n",
            "Epoch 4, Loss: 2.039506\n",
            "Epoch 5, Loss: 1.931041\n",
            "Epoch 6, Loss: 1.880622\n",
            "Epoch 7, Loss: 1.795423\n",
            "Epoch 8, Loss: 1.714833\n",
            "Epoch 9, Loss: 1.701344\n",
            "Epoch 10, Loss: 1.532650\n",
            "Shadow model 32 trained.\n",
            "Epoch 1, Loss: 2.288096\n",
            "Epoch 2, Loss: 2.190865\n",
            "Epoch 3, Loss: 2.089847\n",
            "Epoch 4, Loss: 2.042807\n",
            "Epoch 5, Loss: 1.934879\n",
            "Epoch 6, Loss: 1.852378\n",
            "Epoch 7, Loss: 1.810919\n",
            "Epoch 8, Loss: 1.687256\n",
            "Epoch 9, Loss: 1.602042\n",
            "Epoch 10, Loss: 1.513458\n",
            "Shadow model 33 trained.\n",
            "Epoch 1, Loss: 2.333013\n",
            "Epoch 2, Loss: 2.190751\n",
            "Epoch 3, Loss: 2.124092\n",
            "Epoch 4, Loss: 1.966257\n",
            "Epoch 5, Loss: 2.028441\n",
            "Epoch 6, Loss: 1.875763\n",
            "Epoch 7, Loss: 1.825977\n",
            "Epoch 8, Loss: 1.734976\n",
            "Epoch 9, Loss: 1.664662\n",
            "Epoch 10, Loss: 1.620820\n",
            "Shadow model 34 trained.\n",
            "Epoch 1, Loss: 2.329955\n",
            "Epoch 2, Loss: 2.203340\n",
            "Epoch 3, Loss: 2.142319\n",
            "Epoch 4, Loss: 2.110252\n",
            "Epoch 5, Loss: 1.947871\n",
            "Epoch 6, Loss: 1.820844\n",
            "Epoch 7, Loss: 1.724709\n",
            "Epoch 8, Loss: 1.700383\n",
            "Epoch 9, Loss: 1.578775\n",
            "Epoch 10, Loss: 1.551877\n",
            "Shadow model 35 trained.\n",
            "Epoch 1, Loss: 2.318642\n",
            "Epoch 2, Loss: 2.163118\n",
            "Epoch 3, Loss: 2.043712\n",
            "Epoch 4, Loss: 1.983060\n",
            "Epoch 5, Loss: 1.839950\n",
            "Epoch 6, Loss: 1.802843\n",
            "Epoch 7, Loss: 1.727218\n",
            "Epoch 8, Loss: 1.635262\n",
            "Epoch 9, Loss: 1.543528\n",
            "Epoch 10, Loss: 1.467806\n",
            "Shadow model 36 trained.\n",
            "Epoch 1, Loss: 2.329252\n",
            "Epoch 2, Loss: 2.218671\n",
            "Epoch 3, Loss: 2.123596\n",
            "Epoch 4, Loss: 2.069730\n",
            "Epoch 5, Loss: 2.004371\n",
            "Epoch 6, Loss: 1.945121\n",
            "Epoch 7, Loss: 1.873813\n",
            "Epoch 8, Loss: 1.780993\n",
            "Epoch 9, Loss: 1.678626\n",
            "Epoch 10, Loss: 1.623209\n",
            "Shadow model 37 trained.\n",
            "Epoch 1, Loss: 2.342661\n",
            "Epoch 2, Loss: 2.246359\n",
            "Epoch 3, Loss: 2.168608\n",
            "Epoch 4, Loss: 2.118675\n",
            "Epoch 5, Loss: 2.021072\n",
            "Epoch 6, Loss: 1.974886\n",
            "Epoch 7, Loss: 1.919434\n",
            "Epoch 8, Loss: 1.812202\n",
            "Epoch 9, Loss: 1.736865\n",
            "Epoch 10, Loss: 1.658537\n",
            "Shadow model 38 trained.\n",
            "Epoch 1, Loss: 2.295863\n",
            "Epoch 2, Loss: 2.197079\n",
            "Epoch 3, Loss: 2.129727\n",
            "Epoch 4, Loss: 2.051905\n",
            "Epoch 5, Loss: 1.943921\n",
            "Epoch 6, Loss: 1.810624\n",
            "Epoch 7, Loss: 1.796709\n",
            "Epoch 8, Loss: 1.642309\n",
            "Epoch 9, Loss: 1.621283\n",
            "Epoch 10, Loss: 1.572288\n",
            "Shadow model 39 trained.\n",
            "Epoch 1, Loss: 2.324542\n",
            "Epoch 2, Loss: 2.185258\n",
            "Epoch 3, Loss: 2.132055\n",
            "Epoch 4, Loss: 2.001684\n",
            "Epoch 5, Loss: 1.942299\n",
            "Epoch 6, Loss: 1.910757\n",
            "Epoch 7, Loss: 1.838802\n",
            "Epoch 8, Loss: 1.794872\n",
            "Epoch 9, Loss: 1.667557\n",
            "Epoch 10, Loss: 1.573854\n",
            "Shadow model 40 trained.\n",
            "Epoch 1, Loss: 2.303090\n",
            "Epoch 2, Loss: 2.229602\n",
            "Epoch 3, Loss: 2.149081\n",
            "Epoch 4, Loss: 2.027778\n",
            "Epoch 5, Loss: 1.978728\n",
            "Epoch 6, Loss: 1.887567\n",
            "Epoch 7, Loss: 1.784170\n",
            "Epoch 8, Loss: 1.756090\n",
            "Epoch 9, Loss: 1.736884\n",
            "Epoch 10, Loss: 1.607975\n",
            "Shadow model 41 trained.\n",
            "Epoch 1, Loss: 2.310730\n",
            "Epoch 2, Loss: 2.224335\n",
            "Epoch 3, Loss: 2.159173\n",
            "Epoch 4, Loss: 2.112151\n",
            "Epoch 5, Loss: 2.039420\n",
            "Epoch 6, Loss: 1.942567\n",
            "Epoch 7, Loss: 1.862228\n",
            "Epoch 8, Loss: 1.753421\n",
            "Epoch 9, Loss: 1.599012\n",
            "Epoch 10, Loss: 1.571994\n",
            "Shadow model 42 trained.\n",
            "Epoch 1, Loss: 2.295935\n",
            "Epoch 2, Loss: 2.201773\n",
            "Epoch 3, Loss: 2.150377\n",
            "Epoch 4, Loss: 2.074665\n",
            "Epoch 5, Loss: 1.985378\n",
            "Epoch 6, Loss: 1.890958\n",
            "Epoch 7, Loss: 1.861076\n",
            "Epoch 8, Loss: 1.756289\n",
            "Epoch 9, Loss: 1.647496\n",
            "Epoch 10, Loss: 1.641184\n",
            "Shadow model 43 trained.\n",
            "Epoch 1, Loss: 2.319426\n",
            "Epoch 2, Loss: 2.249780\n",
            "Epoch 3, Loss: 2.138019\n",
            "Epoch 4, Loss: 2.009875\n",
            "Epoch 5, Loss: 1.955427\n",
            "Epoch 6, Loss: 1.787013\n",
            "Epoch 7, Loss: 1.684390\n",
            "Epoch 8, Loss: 1.495168\n",
            "Epoch 9, Loss: 1.430439\n",
            "Epoch 10, Loss: 1.283676\n",
            "Shadow model 44 trained.\n",
            "Epoch 1, Loss: 2.322803\n",
            "Epoch 2, Loss: 2.184006\n",
            "Epoch 3, Loss: 2.043491\n",
            "Epoch 4, Loss: 1.916678\n",
            "Epoch 5, Loss: 1.815557\n",
            "Epoch 6, Loss: 1.737150\n",
            "Epoch 7, Loss: 1.718151\n",
            "Epoch 8, Loss: 1.524279\n",
            "Epoch 9, Loss: 1.403527\n",
            "Epoch 10, Loss: 1.373904\n",
            "Shadow model 45 trained.\n",
            "Epoch 1, Loss: 2.328230\n",
            "Epoch 2, Loss: 2.239994\n",
            "Epoch 3, Loss: 2.156196\n",
            "Epoch 4, Loss: 2.031290\n",
            "Epoch 5, Loss: 2.044077\n",
            "Epoch 6, Loss: 1.952260\n",
            "Epoch 7, Loss: 1.842889\n",
            "Epoch 8, Loss: 1.754282\n",
            "Epoch 9, Loss: 1.694831\n",
            "Epoch 10, Loss: 1.615951\n",
            "Shadow model 46 trained.\n",
            "Epoch 1, Loss: 2.243919\n",
            "Epoch 2, Loss: 2.078759\n",
            "Epoch 3, Loss: 2.038110\n",
            "Epoch 4, Loss: 1.915702\n",
            "Epoch 5, Loss: 1.826737\n",
            "Epoch 6, Loss: 1.793963\n",
            "Epoch 7, Loss: 1.683870\n",
            "Epoch 8, Loss: 1.636079\n",
            "Epoch 9, Loss: 1.551295\n",
            "Epoch 10, Loss: 1.453779\n",
            "Shadow model 47 trained.\n",
            "Epoch 1, Loss: 2.286691\n",
            "Epoch 2, Loss: 2.159989\n",
            "Epoch 3, Loss: 2.077918\n",
            "Epoch 4, Loss: 2.089486\n",
            "Epoch 5, Loss: 1.995296\n",
            "Epoch 6, Loss: 1.909295\n",
            "Epoch 7, Loss: 1.832862\n",
            "Epoch 8, Loss: 1.776973\n",
            "Epoch 9, Loss: 1.751066\n",
            "Epoch 10, Loss: 1.604774\n",
            "Shadow model 48 trained.\n",
            "Epoch 1, Loss: 2.294435\n",
            "Epoch 2, Loss: 2.135101\n",
            "Epoch 3, Loss: 2.050547\n",
            "Epoch 4, Loss: 1.941441\n",
            "Epoch 5, Loss: 1.905791\n",
            "Epoch 6, Loss: 1.804698\n",
            "Epoch 7, Loss: 1.750987\n",
            "Epoch 8, Loss: 1.657249\n",
            "Epoch 9, Loss: 1.459247\n",
            "Epoch 10, Loss: 1.442161\n",
            "Shadow model 49 trained.\n",
            "Epoch 1, Loss: 2.312658\n",
            "Epoch 2, Loss: 2.223580\n",
            "Epoch 3, Loss: 2.111225\n",
            "Epoch 4, Loss: 2.007698\n",
            "Epoch 5, Loss: 1.922337\n",
            "Epoch 6, Loss: 1.887931\n",
            "Epoch 7, Loss: 1.793730\n",
            "Epoch 8, Loss: 1.641791\n",
            "Epoch 9, Loss: 1.558758\n",
            "Epoch 10, Loss: 1.403449\n",
            "Shadow model 50 trained.\n",
            "Epoch 1, Loss: 2.330723\n",
            "Epoch 2, Loss: 2.196614\n",
            "Epoch 3, Loss: 2.106184\n",
            "Epoch 4, Loss: 1.965674\n",
            "Epoch 5, Loss: 1.935827\n",
            "Epoch 6, Loss: 1.831160\n",
            "Epoch 7, Loss: 1.795594\n",
            "Epoch 8, Loss: 1.666817\n",
            "Epoch 9, Loss: 1.605890\n",
            "Epoch 10, Loss: 1.528419\n",
            "Shadow model 51 trained.\n",
            "Epoch 1, Loss: 2.307010\n",
            "Epoch 2, Loss: 2.207860\n",
            "Epoch 3, Loss: 2.141249\n",
            "Epoch 4, Loss: 2.060271\n",
            "Epoch 5, Loss: 2.015820\n",
            "Epoch 6, Loss: 1.975739\n",
            "Epoch 7, Loss: 1.902033\n",
            "Epoch 8, Loss: 1.774708\n",
            "Epoch 9, Loss: 1.739059\n",
            "Epoch 10, Loss: 1.704810\n",
            "Shadow model 52 trained.\n",
            "Epoch 1, Loss: 2.304455\n",
            "Epoch 2, Loss: 2.179197\n",
            "Epoch 3, Loss: 2.135929\n",
            "Epoch 4, Loss: 1.961636\n",
            "Epoch 5, Loss: 1.891241\n",
            "Epoch 6, Loss: 1.806608\n",
            "Epoch 7, Loss: 1.691169\n",
            "Epoch 8, Loss: 1.638447\n",
            "Epoch 9, Loss: 1.590253\n",
            "Epoch 10, Loss: 1.442838\n",
            "Shadow model 53 trained.\n",
            "Epoch 1, Loss: 2.316790\n",
            "Epoch 2, Loss: 2.217251\n",
            "Epoch 3, Loss: 2.126214\n",
            "Epoch 4, Loss: 2.060335\n",
            "Epoch 5, Loss: 1.985493\n",
            "Epoch 6, Loss: 1.879843\n",
            "Epoch 7, Loss: 1.782879\n",
            "Epoch 8, Loss: 1.648955\n",
            "Epoch 9, Loss: 1.625182\n",
            "Epoch 10, Loss: 1.466124\n",
            "Shadow model 54 trained.\n",
            "Epoch 1, Loss: 2.299897\n",
            "Epoch 2, Loss: 2.239038\n",
            "Epoch 3, Loss: 2.098611\n",
            "Epoch 4, Loss: 1.981063\n",
            "Epoch 5, Loss: 1.971545\n",
            "Epoch 6, Loss: 1.895709\n",
            "Epoch 7, Loss: 1.706925\n",
            "Epoch 8, Loss: 1.631374\n",
            "Epoch 9, Loss: 1.649342\n",
            "Epoch 10, Loss: 1.534633\n",
            "Shadow model 55 trained.\n",
            "Epoch 1, Loss: 2.313511\n",
            "Epoch 2, Loss: 2.203176\n",
            "Epoch 3, Loss: 2.087829\n",
            "Epoch 4, Loss: 2.027110\n",
            "Epoch 5, Loss: 1.923284\n",
            "Epoch 6, Loss: 1.866915\n",
            "Epoch 7, Loss: 1.804665\n",
            "Epoch 8, Loss: 1.717644\n",
            "Epoch 9, Loss: 1.612735\n",
            "Epoch 10, Loss: 1.539384\n",
            "Shadow model 56 trained.\n",
            "Epoch 1, Loss: 2.335593\n",
            "Epoch 2, Loss: 2.214518\n",
            "Epoch 3, Loss: 2.112622\n",
            "Epoch 4, Loss: 2.082182\n",
            "Epoch 5, Loss: 1.945384\n",
            "Epoch 6, Loss: 1.856324\n",
            "Epoch 7, Loss: 1.727018\n",
            "Epoch 8, Loss: 1.625297\n",
            "Epoch 9, Loss: 1.518639\n",
            "Epoch 10, Loss: 1.417454\n",
            "Shadow model 57 trained.\n",
            "Epoch 1, Loss: 2.324682\n",
            "Epoch 2, Loss: 2.188456\n",
            "Epoch 3, Loss: 2.081498\n",
            "Epoch 4, Loss: 2.061659\n",
            "Epoch 5, Loss: 1.985979\n",
            "Epoch 6, Loss: 1.820931\n",
            "Epoch 7, Loss: 1.763054\n",
            "Epoch 8, Loss: 1.707680\n",
            "Epoch 9, Loss: 1.668711\n",
            "Epoch 10, Loss: 1.611696\n",
            "Shadow model 58 trained.\n",
            "Epoch 1, Loss: 2.305112\n",
            "Epoch 2, Loss: 2.207424\n",
            "Epoch 3, Loss: 2.102715\n",
            "Epoch 4, Loss: 1.965470\n",
            "Epoch 5, Loss: 1.916768\n",
            "Epoch 6, Loss: 1.845713\n",
            "Epoch 7, Loss: 1.696111\n",
            "Epoch 8, Loss: 1.569099\n",
            "Epoch 9, Loss: 1.516578\n",
            "Epoch 10, Loss: 1.394136\n",
            "Shadow model 59 trained.\n",
            "Epoch 1, Loss: 2.302356\n",
            "Epoch 2, Loss: 2.152041\n",
            "Epoch 3, Loss: 2.120381\n",
            "Epoch 4, Loss: 1.995285\n",
            "Epoch 5, Loss: 1.946326\n",
            "Epoch 6, Loss: 1.891778\n",
            "Epoch 7, Loss: 1.741873\n",
            "Epoch 8, Loss: 1.678919\n",
            "Epoch 9, Loss: 1.525344\n",
            "Epoch 10, Loss: 1.425332\n",
            "Shadow model 60 trained.\n",
            "Epoch 1, Loss: 2.288291\n",
            "Epoch 2, Loss: 2.161151\n",
            "Epoch 3, Loss: 2.015428\n",
            "Epoch 4, Loss: 1.982894\n",
            "Epoch 5, Loss: 1.887708\n",
            "Epoch 6, Loss: 1.727449\n",
            "Epoch 7, Loss: 1.720765\n",
            "Epoch 8, Loss: 1.603458\n",
            "Epoch 9, Loss: 1.491432\n",
            "Epoch 10, Loss: 1.418701\n",
            "Shadow model 61 trained.\n",
            "Epoch 1, Loss: 2.291983\n",
            "Epoch 2, Loss: 2.195465\n",
            "Epoch 3, Loss: 2.122154\n",
            "Epoch 4, Loss: 2.024050\n",
            "Epoch 5, Loss: 1.992221\n",
            "Epoch 6, Loss: 1.872569\n",
            "Epoch 7, Loss: 1.842778\n",
            "Epoch 8, Loss: 1.682291\n",
            "Epoch 9, Loss: 1.615920\n",
            "Epoch 10, Loss: 1.552874\n",
            "Shadow model 62 trained.\n",
            "Epoch 1, Loss: 2.335724\n",
            "Epoch 2, Loss: 2.265407\n",
            "Epoch 3, Loss: 2.189119\n",
            "Epoch 4, Loss: 2.157103\n",
            "Epoch 5, Loss: 2.097791\n",
            "Epoch 6, Loss: 2.044243\n",
            "Epoch 7, Loss: 1.964590\n",
            "Epoch 8, Loss: 1.846688\n",
            "Epoch 9, Loss: 1.807528\n",
            "Epoch 10, Loss: 1.737122\n",
            "Shadow model 63 trained.\n",
            "Epoch 1, Loss: 2.315726\n",
            "Epoch 2, Loss: 2.184858\n",
            "Epoch 3, Loss: 2.122748\n",
            "Epoch 4, Loss: 2.053099\n",
            "Epoch 5, Loss: 1.900178\n",
            "Epoch 6, Loss: 1.827084\n",
            "Epoch 7, Loss: 1.801290\n",
            "Epoch 8, Loss: 1.707722\n",
            "Epoch 9, Loss: 1.555381\n",
            "Epoch 10, Loss: 1.441201\n",
            "Shadow model 64 trained.\n",
            "Epoch 1, Loss: 2.318144\n",
            "Epoch 2, Loss: 2.236865\n",
            "Epoch 3, Loss: 2.187731\n",
            "Epoch 4, Loss: 2.124060\n",
            "Epoch 5, Loss: 2.028656\n",
            "Epoch 6, Loss: 2.010159\n",
            "Epoch 7, Loss: 1.894598\n",
            "Epoch 8, Loss: 1.780007\n",
            "Epoch 9, Loss: 1.702700\n",
            "Epoch 10, Loss: 1.661886\n",
            "Shadow model 65 trained.\n",
            "Epoch 1, Loss: 2.345386\n",
            "Epoch 2, Loss: 2.203176\n",
            "Epoch 3, Loss: 2.148060\n",
            "Epoch 4, Loss: 2.055327\n",
            "Epoch 5, Loss: 2.037763\n",
            "Epoch 6, Loss: 1.889800\n",
            "Epoch 7, Loss: 1.866786\n",
            "Epoch 8, Loss: 1.822833\n",
            "Epoch 9, Loss: 1.666957\n",
            "Epoch 10, Loss: 1.652622\n",
            "Shadow model 66 trained.\n",
            "Epoch 1, Loss: 2.313292\n",
            "Epoch 2, Loss: 2.222805\n",
            "Epoch 3, Loss: 2.205214\n",
            "Epoch 4, Loss: 2.053494\n",
            "Epoch 5, Loss: 1.987065\n",
            "Epoch 6, Loss: 1.980483\n",
            "Epoch 7, Loss: 1.873118\n",
            "Epoch 8, Loss: 1.824073\n",
            "Epoch 9, Loss: 1.645273\n",
            "Epoch 10, Loss: 1.702065\n",
            "Shadow model 67 trained.\n",
            "Epoch 1, Loss: 2.307665\n",
            "Epoch 2, Loss: 2.175471\n",
            "Epoch 3, Loss: 2.050346\n",
            "Epoch 4, Loss: 1.915720\n",
            "Epoch 5, Loss: 1.881418\n",
            "Epoch 6, Loss: 1.732941\n",
            "Epoch 7, Loss: 1.725136\n",
            "Epoch 8, Loss: 1.664273\n",
            "Epoch 9, Loss: 1.577877\n",
            "Epoch 10, Loss: 1.455160\n",
            "Shadow model 68 trained.\n",
            "Epoch 1, Loss: 2.332031\n",
            "Epoch 2, Loss: 2.206869\n",
            "Epoch 3, Loss: 2.086155\n",
            "Epoch 4, Loss: 2.021327\n",
            "Epoch 5, Loss: 1.988625\n",
            "Epoch 6, Loss: 1.769926\n",
            "Epoch 7, Loss: 1.729667\n",
            "Epoch 8, Loss: 1.638812\n",
            "Epoch 9, Loss: 1.579086\n",
            "Epoch 10, Loss: 1.514397\n",
            "Shadow model 69 trained.\n",
            "Epoch 1, Loss: 2.336453\n",
            "Epoch 2, Loss: 2.173480\n",
            "Epoch 3, Loss: 2.072772\n",
            "Epoch 4, Loss: 2.029424\n",
            "Epoch 5, Loss: 1.966815\n",
            "Epoch 6, Loss: 1.854560\n",
            "Epoch 7, Loss: 1.793607\n",
            "Epoch 8, Loss: 1.670494\n",
            "Epoch 9, Loss: 1.624309\n",
            "Epoch 10, Loss: 1.481356\n",
            "Shadow model 70 trained.\n",
            "Epoch 1, Loss: 2.316941\n",
            "Epoch 2, Loss: 2.234787\n",
            "Epoch 3, Loss: 2.121324\n",
            "Epoch 4, Loss: 2.018822\n",
            "Epoch 5, Loss: 1.919129\n",
            "Epoch 6, Loss: 1.861287\n",
            "Epoch 7, Loss: 1.708485\n",
            "Epoch 8, Loss: 1.659950\n",
            "Epoch 9, Loss: 1.586085\n",
            "Epoch 10, Loss: 1.470059\n",
            "Shadow model 71 trained.\n",
            "Epoch 1, Loss: 2.305658\n",
            "Epoch 2, Loss: 2.223383\n",
            "Epoch 3, Loss: 2.110215\n",
            "Epoch 4, Loss: 2.025402\n",
            "Epoch 5, Loss: 1.985922\n",
            "Epoch 6, Loss: 1.872319\n",
            "Epoch 7, Loss: 1.720773\n",
            "Epoch 8, Loss: 1.700490\n",
            "Epoch 9, Loss: 1.476990\n",
            "Epoch 10, Loss: 1.346480\n",
            "Shadow model 72 trained.\n",
            "Epoch 1, Loss: 2.304494\n",
            "Epoch 2, Loss: 2.186690\n",
            "Epoch 3, Loss: 2.068667\n",
            "Epoch 4, Loss: 1.946329\n",
            "Epoch 5, Loss: 1.920718\n",
            "Epoch 6, Loss: 1.779730\n",
            "Epoch 7, Loss: 1.688145\n",
            "Epoch 8, Loss: 1.613699\n",
            "Epoch 9, Loss: 1.473959\n",
            "Epoch 10, Loss: 1.406029\n",
            "Shadow model 73 trained.\n",
            "Epoch 1, Loss: 2.306324\n",
            "Epoch 2, Loss: 2.182584\n",
            "Epoch 3, Loss: 2.074761\n",
            "Epoch 4, Loss: 2.002187\n",
            "Epoch 5, Loss: 1.962796\n",
            "Epoch 6, Loss: 1.836826\n",
            "Epoch 7, Loss: 1.760197\n",
            "Epoch 8, Loss: 1.659057\n",
            "Epoch 9, Loss: 1.613486\n",
            "Epoch 10, Loss: 1.475642\n",
            "Shadow model 74 trained.\n",
            "Epoch 1, Loss: 2.305890\n",
            "Epoch 2, Loss: 2.223351\n",
            "Epoch 3, Loss: 2.104920\n",
            "Epoch 4, Loss: 2.052106\n",
            "Epoch 5, Loss: 1.902566\n",
            "Epoch 6, Loss: 1.822993\n",
            "Epoch 7, Loss: 1.764974\n",
            "Epoch 8, Loss: 1.673882\n",
            "Epoch 9, Loss: 1.600141\n",
            "Epoch 10, Loss: 1.536312\n",
            "Shadow model 75 trained.\n",
            "Epoch 1, Loss: 2.324590\n",
            "Epoch 2, Loss: 2.253946\n",
            "Epoch 3, Loss: 2.146534\n",
            "Epoch 4, Loss: 2.066538\n",
            "Epoch 5, Loss: 2.012077\n",
            "Epoch 6, Loss: 1.949290\n",
            "Epoch 7, Loss: 1.934800\n",
            "Epoch 8, Loss: 1.770702\n",
            "Epoch 9, Loss: 1.651833\n",
            "Epoch 10, Loss: 1.643192\n",
            "Shadow model 76 trained.\n",
            "Epoch 1, Loss: 2.317288\n",
            "Epoch 2, Loss: 2.199102\n",
            "Epoch 3, Loss: 2.059410\n",
            "Epoch 4, Loss: 1.963216\n",
            "Epoch 5, Loss: 1.873631\n",
            "Epoch 6, Loss: 1.857781\n",
            "Epoch 7, Loss: 1.786155\n",
            "Epoch 8, Loss: 1.691948\n",
            "Epoch 9, Loss: 1.676234\n",
            "Epoch 10, Loss: 1.479401\n",
            "Shadow model 77 trained.\n",
            "Epoch 1, Loss: 2.293702\n",
            "Epoch 2, Loss: 2.170479\n",
            "Epoch 3, Loss: 2.082557\n",
            "Epoch 4, Loss: 2.032163\n",
            "Epoch 5, Loss: 1.955487\n",
            "Epoch 6, Loss: 1.866149\n",
            "Epoch 7, Loss: 1.811130\n",
            "Epoch 8, Loss: 1.658132\n",
            "Epoch 9, Loss: 1.599602\n",
            "Epoch 10, Loss: 1.533449\n",
            "Shadow model 78 trained.\n",
            "Epoch 1, Loss: 2.314363\n",
            "Epoch 2, Loss: 2.144893\n",
            "Epoch 3, Loss: 2.023182\n",
            "Epoch 4, Loss: 1.938305\n",
            "Epoch 5, Loss: 1.863203\n",
            "Epoch 6, Loss: 1.838069\n",
            "Epoch 7, Loss: 1.679430\n",
            "Epoch 8, Loss: 1.548752\n",
            "Epoch 9, Loss: 1.541856\n",
            "Epoch 10, Loss: 1.448269\n",
            "Shadow model 79 trained.\n",
            "Epoch 1, Loss: 2.356657\n",
            "Epoch 2, Loss: 2.233892\n",
            "Epoch 3, Loss: 2.168606\n",
            "Epoch 4, Loss: 2.103582\n",
            "Epoch 5, Loss: 2.045087\n",
            "Epoch 6, Loss: 1.967920\n",
            "Epoch 7, Loss: 1.871820\n",
            "Epoch 8, Loss: 1.864871\n",
            "Epoch 9, Loss: 1.701910\n",
            "Epoch 10, Loss: 1.701493\n",
            "Shadow model 80 trained.\n",
            "Epoch 1, Loss: 2.291601\n",
            "Epoch 2, Loss: 2.170787\n",
            "Epoch 3, Loss: 2.071273\n",
            "Epoch 4, Loss: 2.068114\n",
            "Epoch 5, Loss: 2.023670\n",
            "Epoch 6, Loss: 1.901302\n",
            "Epoch 7, Loss: 1.816203\n",
            "Epoch 8, Loss: 1.738307\n",
            "Epoch 9, Loss: 1.618311\n",
            "Epoch 10, Loss: 1.505778\n",
            "Shadow model 81 trained.\n",
            "Epoch 1, Loss: 2.298023\n",
            "Epoch 2, Loss: 2.187358\n",
            "Epoch 3, Loss: 2.070369\n",
            "Epoch 4, Loss: 1.989663\n",
            "Epoch 5, Loss: 1.869893\n",
            "Epoch 6, Loss: 1.823444\n",
            "Epoch 7, Loss: 1.723974\n",
            "Epoch 8, Loss: 1.648564\n",
            "Epoch 9, Loss: 1.562540\n",
            "Epoch 10, Loss: 1.476453\n",
            "Shadow model 82 trained.\n",
            "Epoch 1, Loss: 2.321412\n",
            "Epoch 2, Loss: 2.233808\n",
            "Epoch 3, Loss: 2.065065\n",
            "Epoch 4, Loss: 1.999101\n",
            "Epoch 5, Loss: 1.915662\n",
            "Epoch 6, Loss: 1.847684\n",
            "Epoch 7, Loss: 1.763348\n",
            "Epoch 8, Loss: 1.664717\n",
            "Epoch 9, Loss: 1.534829\n",
            "Epoch 10, Loss: 1.417880\n",
            "Shadow model 83 trained.\n",
            "Epoch 1, Loss: 2.296866\n",
            "Epoch 2, Loss: 2.217453\n",
            "Epoch 3, Loss: 2.151360\n",
            "Epoch 4, Loss: 2.052489\n",
            "Epoch 5, Loss: 1.972876\n",
            "Epoch 6, Loss: 1.906820\n",
            "Epoch 7, Loss: 1.774363\n",
            "Epoch 8, Loss: 1.721067\n",
            "Epoch 9, Loss: 1.644952\n",
            "Epoch 10, Loss: 1.551211\n",
            "Shadow model 84 trained.\n",
            "Epoch 1, Loss: 2.305425\n",
            "Epoch 2, Loss: 2.236527\n",
            "Epoch 3, Loss: 2.140629\n",
            "Epoch 4, Loss: 2.066231\n",
            "Epoch 5, Loss: 2.007193\n",
            "Epoch 6, Loss: 1.958714\n",
            "Epoch 7, Loss: 1.803460\n",
            "Epoch 8, Loss: 1.736514\n",
            "Epoch 9, Loss: 1.635631\n",
            "Epoch 10, Loss: 1.572265\n",
            "Shadow model 85 trained.\n",
            "Epoch 1, Loss: 2.301633\n",
            "Epoch 2, Loss: 2.160992\n",
            "Epoch 3, Loss: 2.079454\n",
            "Epoch 4, Loss: 2.028423\n",
            "Epoch 5, Loss: 1.885760\n",
            "Epoch 6, Loss: 1.832722\n",
            "Epoch 7, Loss: 1.721846\n",
            "Epoch 8, Loss: 1.623004\n",
            "Epoch 9, Loss: 1.600440\n",
            "Epoch 10, Loss: 1.554896\n",
            "Shadow model 86 trained.\n",
            "Epoch 1, Loss: 2.314875\n",
            "Epoch 2, Loss: 2.191936\n",
            "Epoch 3, Loss: 2.114799\n",
            "Epoch 4, Loss: 2.024519\n",
            "Epoch 5, Loss: 1.925153\n",
            "Epoch 6, Loss: 1.870746\n",
            "Epoch 7, Loss: 1.772981\n",
            "Epoch 8, Loss: 1.672256\n",
            "Epoch 9, Loss: 1.646594\n",
            "Epoch 10, Loss: 1.517147\n",
            "Shadow model 87 trained.\n",
            "Epoch 1, Loss: 2.296378\n",
            "Epoch 2, Loss: 2.163406\n",
            "Epoch 3, Loss: 2.106101\n",
            "Epoch 4, Loss: 2.043396\n",
            "Epoch 5, Loss: 1.948310\n",
            "Epoch 6, Loss: 1.864516\n",
            "Epoch 7, Loss: 1.819251\n",
            "Epoch 8, Loss: 1.746687\n",
            "Epoch 9, Loss: 1.649603\n",
            "Epoch 10, Loss: 1.511740\n",
            "Shadow model 88 trained.\n",
            "Epoch 1, Loss: 2.301060\n",
            "Epoch 2, Loss: 2.271433\n",
            "Epoch 3, Loss: 2.183204\n",
            "Epoch 4, Loss: 2.102085\n",
            "Epoch 5, Loss: 2.033141\n",
            "Epoch 6, Loss: 1.957362\n",
            "Epoch 7, Loss: 1.881778\n",
            "Epoch 8, Loss: 1.739616\n",
            "Epoch 9, Loss: 1.734637\n",
            "Epoch 10, Loss: 1.719630\n",
            "Shadow model 89 trained.\n",
            "Epoch 1, Loss: 2.311157\n",
            "Epoch 2, Loss: 2.225492\n",
            "Epoch 3, Loss: 2.154397\n",
            "Epoch 4, Loss: 2.067349\n",
            "Epoch 5, Loss: 1.922081\n",
            "Epoch 6, Loss: 1.896977\n",
            "Epoch 7, Loss: 1.811204\n",
            "Epoch 8, Loss: 1.651005\n",
            "Epoch 9, Loss: 1.536567\n",
            "Epoch 10, Loss: 1.461625\n",
            "Shadow model 90 trained.\n",
            "Epoch 1, Loss: 2.337317\n",
            "Epoch 2, Loss: 2.246448\n",
            "Epoch 3, Loss: 2.170439\n",
            "Epoch 4, Loss: 2.099567\n",
            "Epoch 5, Loss: 2.022232\n",
            "Epoch 6, Loss: 1.942198\n",
            "Epoch 7, Loss: 1.830885\n",
            "Epoch 8, Loss: 1.809578\n",
            "Epoch 9, Loss: 1.587569\n",
            "Epoch 10, Loss: 1.617013\n",
            "Shadow model 91 trained.\n",
            "Epoch 1, Loss: 2.294867\n",
            "Epoch 2, Loss: 2.147813\n",
            "Epoch 3, Loss: 2.086268\n",
            "Epoch 4, Loss: 2.010523\n",
            "Epoch 5, Loss: 1.977974\n",
            "Epoch 6, Loss: 1.872492\n",
            "Epoch 7, Loss: 1.799881\n",
            "Epoch 8, Loss: 1.705009\n",
            "Epoch 9, Loss: 1.658111\n",
            "Epoch 10, Loss: 1.568052\n",
            "Shadow model 92 trained.\n",
            "Epoch 1, Loss: 2.329867\n",
            "Epoch 2, Loss: 2.205143\n",
            "Epoch 3, Loss: 2.137442\n",
            "Epoch 4, Loss: 2.039609\n",
            "Epoch 5, Loss: 1.964123\n",
            "Epoch 6, Loss: 1.926745\n",
            "Epoch 7, Loss: 1.898718\n",
            "Epoch 8, Loss: 1.775595\n",
            "Epoch 9, Loss: 1.754074\n",
            "Epoch 10, Loss: 1.694203\n",
            "Shadow model 93 trained.\n",
            "Epoch 1, Loss: 2.298355\n",
            "Epoch 2, Loss: 2.164402\n",
            "Epoch 3, Loss: 2.103598\n",
            "Epoch 4, Loss: 2.052140\n",
            "Epoch 5, Loss: 2.013238\n",
            "Epoch 6, Loss: 1.985478\n",
            "Epoch 7, Loss: 1.873633\n",
            "Epoch 8, Loss: 1.855412\n",
            "Epoch 9, Loss: 1.784550\n",
            "Epoch 10, Loss: 1.777449\n",
            "Shadow model 94 trained.\n",
            "Epoch 1, Loss: 2.353578\n",
            "Epoch 2, Loss: 2.240655\n",
            "Epoch 3, Loss: 2.182618\n",
            "Epoch 4, Loss: 2.100173\n",
            "Epoch 5, Loss: 2.008053\n",
            "Epoch 6, Loss: 1.958210\n",
            "Epoch 7, Loss: 1.873321\n",
            "Epoch 8, Loss: 1.847302\n",
            "Epoch 9, Loss: 1.738746\n",
            "Epoch 10, Loss: 1.684970\n",
            "Shadow model 95 trained.\n",
            "Epoch 1, Loss: 2.315231\n",
            "Epoch 2, Loss: 2.173898\n",
            "Epoch 3, Loss: 2.106422\n",
            "Epoch 4, Loss: 2.008950\n",
            "Epoch 5, Loss: 1.952208\n",
            "Epoch 6, Loss: 1.853658\n",
            "Epoch 7, Loss: 1.739006\n",
            "Epoch 8, Loss: 1.665313\n",
            "Epoch 9, Loss: 1.603776\n",
            "Epoch 10, Loss: 1.568621\n",
            "Shadow model 96 trained.\n",
            "Epoch 1, Loss: 2.285323\n",
            "Epoch 2, Loss: 2.138183\n",
            "Epoch 3, Loss: 2.072652\n",
            "Epoch 4, Loss: 1.972417\n",
            "Epoch 5, Loss: 1.905387\n",
            "Epoch 6, Loss: 1.810443\n",
            "Epoch 7, Loss: 1.671855\n",
            "Epoch 8, Loss: 1.653930\n",
            "Epoch 9, Loss: 1.565310\n",
            "Epoch 10, Loss: 1.498338\n",
            "Shadow model 97 trained.\n",
            "Epoch 1, Loss: 2.323687\n",
            "Epoch 2, Loss: 2.160024\n",
            "Epoch 3, Loss: 2.072223\n",
            "Epoch 4, Loss: 1.939407\n",
            "Epoch 5, Loss: 1.941148\n",
            "Epoch 6, Loss: 1.822089\n",
            "Epoch 7, Loss: 1.760633\n",
            "Epoch 8, Loss: 1.712606\n",
            "Epoch 9, Loss: 1.631424\n",
            "Epoch 10, Loss: 1.519816\n",
            "Shadow model 98 trained.\n",
            "Epoch 1, Loss: 2.313541\n",
            "Epoch 2, Loss: 2.184811\n",
            "Epoch 3, Loss: 2.089279\n",
            "Epoch 4, Loss: 1.990626\n",
            "Epoch 5, Loss: 1.919894\n",
            "Epoch 6, Loss: 1.796555\n",
            "Epoch 7, Loss: 1.665023\n",
            "Epoch 8, Loss: 1.616071\n",
            "Epoch 9, Loss: 1.514140\n",
            "Epoch 10, Loss: 1.396813\n",
            "Shadow model 99 trained.\n",
            "Epoch 1, Loss: 2.320810\n",
            "Epoch 2, Loss: 2.192723\n",
            "Epoch 3, Loss: 2.095341\n",
            "Epoch 4, Loss: 2.085043\n",
            "Epoch 5, Loss: 1.999108\n",
            "Epoch 6, Loss: 1.960485\n",
            "Epoch 7, Loss: 1.861900\n",
            "Epoch 8, Loss: 1.881342\n",
            "Epoch 9, Loss: 1.741230\n",
            "Epoch 10, Loss: 1.691909\n",
            "Shadow model 100 trained.\n",
            "Epoch 1, Loss: 0.632176\n",
            "Epoch 2, Loss: 0.583218\n",
            "Epoch 3, Loss: 0.572371\n",
            "Epoch 4, Loss: 0.567676\n",
            "Epoch 5, Loss: 0.565416\n",
            "Epoch 6, Loss: 0.564404\n",
            "Epoch 7, Loss: 0.560717\n",
            "Epoch 8, Loss: 0.559179\n",
            "Epoch 9, Loss: 0.558465\n",
            "Epoch 10, Loss: 0.558620\n",
            "Attack model for class 0 trained.\n",
            "Epoch 1, Loss: 0.589117\n",
            "Epoch 2, Loss: 0.507185\n",
            "Epoch 3, Loss: 0.502655\n",
            "Epoch 4, Loss: 0.500839\n",
            "Epoch 5, Loss: 0.501427\n",
            "Epoch 6, Loss: 0.497970\n",
            "Epoch 7, Loss: 0.498096\n",
            "Epoch 8, Loss: 0.497011\n",
            "Epoch 9, Loss: 0.496520\n",
            "Epoch 10, Loss: 0.497121\n",
            "Attack model for class 1 trained.\n",
            "Epoch 1, Loss: 0.625733\n",
            "Epoch 2, Loss: 0.531437\n",
            "Epoch 3, Loss: 0.515327\n",
            "Epoch 4, Loss: 0.505372\n",
            "Epoch 5, Loss: 0.502392\n",
            "Epoch 6, Loss: 0.498832\n",
            "Epoch 7, Loss: 0.496150\n",
            "Epoch 8, Loss: 0.496205\n",
            "Epoch 9, Loss: 0.495114\n",
            "Epoch 10, Loss: 0.494522\n",
            "Attack model for class 2 trained.\n",
            "Epoch 1, Loss: 0.613592\n",
            "Epoch 2, Loss: 0.509662\n",
            "Epoch 3, Loss: 0.493795\n",
            "Epoch 4, Loss: 0.488736\n",
            "Epoch 5, Loss: 0.486717\n",
            "Epoch 6, Loss: 0.485332\n",
            "Epoch 7, Loss: 0.483380\n",
            "Epoch 8, Loss: 0.484304\n",
            "Epoch 9, Loss: 0.482168\n",
            "Epoch 10, Loss: 0.483077\n",
            "Attack model for class 3 trained.\n",
            "Epoch 1, Loss: 0.655533\n",
            "Epoch 2, Loss: 0.573728\n",
            "Epoch 3, Loss: 0.526705\n",
            "Epoch 4, Loss: 0.518330\n",
            "Epoch 5, Loss: 0.514589\n",
            "Epoch 6, Loss: 0.510767\n",
            "Epoch 7, Loss: 0.509600\n",
            "Epoch 8, Loss: 0.507771\n",
            "Epoch 9, Loss: 0.505636\n",
            "Epoch 10, Loss: 0.504616\n",
            "Attack model for class 4 trained.\n",
            "Epoch 1, Loss: 0.625522\n",
            "Epoch 2, Loss: 0.541815\n",
            "Epoch 3, Loss: 0.527889\n",
            "Epoch 4, Loss: 0.524156\n",
            "Epoch 5, Loss: 0.519922\n",
            "Epoch 6, Loss: 0.520050\n",
            "Epoch 7, Loss: 0.516844\n",
            "Epoch 8, Loss: 0.515630\n",
            "Epoch 9, Loss: 0.516679\n",
            "Epoch 10, Loss: 0.514660\n",
            "Attack model for class 5 trained.\n",
            "Epoch 1, Loss: 0.641708\n",
            "Epoch 2, Loss: 0.568257\n",
            "Epoch 3, Loss: 0.549504\n",
            "Epoch 4, Loss: 0.544437\n",
            "Epoch 5, Loss: 0.540798\n",
            "Epoch 6, Loss: 0.540889\n",
            "Epoch 7, Loss: 0.537138\n",
            "Epoch 8, Loss: 0.536746\n",
            "Epoch 9, Loss: 0.537596\n",
            "Epoch 10, Loss: 0.534799\n",
            "Attack model for class 6 trained.\n",
            "Epoch 1, Loss: 0.620076\n",
            "Epoch 2, Loss: 0.536535\n",
            "Epoch 3, Loss: 0.520938\n",
            "Epoch 4, Loss: 0.516441\n",
            "Epoch 5, Loss: 0.513083\n",
            "Epoch 6, Loss: 0.512215\n",
            "Epoch 7, Loss: 0.512117\n",
            "Epoch 8, Loss: 0.511412\n",
            "Epoch 9, Loss: 0.511031\n",
            "Epoch 10, Loss: 0.510547\n",
            "Attack model for class 7 trained.\n",
            "Epoch 1, Loss: 0.614653\n",
            "Epoch 2, Loss: 0.558978\n",
            "Epoch 3, Loss: 0.550227\n",
            "Epoch 4, Loss: 0.546929\n",
            "Epoch 5, Loss: 0.548244\n",
            "Epoch 6, Loss: 0.544821\n",
            "Epoch 7, Loss: 0.542905\n",
            "Epoch 8, Loss: 0.542159\n",
            "Epoch 9, Loss: 0.540232\n",
            "Epoch 10, Loss: 0.539110\n",
            "Attack model for class 8 trained.\n",
            "Epoch 1, Loss: 0.606144\n",
            "Epoch 2, Loss: 0.516744\n",
            "Epoch 3, Loss: 0.511834\n",
            "Epoch 4, Loss: 0.508436\n",
            "Epoch 5, Loss: 0.505646\n",
            "Epoch 6, Loss: 0.504909\n",
            "Epoch 7, Loss: 0.503612\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Accuracies"
      ],
      "metadata": {
        "id": "bkNHfpFq08Jq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_base = CIFAR10Classifier()\n",
        "model_base.load_state_dict(torch.load('baseline_model.pth', map_location=device))\n",
        "model_base.to(device)\n",
        "\n",
        "accuracy_baseline = mia.evaluate_attack_model(train_loader_baseline, test_loader_baseline, model_base)\n",
        "print(f'Accuracy for Attacking to the Baseline Model :  {accuracy_baseline * 100:.2f}%')"
      ],
      "metadata": {
        "id": "fiRttBeXYeJS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf86cba7-1e45-4b15-92cd-1788bf86d76f"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for Attacking to the Baseline Model :  78.23%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_module_prefix(state_dict):\n",
        "    new_state_dict = {}\n",
        "    for k, v in state_dict.items():\n",
        "        if k.startswith('_module.'):\n",
        "            new_state_dict[k[8:]] = v  # remove '_module.' prefix\n",
        "        else:\n",
        "            new_state_dict[k] = v\n",
        "    return new_state_dict\n",
        "\n",
        "model_private = CIFAR10Classifier().to(device)\n",
        "private_state_dict = torch.load('modified_model.pth', map_location=device)\n",
        "model_private.load_state_dict(remove_module_prefix(private_state_dict))\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model_private.to(device)\n",
        "\n",
        "accuracy_private = mia.evaluate_attack_model(train_loader_modified, test_loader_modified, model_private)\n",
        "print(f'Accuracy for Attacking to the Private Model :  {accuracy_private * 100:.2f}%')"
      ],
      "metadata": {
        "id": "RCOA_NbEY6pk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a31aa23e-5185-4f98-e134-de5cf002db44"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for Attacking to the Private Model :  63.15%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MLP with 9 Hidden Layers  :\n",
        "\n",
        "> Learning Rate (in training shadow models) : 0.001\n",
        "\n",
        "> Learning Rate (in training attacking models) : 0.001\n",
        "\n",
        "> Epochs (in training shadow models) : 10\n",
        "\n",
        "> Epochs (in training attacking models) : 50\n",
        "\n",
        "> Batch Size (in training shadow models) : 64\n",
        "\n",
        "> Batch Size (in training attacking models) : 64\n",
        "\n",
        "> num_shadow_models = 50\n",
        "\n",
        "> Optimizer (in training attacking models ) : Adam"
      ],
      "metadata": {
        "id": "bE9i2VzKZKNY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Subset, ConcatDataset\n",
        "from model import CIFAR10Classifier\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class AttackModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AttackModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(10, 64)\n",
        "        self.fc2 = nn.Linear(64, 128)\n",
        "        self.fc3 = nn.Linear(128, 256)\n",
        "        self.fc4 = nn.Linear(256, 512)\n",
        "        self.fc5 = nn.Linear(512, 256)\n",
        "        self.fc6 = nn.Linear(256, 128)\n",
        "        self.fc7 = nn.Linear(128, 64)\n",
        "        self.fc8 = nn.Linear(64, 32)\n",
        "        self.fc9 = nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = F.relu(self.fc4(x))\n",
        "        x = F.relu(self.fc5(x))\n",
        "        x = F.relu(self.fc6(x))\n",
        "        x = F.relu(self.fc7(x))\n",
        "        x = F.relu(self.fc8(x))\n",
        "        x = torch.sigmoid(self.fc9(x))\n",
        "        return x\n",
        "\n",
        "class MembershipInferenceAttackNoPrivacy:\n",
        "    def __init__(self, shadow_model_class, attack_model_class, device='cpu'):\n",
        "        self.shadow_model_class = shadow_model_class\n",
        "        self.attack_model_class = attack_model_class\n",
        "        self.device = device\n",
        "        self.attack_models = {}\n",
        "\n",
        "    def train_shadow_models(self, seen_loaders, num_epochs=10, lr=1e-3):\n",
        "        self.shadow_models = [self.shadow_model_class().to(self.device) for _ in range(len(seen_loaders))]\n",
        "\n",
        "        for i, (shadow_model, seen_loader) in enumerate(zip(self.shadow_models, seen_loaders)):\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "            optimizer = optim.Adam(shadow_model.parameters(), lr=lr)\n",
        "            self._train_model(shadow_model, seen_loader, criterion, optimizer, num_epochs)\n",
        "            print(f'Shadow model {i+1} trained.')\n",
        "\n",
        "    def _train_model(self, model, dataloader, criterion, optimizer, num_epochs):\n",
        "        model.train()\n",
        "        for epoch in range(num_epochs):\n",
        "            running_loss = 0.0\n",
        "            for inputs, labels in dataloader:\n",
        "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                running_loss += loss.item()\n",
        "            print(f'Epoch {epoch+1}, Loss: {running_loss/len(dataloader):.6f}')\n",
        "\n",
        "    def collect_outputs(self, seen_loaders, unseen_loaders):\n",
        "        self.attack_data = []\n",
        "        self.attack_labels = []\n",
        "\n",
        "        for shadow_model, seen_loader, unseen_loader in zip(self.shadow_models, seen_loaders, unseen_loaders):\n",
        "            self._collect_shadow_model_outputs(shadow_model, seen_loader, label=1)  # in\n",
        "            self._collect_shadow_model_outputs(shadow_model, unseen_loader, label=0)  # out\n",
        "\n",
        "        self.attack_data = torch.cat(self.attack_data).to(self.device)\n",
        "        self.attack_labels = torch.cat(self.attack_labels).to(self.device)\n",
        "\n",
        "    def _collect_shadow_model_outputs(self, model, dataloader, label):\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in dataloader:\n",
        "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
        "                outputs = model(inputs)\n",
        "                probabilities = F.softmax(outputs, dim=1)\n",
        "                self.attack_data.append(torch.cat([probabilities, labels.unsqueeze(1).float()], dim=1))\n",
        "                self.attack_labels.append(torch.full((outputs.size(0),), label, dtype=torch.float).to(self.device))\n",
        "\n",
        "    def train_attack_models(self, num_epochs=10, lr=0.001):\n",
        "        attack_dataset = torch.utils.data.TensorDataset(self.attack_data, self.attack_labels)\n",
        "        for class_label in range(10):\n",
        "            class_indices = (self.attack_data[:, -1] == class_label).nonzero().squeeze()\n",
        "            class_data = self.attack_data[class_indices][:, :-1]\n",
        "            class_labels = self.attack_labels[class_indices].view(-1, 1)\n",
        "\n",
        "\n",
        "            attack_dataset = torch.utils.data.TensorDataset(class_data, class_labels)\n",
        "            attack_loader = DataLoader(attack_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "            attack_model = self.attack_model_class().to(self.device)\n",
        "            criterion = nn.BCELoss()\n",
        "            optimizer = optim.Adam(attack_model.parameters(), lr=lr)\n",
        "\n",
        "            self._train_model(attack_model, attack_loader, criterion, optimizer, num_epochs)\n",
        "            self.attack_models[class_label] = attack_model\n",
        "\n",
        "            print(f'Attack model for class {class_label} trained.')\n",
        "\n",
        "    def save_attack_models(self, path):\n",
        "        for class_label, model in self.attack_models.items():\n",
        "            torch.save(model.state_dict(), f'{path}_class_{class_label}.pth')\n",
        "            print(f'Attack model for class {class_label} saved to {path}_class_{class_label}.pth')\n",
        "\n",
        "    def load_attack_models(self, path):\n",
        "        for class_label in range(10):\n",
        "            model = self.attack_model_class().to(self.device)\n",
        "            model.load_state_dict(torch.load(f'{path}_class_{class_label}.pth', map_location=self.device))\n",
        "            self.attack_models[class_label] = model\n",
        "            print(f'Attack model for class {class_label} loaded from {path}_class_{class_label}.pth')\n",
        "\n",
        "    def infer_membership(self, model, seen_loader, unseen_loader , seen_outputs , unseen_outputs , labels):\n",
        "\n",
        "        model_outputs = torch.cat([seen_outputs, unseen_outputs]).to(self.device)\n",
        "\n",
        "        labels = labels.to(self.device)\n",
        "\n",
        "        memberships = []\n",
        "        for output, label in zip(model_outputs, labels):\n",
        "            class_label = label.item()\n",
        "            attack_model = self.attack_models[class_label]\n",
        "            membership_pred = attack_model(output.unsqueeze(0)).item()\n",
        "            memberships.append(membership_pred)\n",
        "        return torch.tensor(memberships, device=self.device)\n",
        "\n",
        "    def evaluate_attack_model(self, seen_loader, unseen_loader, target_model):\n",
        "        seen_outputs , lables_seen= self._get_model_outputs(target_model, seen_loader)\n",
        "        unseen_outputs , labels_unseen = self._get_model_outputs(target_model, unseen_loader)\n",
        "\n",
        "        attack_data = torch.cat([seen_outputs, unseen_outputs]).to(self.device)\n",
        "        attack_labels = torch.cat([torch.ones(len(seen_outputs)), torch.zeros(len(unseen_outputs))]).to(self.device)\n",
        "\n",
        "        labels = torch.cat([lables_seen, labels_unseen]).to(self.device)\n",
        "\n",
        "        memberships = self.infer_membership(target_model, seen_loader, unseen_loader , seen_outputs , unseen_outputs , labels)\n",
        "        membership_preds = (memberships > 0.5).float()\n",
        "        accuracy = (membership_preds == attack_labels).float().mean().item()\n",
        "        return accuracy\n",
        "\n",
        "    def _get_model_outputs(self, model, dataloader):\n",
        "        model.eval()\n",
        "        outputs_list = []\n",
        "        labels_list = []\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in dataloader:\n",
        "                inputs = inputs.to(self.device)\n",
        "                outputs = model(inputs)\n",
        "                probabilities = F.softmax(outputs, dim=1)\n",
        "                outputs_list.append(probabilities)\n",
        "                labels_list.append(labels) # Convert labels to list for easy concatenation\n",
        "        return torch.cat(outputs_list), torch.cat(labels_list)"
      ],
      "metadata": {
        "id": "rxe0Dv40ZR7J"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
        "train_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_size = int(0.8 * len(train_set))\n",
        "remaining_size = len(train_set) - train_size\n",
        "train_subset, remaining_subset = torch.utils.data.random_split(train_set, [train_size, remaining_size])\n",
        "\n",
        "\n",
        "num_shadow_models = 50\n",
        "seen_size_per_model = train_size // num_shadow_models\n",
        "seen_loaders = []\n",
        "\n",
        "for i in range(num_shadow_models):\n",
        "    start_idx = i * seen_size_per_model\n",
        "    end_idx = (i + 1) * seen_size_per_model\n",
        "    seen_indices = torch.arange(start_idx, end_idx)\n",
        "    seen_train_set = Subset(train_subset, seen_indices)\n",
        "    seen_loader = DataLoader(seen_train_set, batch_size=64, shuffle=True)\n",
        "    seen_loaders.append(seen_loader)\n",
        "\n",
        "unseen_dataset = ConcatDataset([remaining_subset, test_set])\n",
        "unseen_size_per_model = len(unseen_dataset) // num_shadow_models\n",
        "unseen_loaders = []\n",
        "\n",
        "for i in range(num_shadow_models):\n",
        "    start_idx = i * unseen_size_per_model\n",
        "    end_idx = (i + 1) * unseen_size_per_model\n",
        "    unseen_indices = torch.arange(start_idx, end_idx)\n",
        "    unseen_subset = Subset(unseen_dataset, unseen_indices)\n",
        "    unseen_loader = DataLoader(unseen_subset, batch_size=64, shuffle=False)\n",
        "    unseen_loaders.append(unseen_loader)\n",
        "\n",
        "test_loader = DataLoader(test_set, batch_size=10, shuffle=False)\n",
        "\n",
        "# Initialize MembershipInferenceAttackNoPrivacy\n",
        "mia = MembershipInferenceAttackNoPrivacy(CIFAR10Classifier, AttackModel, device)\n",
        "\n",
        "# Train shadow models without differential privacy\n",
        "mia.train_shadow_models(seen_loaders, num_epochs=10)\n",
        "\n",
        "# Collect outputs for attack model\n",
        "mia.collect_outputs(seen_loaders, unseen_loaders)\n",
        "\n",
        "# Train attack models\n",
        "mia.train_attack_models(num_epochs=50)\n",
        "\n",
        "# Save the attack models\n",
        "mia.save_attack_models('attack_model')\n",
        "\n",
        "# Load the attack models (for future use)\n",
        "mia.load_attack_models('attack_model')"
      ],
      "metadata": {
        "id": "Sv5QrxLVZai_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3c73b7b-e67f-4a80-ed3a-02ab3cacbe18"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch 1, Loss: 2.280746\n",
            "Epoch 2, Loss: 2.118312\n",
            "Epoch 3, Loss: 2.045994\n",
            "Epoch 4, Loss: 1.965019\n",
            "Epoch 5, Loss: 1.906267\n",
            "Epoch 6, Loss: 1.838629\n",
            "Epoch 7, Loss: 1.746467\n",
            "Epoch 8, Loss: 1.635649\n",
            "Epoch 9, Loss: 1.630207\n",
            "Epoch 10, Loss: 1.516875\n",
            "Shadow model 1 trained.\n",
            "Epoch 1, Loss: 2.287084\n",
            "Epoch 2, Loss: 2.152396\n",
            "Epoch 3, Loss: 2.079041\n",
            "Epoch 4, Loss: 1.936000\n",
            "Epoch 5, Loss: 1.813020\n",
            "Epoch 6, Loss: 1.771416\n",
            "Epoch 7, Loss: 1.660080\n",
            "Epoch 8, Loss: 1.550930\n",
            "Epoch 9, Loss: 1.416992\n",
            "Epoch 10, Loss: 1.319677\n",
            "Shadow model 2 trained.\n",
            "Epoch 1, Loss: 2.268193\n",
            "Epoch 2, Loss: 2.115432\n",
            "Epoch 3, Loss: 2.009951\n",
            "Epoch 4, Loss: 1.897934\n",
            "Epoch 5, Loss: 1.796283\n",
            "Epoch 6, Loss: 1.709851\n",
            "Epoch 7, Loss: 1.609906\n",
            "Epoch 8, Loss: 1.525193\n",
            "Epoch 9, Loss: 1.474293\n",
            "Epoch 10, Loss: 1.378292\n",
            "Shadow model 3 trained.\n",
            "Epoch 1, Loss: 2.310143\n",
            "Epoch 2, Loss: 2.208501\n",
            "Epoch 3, Loss: 2.122663\n",
            "Epoch 4, Loss: 2.001128\n",
            "Epoch 5, Loss: 1.931343\n",
            "Epoch 6, Loss: 1.831597\n",
            "Epoch 7, Loss: 1.733170\n",
            "Epoch 8, Loss: 1.641346\n",
            "Epoch 9, Loss: 1.518114\n",
            "Epoch 10, Loss: 1.399215\n",
            "Shadow model 4 trained.\n",
            "Epoch 1, Loss: 2.277824\n",
            "Epoch 2, Loss: 2.110615\n",
            "Epoch 3, Loss: 1.981151\n",
            "Epoch 4, Loss: 1.879250\n",
            "Epoch 5, Loss: 1.813713\n",
            "Epoch 6, Loss: 1.693825\n",
            "Epoch 7, Loss: 1.613546\n",
            "Epoch 8, Loss: 1.462790\n",
            "Epoch 9, Loss: 1.330940\n",
            "Epoch 10, Loss: 1.254434\n",
            "Shadow model 5 trained.\n",
            "Epoch 1, Loss: 2.296413\n",
            "Epoch 2, Loss: 2.155601\n",
            "Epoch 3, Loss: 2.081356\n",
            "Epoch 4, Loss: 1.963164\n",
            "Epoch 5, Loss: 1.862661\n",
            "Epoch 6, Loss: 1.782735\n",
            "Epoch 7, Loss: 1.689563\n",
            "Epoch 8, Loss: 1.617643\n",
            "Epoch 9, Loss: 1.460892\n",
            "Epoch 10, Loss: 1.356422\n",
            "Shadow model 6 trained.\n",
            "Epoch 1, Loss: 2.225877\n",
            "Epoch 2, Loss: 2.040748\n",
            "Epoch 3, Loss: 1.897107\n",
            "Epoch 4, Loss: 1.785130\n",
            "Epoch 5, Loss: 1.643586\n",
            "Epoch 6, Loss: 1.576514\n",
            "Epoch 7, Loss: 1.456872\n",
            "Epoch 8, Loss: 1.311798\n",
            "Epoch 9, Loss: 1.208633\n",
            "Epoch 10, Loss: 1.120558\n",
            "Shadow model 7 trained.\n",
            "Epoch 1, Loss: 2.314676\n",
            "Epoch 2, Loss: 2.188518\n",
            "Epoch 3, Loss: 2.070897\n",
            "Epoch 4, Loss: 2.009579\n",
            "Epoch 5, Loss: 1.916002\n",
            "Epoch 6, Loss: 1.769008\n",
            "Epoch 7, Loss: 1.684753\n",
            "Epoch 8, Loss: 1.575501\n",
            "Epoch 9, Loss: 1.495916\n",
            "Epoch 10, Loss: 1.392649\n",
            "Shadow model 8 trained.\n",
            "Epoch 1, Loss: 2.305805\n",
            "Epoch 2, Loss: 2.203001\n",
            "Epoch 3, Loss: 2.098259\n",
            "Epoch 4, Loss: 1.999231\n",
            "Epoch 5, Loss: 1.938060\n",
            "Epoch 6, Loss: 1.865460\n",
            "Epoch 7, Loss: 1.778089\n",
            "Epoch 8, Loss: 1.662783\n",
            "Epoch 9, Loss: 1.577277\n",
            "Epoch 10, Loss: 1.523490\n",
            "Shadow model 9 trained.\n",
            "Epoch 1, Loss: 2.302628\n",
            "Epoch 2, Loss: 2.199909\n",
            "Epoch 3, Loss: 2.084829\n",
            "Epoch 4, Loss: 2.019547\n",
            "Epoch 5, Loss: 1.908516\n",
            "Epoch 6, Loss: 1.824168\n",
            "Epoch 7, Loss: 1.708899\n",
            "Epoch 8, Loss: 1.626828\n",
            "Epoch 9, Loss: 1.584347\n",
            "Epoch 10, Loss: 1.439324\n",
            "Shadow model 10 trained.\n",
            "Epoch 1, Loss: 2.299393\n",
            "Epoch 2, Loss: 2.158154\n",
            "Epoch 3, Loss: 2.080407\n",
            "Epoch 4, Loss: 2.014505\n",
            "Epoch 5, Loss: 1.867278\n",
            "Epoch 6, Loss: 1.774606\n",
            "Epoch 7, Loss: 1.697373\n",
            "Epoch 8, Loss: 1.623910\n",
            "Epoch 9, Loss: 1.531656\n",
            "Epoch 10, Loss: 1.449579\n",
            "Shadow model 11 trained.\n",
            "Epoch 1, Loss: 2.277418\n",
            "Epoch 2, Loss: 2.131063\n",
            "Epoch 3, Loss: 1.993102\n",
            "Epoch 4, Loss: 1.936699\n",
            "Epoch 5, Loss: 1.816143\n",
            "Epoch 6, Loss: 1.687007\n",
            "Epoch 7, Loss: 1.578235\n",
            "Epoch 8, Loss: 1.547600\n",
            "Epoch 9, Loss: 1.405106\n",
            "Epoch 10, Loss: 1.345550\n",
            "Shadow model 12 trained.\n",
            "Epoch 1, Loss: 2.266559\n",
            "Epoch 2, Loss: 2.145939\n",
            "Epoch 3, Loss: 2.020989\n",
            "Epoch 4, Loss: 1.957759\n",
            "Epoch 5, Loss: 1.863998\n",
            "Epoch 6, Loss: 1.792524\n",
            "Epoch 7, Loss: 1.714517\n",
            "Epoch 8, Loss: 1.620949\n",
            "Epoch 9, Loss: 1.560344\n",
            "Epoch 10, Loss: 1.471165\n",
            "Shadow model 13 trained.\n",
            "Epoch 1, Loss: 2.268302\n",
            "Epoch 2, Loss: 2.120059\n",
            "Epoch 3, Loss: 1.966055\n",
            "Epoch 4, Loss: 1.904693\n",
            "Epoch 5, Loss: 1.774498\n",
            "Epoch 6, Loss: 1.617787\n",
            "Epoch 7, Loss: 1.557424\n",
            "Epoch 8, Loss: 1.385376\n",
            "Epoch 9, Loss: 1.305972\n",
            "Epoch 10, Loss: 1.240215\n",
            "Shadow model 14 trained.\n",
            "Epoch 1, Loss: 2.289968\n",
            "Epoch 2, Loss: 2.126971\n",
            "Epoch 3, Loss: 2.088956\n",
            "Epoch 4, Loss: 1.984489\n",
            "Epoch 5, Loss: 1.882704\n",
            "Epoch 6, Loss: 1.760244\n",
            "Epoch 7, Loss: 1.704601\n",
            "Epoch 8, Loss: 1.558690\n",
            "Epoch 9, Loss: 1.512323\n",
            "Epoch 10, Loss: 1.441935\n",
            "Shadow model 15 trained.\n",
            "Epoch 1, Loss: 2.296160\n",
            "Epoch 2, Loss: 2.133890\n",
            "Epoch 3, Loss: 2.004636\n",
            "Epoch 4, Loss: 1.961238\n",
            "Epoch 5, Loss: 1.841482\n",
            "Epoch 6, Loss: 1.752438\n",
            "Epoch 7, Loss: 1.664876\n",
            "Epoch 8, Loss: 1.569563\n",
            "Epoch 9, Loss: 1.488842\n",
            "Epoch 10, Loss: 1.427485\n",
            "Shadow model 16 trained.\n",
            "Epoch 1, Loss: 2.275762\n",
            "Epoch 2, Loss: 2.122700\n",
            "Epoch 3, Loss: 1.983029\n",
            "Epoch 4, Loss: 1.907477\n",
            "Epoch 5, Loss: 1.844491\n",
            "Epoch 6, Loss: 1.786252\n",
            "Epoch 7, Loss: 1.690264\n",
            "Epoch 8, Loss: 1.616761\n",
            "Epoch 9, Loss: 1.513572\n",
            "Epoch 10, Loss: 1.417473\n",
            "Shadow model 17 trained.\n",
            "Epoch 1, Loss: 2.270372\n",
            "Epoch 2, Loss: 2.132096\n",
            "Epoch 3, Loss: 2.049912\n",
            "Epoch 4, Loss: 1.962561\n",
            "Epoch 5, Loss: 1.901972\n",
            "Epoch 6, Loss: 1.778764\n",
            "Epoch 7, Loss: 1.697877\n",
            "Epoch 8, Loss: 1.626817\n",
            "Epoch 9, Loss: 1.557564\n",
            "Epoch 10, Loss: 1.515464\n",
            "Shadow model 18 trained.\n",
            "Epoch 1, Loss: 2.276882\n",
            "Epoch 2, Loss: 2.138956\n",
            "Epoch 3, Loss: 2.049828\n",
            "Epoch 4, Loss: 1.972189\n",
            "Epoch 5, Loss: 1.857876\n",
            "Epoch 6, Loss: 1.825211\n",
            "Epoch 7, Loss: 1.733845\n",
            "Epoch 8, Loss: 1.670894\n",
            "Epoch 9, Loss: 1.619820\n",
            "Epoch 10, Loss: 1.552961\n",
            "Shadow model 19 trained.\n",
            "Epoch 1, Loss: 2.270837\n",
            "Epoch 2, Loss: 2.138282\n",
            "Epoch 3, Loss: 2.042887\n",
            "Epoch 4, Loss: 1.892710\n",
            "Epoch 5, Loss: 1.822999\n",
            "Epoch 6, Loss: 1.702532\n",
            "Epoch 7, Loss: 1.639075\n",
            "Epoch 8, Loss: 1.549085\n",
            "Epoch 9, Loss: 1.428237\n",
            "Epoch 10, Loss: 1.424521\n",
            "Shadow model 20 trained.\n",
            "Epoch 1, Loss: 2.270914\n",
            "Epoch 2, Loss: 2.091759\n",
            "Epoch 3, Loss: 2.006499\n",
            "Epoch 4, Loss: 1.915053\n",
            "Epoch 5, Loss: 1.790882\n",
            "Epoch 6, Loss: 1.698631\n",
            "Epoch 7, Loss: 1.613857\n",
            "Epoch 8, Loss: 1.561243\n",
            "Epoch 9, Loss: 1.450337\n",
            "Epoch 10, Loss: 1.403682\n",
            "Shadow model 21 trained.\n",
            "Epoch 1, Loss: 2.287689\n",
            "Epoch 2, Loss: 2.141344\n",
            "Epoch 3, Loss: 2.017169\n",
            "Epoch 4, Loss: 1.924913\n",
            "Epoch 5, Loss: 1.825497\n",
            "Epoch 6, Loss: 1.731810\n",
            "Epoch 7, Loss: 1.615185\n",
            "Epoch 8, Loss: 1.540080\n",
            "Epoch 9, Loss: 1.403443\n",
            "Epoch 10, Loss: 1.315389\n",
            "Shadow model 22 trained.\n",
            "Epoch 1, Loss: 2.304863\n",
            "Epoch 2, Loss: 2.190906\n",
            "Epoch 3, Loss: 2.113725\n",
            "Epoch 4, Loss: 2.034973\n",
            "Epoch 5, Loss: 1.981188\n",
            "Epoch 6, Loss: 1.890345\n",
            "Epoch 7, Loss: 1.805208\n",
            "Epoch 8, Loss: 1.716391\n",
            "Epoch 9, Loss: 1.603394\n",
            "Epoch 10, Loss: 1.539797\n",
            "Shadow model 23 trained.\n",
            "Epoch 1, Loss: 2.282044\n",
            "Epoch 2, Loss: 2.125131\n",
            "Epoch 3, Loss: 2.078657\n",
            "Epoch 4, Loss: 1.970654\n",
            "Epoch 5, Loss: 1.874074\n",
            "Epoch 6, Loss: 1.796489\n",
            "Epoch 7, Loss: 1.728652\n",
            "Epoch 8, Loss: 1.622742\n",
            "Epoch 9, Loss: 1.586315\n",
            "Epoch 10, Loss: 1.444410\n",
            "Shadow model 24 trained.\n",
            "Epoch 1, Loss: 2.244377\n",
            "Epoch 2, Loss: 2.082389\n",
            "Epoch 3, Loss: 1.961758\n",
            "Epoch 4, Loss: 1.831181\n",
            "Epoch 5, Loss: 1.739027\n",
            "Epoch 6, Loss: 1.684625\n",
            "Epoch 7, Loss: 1.573317\n",
            "Epoch 8, Loss: 1.465318\n",
            "Epoch 9, Loss: 1.349827\n",
            "Epoch 10, Loss: 1.269208\n",
            "Shadow model 25 trained.\n",
            "Epoch 1, Loss: 2.294185\n",
            "Epoch 2, Loss: 2.152807\n",
            "Epoch 3, Loss: 2.078700\n",
            "Epoch 4, Loss: 2.025266\n",
            "Epoch 5, Loss: 1.934003\n",
            "Epoch 6, Loss: 1.850600\n",
            "Epoch 7, Loss: 1.733453\n",
            "Epoch 8, Loss: 1.711554\n",
            "Epoch 9, Loss: 1.608193\n",
            "Epoch 10, Loss: 1.531220\n",
            "Shadow model 26 trained.\n",
            "Epoch 1, Loss: 2.312421\n",
            "Epoch 2, Loss: 2.167099\n",
            "Epoch 3, Loss: 2.093050\n",
            "Epoch 4, Loss: 2.012249\n",
            "Epoch 5, Loss: 1.947529\n",
            "Epoch 6, Loss: 1.844997\n",
            "Epoch 7, Loss: 1.764286\n",
            "Epoch 8, Loss: 1.634432\n",
            "Epoch 9, Loss: 1.555195\n",
            "Epoch 10, Loss: 1.464538\n",
            "Shadow model 27 trained.\n",
            "Epoch 1, Loss: 2.302117\n",
            "Epoch 2, Loss: 2.179638\n",
            "Epoch 3, Loss: 2.084868\n",
            "Epoch 4, Loss: 1.977134\n",
            "Epoch 5, Loss: 1.822446\n",
            "Epoch 6, Loss: 1.784667\n",
            "Epoch 7, Loss: 1.663254\n",
            "Epoch 8, Loss: 1.544880\n",
            "Epoch 9, Loss: 1.445062\n",
            "Epoch 10, Loss: 1.362132\n",
            "Shadow model 28 trained.\n",
            "Epoch 1, Loss: 2.252040\n",
            "Epoch 2, Loss: 2.100334\n",
            "Epoch 3, Loss: 2.011853\n",
            "Epoch 4, Loss: 1.906618\n",
            "Epoch 5, Loss: 1.803209\n",
            "Epoch 6, Loss: 1.692641\n",
            "Epoch 7, Loss: 1.661081\n",
            "Epoch 8, Loss: 1.541112\n",
            "Epoch 9, Loss: 1.446206\n",
            "Epoch 10, Loss: 1.382007\n",
            "Shadow model 29 trained.\n",
            "Epoch 1, Loss: 2.252771\n",
            "Epoch 2, Loss: 2.159751\n",
            "Epoch 3, Loss: 2.072594\n",
            "Epoch 4, Loss: 2.036733\n",
            "Epoch 5, Loss: 1.948484\n",
            "Epoch 6, Loss: 1.894122\n",
            "Epoch 7, Loss: 1.806346\n",
            "Epoch 8, Loss: 1.778677\n",
            "Epoch 9, Loss: 1.723445\n",
            "Epoch 10, Loss: 1.636866\n",
            "Shadow model 30 trained.\n",
            "Epoch 1, Loss: 2.292288\n",
            "Epoch 2, Loss: 2.183940\n",
            "Epoch 3, Loss: 2.097331\n",
            "Epoch 4, Loss: 2.022978\n",
            "Epoch 5, Loss: 1.966796\n",
            "Epoch 6, Loss: 1.862100\n",
            "Epoch 7, Loss: 1.752702\n",
            "Epoch 8, Loss: 1.666265\n",
            "Epoch 9, Loss: 1.579341\n",
            "Epoch 10, Loss: 1.477338\n",
            "Shadow model 31 trained.\n",
            "Epoch 1, Loss: 2.284524\n",
            "Epoch 2, Loss: 2.195883\n",
            "Epoch 3, Loss: 2.064438\n",
            "Epoch 4, Loss: 1.970607\n",
            "Epoch 5, Loss: 1.830595\n",
            "Epoch 6, Loss: 1.797207\n",
            "Epoch 7, Loss: 1.705419\n",
            "Epoch 8, Loss: 1.603154\n",
            "Epoch 9, Loss: 1.518342\n",
            "Epoch 10, Loss: 1.435807\n",
            "Shadow model 32 trained.\n",
            "Epoch 1, Loss: 2.261758\n",
            "Epoch 2, Loss: 2.107928\n",
            "Epoch 3, Loss: 2.008198\n",
            "Epoch 4, Loss: 1.885082\n",
            "Epoch 5, Loss: 1.745333\n",
            "Epoch 6, Loss: 1.657586\n",
            "Epoch 7, Loss: 1.588819\n",
            "Epoch 8, Loss: 1.426014\n",
            "Epoch 9, Loss: 1.350655\n",
            "Epoch 10, Loss: 1.261169\n",
            "Shadow model 33 trained.\n",
            "Epoch 1, Loss: 2.263335\n",
            "Epoch 2, Loss: 2.084344\n",
            "Epoch 3, Loss: 1.979951\n",
            "Epoch 4, Loss: 1.892551\n",
            "Epoch 5, Loss: 1.818664\n",
            "Epoch 6, Loss: 1.727902\n",
            "Epoch 7, Loss: 1.614347\n",
            "Epoch 8, Loss: 1.544412\n",
            "Epoch 9, Loss: 1.417712\n",
            "Epoch 10, Loss: 1.356268\n",
            "Shadow model 34 trained.\n",
            "Epoch 1, Loss: 2.309561\n",
            "Epoch 2, Loss: 2.189913\n",
            "Epoch 3, Loss: 2.109172\n",
            "Epoch 4, Loss: 2.011014\n",
            "Epoch 5, Loss: 1.924287\n",
            "Epoch 6, Loss: 1.812002\n",
            "Epoch 7, Loss: 1.692616\n",
            "Epoch 8, Loss: 1.652722\n",
            "Epoch 9, Loss: 1.550053\n",
            "Epoch 10, Loss: 1.471514\n",
            "Shadow model 35 trained.\n",
            "Epoch 1, Loss: 2.271872\n",
            "Epoch 2, Loss: 2.159219\n",
            "Epoch 3, Loss: 2.031466\n",
            "Epoch 4, Loss: 1.918235\n",
            "Epoch 5, Loss: 1.837722\n",
            "Epoch 6, Loss: 1.711232\n",
            "Epoch 7, Loss: 1.607021\n",
            "Epoch 8, Loss: 1.536399\n",
            "Epoch 9, Loss: 1.410476\n",
            "Epoch 10, Loss: 1.345624\n",
            "Shadow model 36 trained.\n",
            "Epoch 1, Loss: 2.271194\n",
            "Epoch 2, Loss: 2.130038\n",
            "Epoch 3, Loss: 2.020264\n",
            "Epoch 4, Loss: 1.951514\n",
            "Epoch 5, Loss: 1.839708\n",
            "Epoch 6, Loss: 1.741804\n",
            "Epoch 7, Loss: 1.581737\n",
            "Epoch 8, Loss: 1.551913\n",
            "Epoch 9, Loss: 1.348008\n",
            "Epoch 10, Loss: 1.283136\n",
            "Shadow model 37 trained.\n",
            "Epoch 1, Loss: 2.304363\n",
            "Epoch 2, Loss: 2.207957\n",
            "Epoch 3, Loss: 2.069315\n",
            "Epoch 4, Loss: 1.952252\n",
            "Epoch 5, Loss: 1.848507\n",
            "Epoch 6, Loss: 1.733735\n",
            "Epoch 7, Loss: 1.653686\n",
            "Epoch 8, Loss: 1.527352\n",
            "Epoch 9, Loss: 1.386706\n",
            "Epoch 10, Loss: 1.362417\n",
            "Shadow model 38 trained.\n",
            "Epoch 1, Loss: 2.255443\n",
            "Epoch 2, Loss: 2.094756\n",
            "Epoch 3, Loss: 1.970378\n",
            "Epoch 4, Loss: 1.849052\n",
            "Epoch 5, Loss: 1.726576\n",
            "Epoch 6, Loss: 1.617524\n",
            "Epoch 7, Loss: 1.508586\n",
            "Epoch 8, Loss: 1.447567\n",
            "Epoch 9, Loss: 1.353930\n",
            "Epoch 10, Loss: 1.247900\n",
            "Shadow model 39 trained.\n",
            "Epoch 1, Loss: 2.260076\n",
            "Epoch 2, Loss: 2.103039\n",
            "Epoch 3, Loss: 2.006715\n",
            "Epoch 4, Loss: 1.919338\n",
            "Epoch 5, Loss: 1.860089\n",
            "Epoch 6, Loss: 1.783913\n",
            "Epoch 7, Loss: 1.692970\n",
            "Epoch 8, Loss: 1.603696\n",
            "Epoch 9, Loss: 1.534011\n",
            "Epoch 10, Loss: 1.451357\n",
            "Shadow model 40 trained.\n",
            "Epoch 1, Loss: 2.311106\n",
            "Epoch 2, Loss: 2.200036\n",
            "Epoch 3, Loss: 2.073269\n",
            "Epoch 4, Loss: 2.003702\n",
            "Epoch 5, Loss: 1.896103\n",
            "Epoch 6, Loss: 1.759320\n",
            "Epoch 7, Loss: 1.677194\n",
            "Epoch 8, Loss: 1.657650\n",
            "Epoch 9, Loss: 1.494674\n",
            "Epoch 10, Loss: 1.379589\n",
            "Shadow model 41 trained.\n",
            "Epoch 1, Loss: 2.251052\n",
            "Epoch 2, Loss: 2.106050\n",
            "Epoch 3, Loss: 1.976618\n",
            "Epoch 4, Loss: 1.849302\n",
            "Epoch 5, Loss: 1.712599\n",
            "Epoch 6, Loss: 1.574817\n",
            "Epoch 7, Loss: 1.539606\n",
            "Epoch 8, Loss: 1.412115\n",
            "Epoch 9, Loss: 1.297555\n",
            "Epoch 10, Loss: 1.257338\n",
            "Shadow model 42 trained.\n",
            "Epoch 1, Loss: 2.264668\n",
            "Epoch 2, Loss: 2.110706\n",
            "Epoch 3, Loss: 2.003602\n",
            "Epoch 4, Loss: 1.883253\n",
            "Epoch 5, Loss: 1.806612\n",
            "Epoch 6, Loss: 1.698331\n",
            "Epoch 7, Loss: 1.602082\n",
            "Epoch 8, Loss: 1.523596\n",
            "Epoch 9, Loss: 1.387174\n",
            "Epoch 10, Loss: 1.322163\n",
            "Shadow model 43 trained.\n",
            "Epoch 1, Loss: 2.275656\n",
            "Epoch 2, Loss: 2.175820\n",
            "Epoch 3, Loss: 2.065681\n",
            "Epoch 4, Loss: 1.964780\n",
            "Epoch 5, Loss: 1.887607\n",
            "Epoch 6, Loss: 1.753843\n",
            "Epoch 7, Loss: 1.696064\n",
            "Epoch 8, Loss: 1.615397\n",
            "Epoch 9, Loss: 1.556100\n",
            "Epoch 10, Loss: 1.413676\n",
            "Shadow model 44 trained.\n",
            "Epoch 1, Loss: 2.289461\n",
            "Epoch 2, Loss: 2.185795\n",
            "Epoch 3, Loss: 2.078886\n",
            "Epoch 4, Loss: 1.982366\n",
            "Epoch 5, Loss: 1.879886\n",
            "Epoch 6, Loss: 1.842881\n",
            "Epoch 7, Loss: 1.740931\n",
            "Epoch 8, Loss: 1.652060\n",
            "Epoch 9, Loss: 1.577305\n",
            "Epoch 10, Loss: 1.514310\n",
            "Shadow model 45 trained.\n",
            "Epoch 1, Loss: 2.268931\n",
            "Epoch 2, Loss: 2.119826\n",
            "Epoch 3, Loss: 1.974190\n",
            "Epoch 4, Loss: 1.877643\n",
            "Epoch 5, Loss: 1.772133\n",
            "Epoch 6, Loss: 1.693857\n",
            "Epoch 7, Loss: 1.569951\n",
            "Epoch 8, Loss: 1.473789\n",
            "Epoch 9, Loss: 1.327797\n",
            "Epoch 10, Loss: 1.279993\n",
            "Shadow model 46 trained.\n",
            "Epoch 1, Loss: 2.266881\n",
            "Epoch 2, Loss: 2.106872\n",
            "Epoch 3, Loss: 2.015314\n",
            "Epoch 4, Loss: 1.912531\n",
            "Epoch 5, Loss: 1.799022\n",
            "Epoch 6, Loss: 1.700850\n",
            "Epoch 7, Loss: 1.581753\n",
            "Epoch 8, Loss: 1.482968\n",
            "Epoch 9, Loss: 1.405197\n",
            "Epoch 10, Loss: 1.276295\n",
            "Shadow model 47 trained.\n",
            "Epoch 1, Loss: 2.302041\n",
            "Epoch 2, Loss: 2.205255\n",
            "Epoch 3, Loss: 2.110795\n",
            "Epoch 4, Loss: 1.990996\n",
            "Epoch 5, Loss: 1.897268\n",
            "Epoch 6, Loss: 1.805317\n",
            "Epoch 7, Loss: 1.673290\n",
            "Epoch 8, Loss: 1.563029\n",
            "Epoch 9, Loss: 1.517240\n",
            "Epoch 10, Loss: 1.421984\n",
            "Shadow model 48 trained.\n",
            "Epoch 1, Loss: 2.255963\n",
            "Epoch 2, Loss: 2.096045\n",
            "Epoch 3, Loss: 1.990510\n",
            "Epoch 4, Loss: 1.865296\n",
            "Epoch 5, Loss: 1.776989\n",
            "Epoch 6, Loss: 1.690202\n",
            "Epoch 7, Loss: 1.609250\n",
            "Epoch 8, Loss: 1.557486\n",
            "Epoch 9, Loss: 1.453717\n",
            "Epoch 10, Loss: 1.315410\n",
            "Shadow model 49 trained.\n",
            "Epoch 1, Loss: 2.273682\n",
            "Epoch 2, Loss: 2.132237\n",
            "Epoch 3, Loss: 2.016425\n",
            "Epoch 4, Loss: 1.932446\n",
            "Epoch 5, Loss: 1.810229\n",
            "Epoch 6, Loss: 1.734675\n",
            "Epoch 7, Loss: 1.650149\n",
            "Epoch 8, Loss: 1.528985\n",
            "Epoch 9, Loss: 1.418172\n",
            "Epoch 10, Loss: 1.356142\n",
            "Shadow model 50 trained.\n",
            "Epoch 1, Loss: 0.622567\n",
            "Epoch 2, Loss: 0.586396\n",
            "Epoch 3, Loss: 0.577245\n",
            "Epoch 4, Loss: 0.574281\n",
            "Epoch 5, Loss: 0.571141\n",
            "Epoch 6, Loss: 0.571221\n",
            "Epoch 7, Loss: 0.561663\n",
            "Epoch 8, Loss: 0.560597\n",
            "Epoch 9, Loss: 0.559702\n",
            "Epoch 10, Loss: 0.558080\n",
            "Epoch 11, Loss: 0.558353\n",
            "Epoch 12, Loss: 0.558611\n",
            "Epoch 13, Loss: 0.555693\n",
            "Epoch 14, Loss: 0.554140\n",
            "Epoch 15, Loss: 0.556929\n",
            "Epoch 16, Loss: 0.556591\n",
            "Epoch 17, Loss: 0.553980\n",
            "Epoch 18, Loss: 0.554716\n",
            "Epoch 19, Loss: 0.556287\n",
            "Epoch 20, Loss: 0.555495\n",
            "Epoch 21, Loss: 0.555213\n",
            "Epoch 22, Loss: 0.552852\n",
            "Epoch 23, Loss: 0.552617\n",
            "Epoch 24, Loss: 0.548186\n",
            "Epoch 25, Loss: 0.551837\n",
            "Epoch 26, Loss: 0.547036\n",
            "Epoch 27, Loss: 0.548602\n",
            "Epoch 28, Loss: 0.546736\n",
            "Epoch 29, Loss: 0.549364\n",
            "Epoch 30, Loss: 0.548091\n",
            "Epoch 31, Loss: 0.548752\n",
            "Epoch 32, Loss: 0.548173\n",
            "Epoch 33, Loss: 0.545524\n",
            "Epoch 34, Loss: 0.546033\n",
            "Epoch 35, Loss: 0.548724\n",
            "Epoch 36, Loss: 0.543965\n",
            "Epoch 37, Loss: 0.542918\n",
            "Epoch 38, Loss: 0.543384\n",
            "Epoch 39, Loss: 0.548595\n",
            "Epoch 40, Loss: 0.544411\n",
            "Epoch 41, Loss: 0.543534\n",
            "Epoch 42, Loss: 0.542968\n",
            "Epoch 43, Loss: 0.545035\n",
            "Epoch 44, Loss: 0.543479\n",
            "Epoch 45, Loss: 0.543124\n",
            "Epoch 46, Loss: 0.544880\n",
            "Epoch 47, Loss: 0.545677\n",
            "Epoch 48, Loss: 0.544328\n",
            "Epoch 49, Loss: 0.541807\n",
            "Epoch 50, Loss: 0.540792\n",
            "Attack model for class 0 trained.\n",
            "Epoch 1, Loss: 0.593611\n",
            "Epoch 2, Loss: 0.537889\n",
            "Epoch 3, Loss: 0.533500\n",
            "Epoch 4, Loss: 0.529312\n",
            "Epoch 5, Loss: 0.533250\n",
            "Epoch 6, Loss: 0.532020\n",
            "Epoch 7, Loss: 0.528673\n",
            "Epoch 8, Loss: 0.526950\n",
            "Epoch 9, Loss: 0.526565\n",
            "Epoch 10, Loss: 0.525703\n",
            "Epoch 11, Loss: 0.526423\n",
            "Epoch 12, Loss: 0.522881\n",
            "Epoch 13, Loss: 0.523371\n",
            "Epoch 14, Loss: 0.533845\n",
            "Epoch 15, Loss: 0.524210\n",
            "Epoch 16, Loss: 0.522430\n",
            "Epoch 17, Loss: 0.524745\n",
            "Epoch 18, Loss: 0.522883\n",
            "Epoch 19, Loss: 0.523297\n",
            "Epoch 20, Loss: 0.523245\n",
            "Epoch 21, Loss: 0.522948\n",
            "Epoch 22, Loss: 0.521615\n",
            "Epoch 23, Loss: 0.521064\n",
            "Epoch 24, Loss: 0.519727\n",
            "Epoch 25, Loss: 0.522532\n",
            "Epoch 26, Loss: 0.521549\n",
            "Epoch 27, Loss: 0.519814\n",
            "Epoch 28, Loss: 0.523722\n",
            "Epoch 29, Loss: 0.521100\n",
            "Epoch 30, Loss: 0.520909\n",
            "Epoch 31, Loss: 0.525510\n",
            "Epoch 32, Loss: 0.521736\n",
            "Epoch 33, Loss: 0.524055\n",
            "Epoch 34, Loss: 0.519347\n",
            "Epoch 35, Loss: 0.521382\n",
            "Epoch 36, Loss: 0.520892\n",
            "Epoch 37, Loss: 0.520480\n",
            "Epoch 38, Loss: 0.519086\n",
            "Epoch 39, Loss: 0.518056\n",
            "Epoch 40, Loss: 0.520220\n",
            "Epoch 41, Loss: 0.518524\n",
            "Epoch 42, Loss: 0.518652\n",
            "Epoch 43, Loss: 0.518136\n",
            "Epoch 44, Loss: 0.521358\n",
            "Epoch 45, Loss: 0.518156\n",
            "Epoch 46, Loss: 0.517524\n",
            "Epoch 47, Loss: 0.518040\n",
            "Epoch 48, Loss: 0.520506\n",
            "Epoch 49, Loss: 0.520258\n",
            "Epoch 50, Loss: 0.517655\n",
            "Attack model for class 1 trained.\n",
            "Epoch 1, Loss: 0.596972\n",
            "Epoch 2, Loss: 0.520727\n",
            "Epoch 3, Loss: 0.510993\n",
            "Epoch 4, Loss: 0.507965\n",
            "Epoch 5, Loss: 0.513108\n",
            "Epoch 6, Loss: 0.511741\n",
            "Epoch 7, Loss: 0.507874\n",
            "Epoch 8, Loss: 0.499650\n",
            "Epoch 9, Loss: 0.504457\n",
            "Epoch 10, Loss: 0.502394\n",
            "Epoch 11, Loss: 0.501290\n",
            "Epoch 12, Loss: 0.498339\n",
            "Epoch 13, Loss: 0.502428\n",
            "Epoch 14, Loss: 0.496498\n",
            "Epoch 15, Loss: 0.497922\n",
            "Epoch 16, Loss: 0.494672\n",
            "Epoch 17, Loss: 0.492359\n",
            "Epoch 18, Loss: 0.497896\n",
            "Epoch 19, Loss: 0.494781\n",
            "Epoch 20, Loss: 0.495031\n",
            "Epoch 21, Loss: 0.491010\n",
            "Epoch 22, Loss: 0.498743\n",
            "Epoch 23, Loss: 0.494318\n",
            "Epoch 24, Loss: 0.491821\n",
            "Epoch 25, Loss: 0.493952\n",
            "Epoch 26, Loss: 0.491472\n",
            "Epoch 27, Loss: 0.493261\n",
            "Epoch 28, Loss: 0.491524\n",
            "Epoch 29, Loss: 0.490106\n",
            "Epoch 30, Loss: 0.492853\n",
            "Epoch 31, Loss: 0.487472\n",
            "Epoch 32, Loss: 0.490249\n",
            "Epoch 33, Loss: 0.488002\n",
            "Epoch 34, Loss: 0.485470\n",
            "Epoch 35, Loss: 0.488036\n",
            "Epoch 36, Loss: 0.487439\n",
            "Epoch 37, Loss: 0.485511\n",
            "Epoch 38, Loss: 0.483275\n",
            "Epoch 39, Loss: 0.487081\n",
            "Epoch 40, Loss: 0.482113\n",
            "Epoch 41, Loss: 0.485537\n",
            "Epoch 42, Loss: 0.483776\n",
            "Epoch 43, Loss: 0.480116\n",
            "Epoch 44, Loss: 0.482821\n",
            "Epoch 45, Loss: 0.483777\n",
            "Epoch 46, Loss: 0.486225\n",
            "Epoch 47, Loss: 0.483429\n",
            "Epoch 48, Loss: 0.479715\n",
            "Epoch 49, Loss: 0.478819\n",
            "Epoch 50, Loss: 0.479893\n",
            "Attack model for class 2 trained.\n",
            "Epoch 1, Loss: 0.594602\n",
            "Epoch 2, Loss: 0.499548\n",
            "Epoch 3, Loss: 0.490727\n",
            "Epoch 4, Loss: 0.486111\n",
            "Epoch 5, Loss: 0.486041\n",
            "Epoch 6, Loss: 0.478335\n",
            "Epoch 7, Loss: 0.477429\n",
            "Epoch 8, Loss: 0.478638\n",
            "Epoch 9, Loss: 0.478027\n",
            "Epoch 10, Loss: 0.482191\n",
            "Epoch 11, Loss: 0.476730\n",
            "Epoch 12, Loss: 0.480717\n",
            "Epoch 13, Loss: 0.474771\n",
            "Epoch 14, Loss: 0.476577\n",
            "Epoch 15, Loss: 0.475236\n",
            "Epoch 16, Loss: 0.476714\n",
            "Epoch 17, Loss: 0.472449\n",
            "Epoch 18, Loss: 0.478316\n",
            "Epoch 19, Loss: 0.474955\n",
            "Epoch 20, Loss: 0.473643\n",
            "Epoch 21, Loss: 0.474899\n",
            "Epoch 22, Loss: 0.471658\n",
            "Epoch 23, Loss: 0.473987\n",
            "Epoch 24, Loss: 0.471682\n",
            "Epoch 25, Loss: 0.470672\n",
            "Epoch 26, Loss: 0.478804\n",
            "Epoch 27, Loss: 0.474009\n",
            "Epoch 28, Loss: 0.470639\n",
            "Epoch 29, Loss: 0.472163\n",
            "Epoch 30, Loss: 0.470766\n",
            "Epoch 31, Loss: 0.471012\n",
            "Epoch 32, Loss: 0.471867\n",
            "Epoch 33, Loss: 0.476125\n",
            "Epoch 34, Loss: 0.466825\n",
            "Epoch 35, Loss: 0.470332\n",
            "Epoch 36, Loss: 0.471409\n",
            "Epoch 37, Loss: 0.468682\n",
            "Epoch 38, Loss: 0.468131\n",
            "Epoch 39, Loss: 0.469586\n",
            "Epoch 40, Loss: 0.467191\n",
            "Epoch 41, Loss: 0.468961\n",
            "Epoch 42, Loss: 0.469091\n",
            "Epoch 43, Loss: 0.465517\n",
            "Epoch 44, Loss: 0.467676\n",
            "Epoch 45, Loss: 0.468700\n",
            "Epoch 46, Loss: 0.468928\n",
            "Epoch 47, Loss: 0.466403\n",
            "Epoch 48, Loss: 0.464678\n",
            "Epoch 49, Loss: 0.466626\n",
            "Epoch 50, Loss: 0.463390\n",
            "Attack model for class 3 trained.\n",
            "Epoch 1, Loss: 0.605384\n",
            "Epoch 2, Loss: 0.555770\n",
            "Epoch 3, Loss: 0.544851\n",
            "Epoch 4, Loss: 0.540226\n",
            "Epoch 5, Loss: 0.529922\n",
            "Epoch 6, Loss: 0.532128\n",
            "Epoch 7, Loss: 0.526542\n",
            "Epoch 8, Loss: 0.527828\n",
            "Epoch 9, Loss: 0.525555\n",
            "Epoch 10, Loss: 0.526742\n",
            "Epoch 11, Loss: 0.525380\n",
            "Epoch 12, Loss: 0.520744\n",
            "Epoch 13, Loss: 0.519143\n",
            "Epoch 14, Loss: 0.520259\n",
            "Epoch 15, Loss: 0.517513\n",
            "Epoch 16, Loss: 0.520163\n",
            "Epoch 17, Loss: 0.522142\n",
            "Epoch 18, Loss: 0.518769\n",
            "Epoch 19, Loss: 0.516613\n",
            "Epoch 20, Loss: 0.515881\n",
            "Epoch 21, Loss: 0.518820\n",
            "Epoch 22, Loss: 0.515490\n",
            "Epoch 23, Loss: 0.515179\n",
            "Epoch 24, Loss: 0.513007\n",
            "Epoch 25, Loss: 0.513408\n",
            "Epoch 26, Loss: 0.514433\n",
            "Epoch 27, Loss: 0.517144\n",
            "Epoch 28, Loss: 0.516767\n",
            "Epoch 29, Loss: 0.511114\n",
            "Epoch 30, Loss: 0.509811\n",
            "Epoch 31, Loss: 0.513107\n",
            "Epoch 32, Loss: 0.511935\n",
            "Epoch 33, Loss: 0.512218\n",
            "Epoch 34, Loss: 0.512559\n",
            "Epoch 35, Loss: 0.510765\n",
            "Epoch 36, Loss: 0.510227\n",
            "Epoch 37, Loss: 0.511486\n",
            "Epoch 38, Loss: 0.512231\n",
            "Epoch 39, Loss: 0.508265\n",
            "Epoch 40, Loss: 0.512912\n",
            "Epoch 41, Loss: 0.508390\n",
            "Epoch 42, Loss: 0.508182\n",
            "Epoch 43, Loss: 0.509017\n",
            "Epoch 44, Loss: 0.508000\n",
            "Epoch 45, Loss: 0.512051\n",
            "Epoch 46, Loss: 0.509923\n",
            "Epoch 47, Loss: 0.507345\n",
            "Epoch 48, Loss: 0.505610\n",
            "Epoch 49, Loss: 0.509158\n",
            "Epoch 50, Loss: 0.507464\n",
            "Attack model for class 4 trained.\n",
            "Epoch 1, Loss: 0.607090\n",
            "Epoch 2, Loss: 0.545792\n",
            "Epoch 3, Loss: 0.537131\n",
            "Epoch 4, Loss: 0.529632\n",
            "Epoch 5, Loss: 0.530190\n",
            "Epoch 6, Loss: 0.532462\n",
            "Epoch 7, Loss: 0.528552\n",
            "Epoch 8, Loss: 0.528778\n",
            "Epoch 9, Loss: 0.526883\n",
            "Epoch 10, Loss: 0.525025\n",
            "Epoch 11, Loss: 0.522036\n",
            "Epoch 12, Loss: 0.521386\n",
            "Epoch 13, Loss: 0.520878\n",
            "Epoch 14, Loss: 0.517004\n",
            "Epoch 15, Loss: 0.525836\n",
            "Epoch 16, Loss: 0.519273\n",
            "Epoch 17, Loss: 0.516340\n",
            "Epoch 18, Loss: 0.517090\n",
            "Epoch 19, Loss: 0.516531\n",
            "Epoch 20, Loss: 0.515970\n",
            "Epoch 21, Loss: 0.517306\n",
            "Epoch 22, Loss: 0.519782\n",
            "Epoch 23, Loss: 0.513571\n",
            "Epoch 24, Loss: 0.515461\n",
            "Epoch 25, Loss: 0.515266\n",
            "Epoch 26, Loss: 0.519981\n",
            "Epoch 27, Loss: 0.514964\n",
            "Epoch 28, Loss: 0.515080\n",
            "Epoch 29, Loss: 0.515117\n",
            "Epoch 30, Loss: 0.510936\n",
            "Epoch 31, Loss: 0.512147\n",
            "Epoch 32, Loss: 0.515354\n",
            "Epoch 33, Loss: 0.510454\n",
            "Epoch 34, Loss: 0.510424\n",
            "Epoch 35, Loss: 0.508311\n",
            "Epoch 36, Loss: 0.509258\n",
            "Epoch 37, Loss: 0.509400\n",
            "Epoch 38, Loss: 0.512108\n",
            "Epoch 39, Loss: 0.509954\n",
            "Epoch 40, Loss: 0.508311\n",
            "Epoch 41, Loss: 0.508534\n",
            "Epoch 42, Loss: 0.509018\n",
            "Epoch 43, Loss: 0.507703\n",
            "Epoch 44, Loss: 0.507299\n",
            "Epoch 45, Loss: 0.506274\n",
            "Epoch 46, Loss: 0.506341\n",
            "Epoch 47, Loss: 0.507479\n",
            "Epoch 48, Loss: 0.507609\n",
            "Epoch 49, Loss: 0.511955\n",
            "Epoch 50, Loss: 0.505029\n",
            "Attack model for class 5 trained.\n",
            "Epoch 1, Loss: 0.599411\n",
            "Epoch 2, Loss: 0.564238\n",
            "Epoch 3, Loss: 0.558681\n",
            "Epoch 4, Loss: 0.552316\n",
            "Epoch 5, Loss: 0.546994\n",
            "Epoch 6, Loss: 0.548637\n",
            "Epoch 7, Loss: 0.550688\n",
            "Epoch 8, Loss: 0.548851\n",
            "Epoch 9, Loss: 0.546324\n",
            "Epoch 10, Loss: 0.543042\n",
            "Epoch 11, Loss: 0.545035\n",
            "Epoch 12, Loss: 0.541136\n",
            "Epoch 13, Loss: 0.543226\n",
            "Epoch 14, Loss: 0.546054\n",
            "Epoch 15, Loss: 0.543618\n",
            "Epoch 16, Loss: 0.541405\n",
            "Epoch 17, Loss: 0.538584\n",
            "Epoch 18, Loss: 0.541108\n",
            "Epoch 19, Loss: 0.538563\n",
            "Epoch 20, Loss: 0.538057\n",
            "Epoch 21, Loss: 0.537005\n",
            "Epoch 22, Loss: 0.539369\n",
            "Epoch 23, Loss: 0.535114\n",
            "Epoch 24, Loss: 0.539323\n",
            "Epoch 25, Loss: 0.536268\n",
            "Epoch 26, Loss: 0.534228\n",
            "Epoch 27, Loss: 0.536587\n",
            "Epoch 28, Loss: 0.537846\n",
            "Epoch 29, Loss: 0.532336\n",
            "Epoch 30, Loss: 0.533677\n",
            "Epoch 31, Loss: 0.532222\n",
            "Epoch 32, Loss: 0.536036\n",
            "Epoch 33, Loss: 0.533306\n",
            "Epoch 34, Loss: 0.530496\n",
            "Epoch 35, Loss: 0.535875\n",
            "Epoch 36, Loss: 0.531964\n",
            "Epoch 37, Loss: 0.530053\n",
            "Epoch 38, Loss: 0.529617\n",
            "Epoch 39, Loss: 0.530104\n",
            "Epoch 40, Loss: 0.530092\n",
            "Epoch 41, Loss: 0.528901\n",
            "Epoch 42, Loss: 0.528561\n",
            "Epoch 43, Loss: 0.526238\n",
            "Epoch 44, Loss: 0.527635\n",
            "Epoch 45, Loss: 0.529052\n",
            "Epoch 46, Loss: 0.527385\n",
            "Epoch 47, Loss: 0.528649\n",
            "Epoch 48, Loss: 0.525252\n",
            "Epoch 49, Loss: 0.526356\n",
            "Epoch 50, Loss: 0.525654\n",
            "Attack model for class 6 trained.\n",
            "Epoch 1, Loss: 0.595528\n",
            "Epoch 2, Loss: 0.540181\n",
            "Epoch 3, Loss: 0.535453\n",
            "Epoch 4, Loss: 0.536107\n",
            "Epoch 5, Loss: 0.533902\n",
            "Epoch 6, Loss: 0.535694\n",
            "Epoch 7, Loss: 0.529460\n",
            "Epoch 8, Loss: 0.528711\n",
            "Epoch 9, Loss: 0.531809\n",
            "Epoch 10, Loss: 0.525067\n",
            "Epoch 11, Loss: 0.531328\n",
            "Epoch 12, Loss: 0.525318\n",
            "Epoch 13, Loss: 0.526391\n",
            "Epoch 14, Loss: 0.525216\n",
            "Epoch 15, Loss: 0.523493\n",
            "Epoch 16, Loss: 0.526408\n",
            "Epoch 17, Loss: 0.528596\n",
            "Epoch 18, Loss: 0.524939\n",
            "Epoch 19, Loss: 0.522051\n",
            "Epoch 20, Loss: 0.525601\n",
            "Epoch 21, Loss: 0.520775\n",
            "Epoch 22, Loss: 0.522199\n",
            "Epoch 23, Loss: 0.522004\n",
            "Epoch 24, Loss: 0.518749\n",
            "Epoch 25, Loss: 0.522389\n",
            "Epoch 26, Loss: 0.520011\n",
            "Epoch 27, Loss: 0.517894\n",
            "Epoch 28, Loss: 0.520516\n",
            "Epoch 29, Loss: 0.519357\n",
            "Epoch 30, Loss: 0.517718\n",
            "Epoch 31, Loss: 0.515690\n",
            "Epoch 32, Loss: 0.519237\n",
            "Epoch 33, Loss: 0.516708\n",
            "Epoch 34, Loss: 0.520282\n",
            "Epoch 35, Loss: 0.518887\n",
            "Epoch 36, Loss: 0.514315\n",
            "Epoch 37, Loss: 0.513528\n",
            "Epoch 38, Loss: 0.514180\n",
            "Epoch 39, Loss: 0.516642\n",
            "Epoch 40, Loss: 0.514685\n",
            "Epoch 41, Loss: 0.512340\n",
            "Epoch 42, Loss: 0.510982\n",
            "Epoch 43, Loss: 0.515034\n",
            "Epoch 44, Loss: 0.513975\n",
            "Epoch 45, Loss: 0.511733\n",
            "Epoch 46, Loss: 0.511228\n",
            "Epoch 47, Loss: 0.512334\n",
            "Epoch 48, Loss: 0.511815\n",
            "Epoch 49, Loss: 0.509548\n",
            "Epoch 50, Loss: 0.511300\n",
            "Attack model for class 7 trained.\n",
            "Epoch 1, Loss: 0.619991\n",
            "Epoch 2, Loss: 0.575056\n",
            "Epoch 3, Loss: 0.565237\n",
            "Epoch 4, Loss: 0.563617\n",
            "Epoch 5, Loss: 0.558445\n",
            "Epoch 6, Loss: 0.556929\n",
            "Epoch 7, Loss: 0.555210\n",
            "Epoch 8, Loss: 0.558571\n",
            "Epoch 9, Loss: 0.555556\n",
            "Epoch 10, Loss: 0.556994\n",
            "Epoch 11, Loss: 0.558731\n",
            "Epoch 12, Loss: 0.553542\n",
            "Epoch 13, Loss: 0.552931\n",
            "Epoch 14, Loss: 0.552537\n",
            "Epoch 15, Loss: 0.551581\n",
            "Epoch 16, Loss: 0.549876\n",
            "Epoch 17, Loss: 0.552025\n",
            "Epoch 18, Loss: 0.549486\n",
            "Epoch 19, Loss: 0.551290\n",
            "Epoch 20, Loss: 0.555086\n",
            "Epoch 21, Loss: 0.549900\n",
            "Epoch 22, Loss: 0.550693\n",
            "Epoch 23, Loss: 0.549412\n",
            "Epoch 24, Loss: 0.549169\n",
            "Epoch 25, Loss: 0.549215\n",
            "Epoch 26, Loss: 0.548780\n",
            "Epoch 27, Loss: 0.548034\n",
            "Epoch 28, Loss: 0.547388\n",
            "Epoch 29, Loss: 0.547171\n",
            "Epoch 30, Loss: 0.550769\n",
            "Epoch 31, Loss: 0.547940\n",
            "Epoch 32, Loss: 0.543275\n",
            "Epoch 33, Loss: 0.552098\n",
            "Epoch 34, Loss: 0.547457\n",
            "Epoch 35, Loss: 0.545706\n",
            "Epoch 36, Loss: 0.544624\n",
            "Epoch 37, Loss: 0.547285\n",
            "Epoch 38, Loss: 0.546564\n",
            "Epoch 39, Loss: 0.545953\n",
            "Epoch 40, Loss: 0.543571\n",
            "Epoch 41, Loss: 0.545383\n",
            "Epoch 42, Loss: 0.544394\n",
            "Epoch 43, Loss: 0.544172\n",
            "Epoch 44, Loss: 0.543935\n",
            "Epoch 45, Loss: 0.543186\n",
            "Epoch 46, Loss: 0.542764\n",
            "Epoch 47, Loss: 0.544867\n",
            "Epoch 48, Loss: 0.544715\n",
            "Epoch 49, Loss: 0.545770\n",
            "Epoch 50, Loss: 0.539729\n",
            "Attack model for class 8 trained.\n",
            "Epoch 1, Loss: 0.588547\n",
            "Epoch 2, Loss: 0.528244\n",
            "Epoch 3, Loss: 0.515803\n",
            "Epoch 4, Loss: 0.516348\n",
            "Epoch 5, Loss: 0.516757\n",
            "Epoch 6, Loss: 0.512621\n",
            "Epoch 7, Loss: 0.513726\n",
            "Epoch 8, Loss: 0.511638\n",
            "Epoch 9, Loss: 0.510902\n",
            "Epoch 10, Loss: 0.508912\n",
            "Epoch 11, Loss: 0.507170\n",
            "Epoch 12, Loss: 0.509576\n",
            "Epoch 13, Loss: 0.505973\n",
            "Epoch 14, Loss: 0.506386\n",
            "Epoch 15, Loss: 0.512367\n",
            "Epoch 16, Loss: 0.505334\n",
            "Epoch 17, Loss: 0.505812\n",
            "Epoch 18, Loss: 0.504770\n",
            "Epoch 19, Loss: 0.504419\n",
            "Epoch 20, Loss: 0.505450\n",
            "Epoch 21, Loss: 0.506138\n",
            "Epoch 22, Loss: 0.504120\n",
            "Epoch 23, Loss: 0.503188\n",
            "Epoch 24, Loss: 0.503918\n",
            "Epoch 25, Loss: 0.504653\n",
            "Epoch 26, Loss: 0.503685\n",
            "Epoch 27, Loss: 0.503923\n",
            "Epoch 28, Loss: 0.502633\n",
            "Epoch 29, Loss: 0.498893\n",
            "Epoch 30, Loss: 0.499907\n",
            "Epoch 31, Loss: 0.503545\n",
            "Epoch 32, Loss: 0.505171\n",
            "Epoch 33, Loss: 0.498385\n",
            "Epoch 34, Loss: 0.500922\n",
            "Epoch 35, Loss: 0.499506\n",
            "Epoch 36, Loss: 0.496270\n",
            "Epoch 37, Loss: 0.495449\n",
            "Epoch 38, Loss: 0.496446\n",
            "Epoch 39, Loss: 0.494716\n",
            "Epoch 40, Loss: 0.494293\n",
            "Epoch 41, Loss: 0.497581\n",
            "Epoch 42, Loss: 0.495696\n",
            "Epoch 43, Loss: 0.497990\n",
            "Epoch 44, Loss: 0.497283\n",
            "Epoch 45, Loss: 0.494485\n",
            "Epoch 46, Loss: 0.497900\n",
            "Epoch 47, Loss: 0.493942\n",
            "Epoch 48, Loss: 0.495360\n",
            "Epoch 49, Loss: 0.491735\n",
            "Epoch 50, Loss: 0.494044\n",
            "Attack model for class 9 trained.\n",
            "Attack model for class 0 saved to attack_model_class_0.pth\n",
            "Attack model for class 1 saved to attack_model_class_1.pth\n",
            "Attack model for class 2 saved to attack_model_class_2.pth\n",
            "Attack model for class 3 saved to attack_model_class_3.pth\n",
            "Attack model for class 4 saved to attack_model_class_4.pth\n",
            "Attack model for class 5 saved to attack_model_class_5.pth\n",
            "Attack model for class 6 saved to attack_model_class_6.pth\n",
            "Attack model for class 7 saved to attack_model_class_7.pth\n",
            "Attack model for class 8 saved to attack_model_class_8.pth\n",
            "Attack model for class 9 saved to attack_model_class_9.pth\n",
            "Attack model for class 0 loaded from attack_model_class_0.pth\n",
            "Attack model for class 1 loaded from attack_model_class_1.pth\n",
            "Attack model for class 2 loaded from attack_model_class_2.pth\n",
            "Attack model for class 3 loaded from attack_model_class_3.pth\n",
            "Attack model for class 4 loaded from attack_model_class_4.pth\n",
            "Attack model for class 5 loaded from attack_model_class_5.pth\n",
            "Attack model for class 6 loaded from attack_model_class_6.pth\n",
            "Attack model for class 7 loaded from attack_model_class_7.pth\n",
            "Attack model for class 8 loaded from attack_model_class_8.pth\n",
            "Attack model for class 9 loaded from attack_model_class_9.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Accuracies"
      ],
      "metadata": {
        "id": "IQRbGPn21eMm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_base = CIFAR10Classifier()\n",
        "model_base.load_state_dict(torch.load('baseline_model.pth', map_location=device))\n",
        "model_base.to(device)\n",
        "\n",
        "accuracy_baseline = mia.evaluate_attack_model(train_loader_baseline, test_loader_baseline, model_base)\n",
        "print(f'Accuracy for Attacking to the Baseline Model :  {accuracy_baseline * 100:.2f}%')"
      ],
      "metadata": {
        "id": "_1ioptCcZcbC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8e5d601-4610-488f-a82c-3c2060365f03"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for Attacking to the Baseline Model :  78.12%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_module_prefix(state_dict):\n",
        "    new_state_dict = {}\n",
        "    for k, v in state_dict.items():\n",
        "        if k.startswith('_module.'):\n",
        "            new_state_dict[k[8:]] = v  # remove '_module.' prefix\n",
        "        else:\n",
        "            new_state_dict[k] = v\n",
        "    return new_state_dict\n",
        "\n",
        "model_private = CIFAR10Classifier().to(device)\n",
        "private_state_dict = torch.load('modified_model.pth', map_location=device)\n",
        "model_private.load_state_dict(remove_module_prefix(private_state_dict))\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model_private.to(device)\n",
        "\n",
        "accuracy_private = mia.evaluate_attack_model(train_loader_modified, test_loader_modified, model_private)\n",
        "print(f'Accuracy for Attacking to the Private Model :  {accuracy_private * 100:.2f}%')"
      ],
      "metadata": {
        "id": "4Jdk0cRCZgJU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed9b26d7-8b4b-46e9-c604-ebdafc8b11cf"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for Attacking to the Private Model :  63.96%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MLP with 2 Hidden Layers and Xavier/Glorot Initialization  :\n",
        "\n",
        "> Learning Rate (in training shadow models) : 0.001\n",
        "\n",
        "> Learning Rate (in training attacking models) : 0.001\n",
        "\n",
        "> Epochs (in training shadow models) : 10\n",
        "\n",
        "> Epochs (in training attacking models) : 50\n",
        "\n",
        "> Batch Size (in training shadow models) : 50\n",
        "\n",
        "> Batch Size (in training attacking models) : 64\n",
        "\n",
        "> num_shadow_models = 50\n",
        "\n",
        "> Optimizer (in training attacking models ) : RMSpop\n"
      ],
      "metadata": {
        "id": "qLXd_KXlZv3G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Subset, ConcatDataset\n",
        "from model import CIFAR10Classifier\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class AttackModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AttackModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(10, 64)\n",
        "        self.fc2 = nn.Linear(64, 1)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = torch.sigmoid(self.fc2(x))\n",
        "        return x\n",
        "\n",
        "class MembershipInferenceAttackNoPrivacy:\n",
        "    def __init__(self, shadow_model_class, attack_model_class, device='cpu'):\n",
        "        self.shadow_model_class = shadow_model_class\n",
        "        self.attack_model_class = attack_model_class\n",
        "        self.device = device\n",
        "        self.attack_models = {}\n",
        "\n",
        "    def train_shadow_models(self, seen_loaders, num_epochs=20, lr=1e-3):\n",
        "        self.shadow_models = [self.shadow_model_class().to(self.device) for _ in range(len(seen_loaders))]\n",
        "\n",
        "        for i, (shadow_model, seen_loader) in enumerate(zip(self.shadow_models, seen_loaders)):\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "            optimizer = optim.Adam(shadow_model.parameters(), lr=lr)\n",
        "            self._train_model(shadow_model, seen_loader, criterion, optimizer, num_epochs)\n",
        "            print(f'Shadow model {i+1} trained.')\n",
        "\n",
        "    def _train_model(self, model, dataloader, criterion, optimizer, num_epochs):\n",
        "        model.train()\n",
        "        for epoch in range(num_epochs):\n",
        "            running_loss = 0.0\n",
        "            for inputs, labels in dataloader:\n",
        "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                running_loss += loss.item()\n",
        "            print(f'Epoch {epoch+1}, Loss: {running_loss/len(dataloader):.6f}')\n",
        "\n",
        "    def collect_outputs(self, seen_loaders, unseen_loaders):\n",
        "        self.attack_data = []\n",
        "        self.attack_labels = []\n",
        "\n",
        "        for shadow_model, seen_loader, unseen_loader in zip(self.shadow_models, seen_loaders, unseen_loaders):\n",
        "            self._collect_shadow_model_outputs(shadow_model, seen_loader, label=1)  # in\n",
        "            self._collect_shadow_model_outputs(shadow_model, unseen_loader, label=0)  # out\n",
        "\n",
        "        self.attack_data = torch.cat(self.attack_data).to(self.device)\n",
        "        self.attack_labels = torch.cat(self.attack_labels).to(self.device)\n",
        "\n",
        "    def _collect_shadow_model_outputs(self, model, dataloader, label):\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in dataloader:\n",
        "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
        "                outputs = model(inputs)\n",
        "                probabilities = F.softmax(outputs, dim=1)\n",
        "                self.attack_data.append(torch.cat([probabilities, labels.unsqueeze(1).float()], dim=1))\n",
        "                self.attack_labels.append(torch.full((outputs.size(0),), label, dtype=torch.float).to(self.device))\n",
        "\n",
        "    def train_attack_models(self, num_epochs=30, lr=0.001):\n",
        "        attack_dataset = torch.utils.data.TensorDataset(self.attack_data, self.attack_labels)\n",
        "        for class_label in range(10):\n",
        "            class_indices = (self.attack_data[:, -1] == class_label).nonzero().squeeze()\n",
        "            class_data = self.attack_data[class_indices][:, :-1]\n",
        "            class_labels = self.attack_labels[class_indices].view(-1, 1)\n",
        "\n",
        "\n",
        "            attack_dataset = torch.utils.data.TensorDataset(class_data, class_labels)\n",
        "            attack_loader = DataLoader(attack_dataset, batch_size=50, shuffle=True)\n",
        "\n",
        "            attack_model = self.attack_model_class().to(self.device)\n",
        "            criterion = nn.BCELoss()\n",
        "            optimizer = optim.RMSprop(attack_model.parameters(), lr=lr)\n",
        "\n",
        "            self._train_model(attack_model, attack_loader, criterion, optimizer, num_epochs)\n",
        "            self.attack_models[class_label] = attack_model\n",
        "\n",
        "            print(f'Attack model for class {class_label} trained.')\n",
        "\n",
        "    def save_attack_models(self, path):\n",
        "        for class_label, model in self.attack_models.items():\n",
        "            torch.save(model.state_dict(), f'{path}_class_{class_label}.pth')\n",
        "            print(f'Attack model for class {class_label} saved to {path}_class_{class_label}.pth')\n",
        "\n",
        "    def load_attack_models(self, path):\n",
        "        for class_label in range(10):\n",
        "            model = self.attack_model_class().to(self.device)\n",
        "            model.load_state_dict(torch.load(f'{path}_class_{class_label}.pth', map_location=self.device))\n",
        "            self.attack_models[class_label] = model\n",
        "            print(f'Attack model for class {class_label} loaded from {path}_class_{class_label}.pth')\n",
        "\n",
        "    def infer_membership(self, model, seen_loader, unseen_loader , seen_outputs , unseen_outputs , labels):\n",
        "\n",
        "        model_outputs = torch.cat([seen_outputs, unseen_outputs]).to(self.device)\n",
        "        labels = labels.to(self.device)\n",
        "\n",
        "        memberships = []\n",
        "        for output, label in zip(model_outputs, labels):\n",
        "            class_label = label.item()\n",
        "            attack_model = self.attack_models[class_label]\n",
        "            membership_pred = attack_model(output.unsqueeze(0)).item()\n",
        "            memberships.append(membership_pred)\n",
        "\n",
        "        return torch.tensor(memberships, device=self.device)\n",
        "\n",
        "    def evaluate_attack_model(self, seen_loader, unseen_loader, target_model):\n",
        "        seen_outputs , lables_seen= self._get_model_outputs(target_model, seen_loader)\n",
        "        unseen_outputs , labels_unseen = self._get_model_outputs(target_model, unseen_loader)\n",
        "\n",
        "        attack_data = torch.cat([seen_outputs, unseen_outputs]).to(self.device)\n",
        "        attack_labels = torch.cat([torch.ones(len(seen_outputs)), torch.zeros(len(unseen_outputs))]).to(self.device)\n",
        "\n",
        "        labels = torch.cat([lables_seen, labels_unseen]).to(self.device)\n",
        "\n",
        "        memberships = self.infer_membership(target_model, seen_loader, unseen_loader , seen_outputs , unseen_outputs , labels)\n",
        "        membership_preds = (memberships > 0.5).float()\n",
        "        accuracy = (membership_preds == attack_labels).float().mean().item()\n",
        "        return accuracy\n",
        "\n",
        "    def _get_model_outputs(self, model, dataloader):\n",
        "        model.eval()\n",
        "        outputs_list = []\n",
        "        labels_list = []\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in dataloader:\n",
        "                inputs = inputs.to(self.device)\n",
        "                outputs = model(inputs)\n",
        "                probabilities = F.softmax(outputs, dim=1)\n",
        "                outputs_list.append(probabilities)\n",
        "                labels_list.append(labels) # Convert labels to list for easy concatenation\n",
        "        return torch.cat(outputs_list), torch.cat(labels_list)\n"
      ],
      "metadata": {
        "id": "0acn3Eo6Z3ME"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
        "train_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_size = int(0.8 * len(train_set))\n",
        "remaining_size = len(train_set) - train_size\n",
        "train_subset, remaining_subset = torch.utils.data.random_split(train_set, [train_size, remaining_size])\n",
        "\n",
        "\n",
        "num_shadow_models = 50\n",
        "seen_size_per_model = train_size // num_shadow_models\n",
        "seen_loaders = []\n",
        "\n",
        "for i in range(num_shadow_models):\n",
        "    start_idx = i * seen_size_per_model\n",
        "    end_idx = (i + 1) * seen_size_per_model\n",
        "    seen_indices = torch.arange(start_idx, end_idx)\n",
        "    seen_train_set = Subset(train_subset, seen_indices)\n",
        "    seen_loader = DataLoader(seen_train_set, batch_size=64, shuffle=True)\n",
        "    seen_loaders.append(seen_loader)\n",
        "\n",
        "unseen_dataset = ConcatDataset([remaining_subset, test_set])\n",
        "unseen_size_per_model = len(unseen_dataset) // num_shadow_models\n",
        "unseen_loaders = []\n",
        "\n",
        "for i in range(num_shadow_models):\n",
        "    start_idx = i * unseen_size_per_model\n",
        "    end_idx = (i + 1) * unseen_size_per_model\n",
        "    unseen_indices = torch.arange(start_idx, end_idx)\n",
        "    unseen_subset = Subset(unseen_dataset, unseen_indices)\n",
        "    unseen_loader = DataLoader(unseen_subset, batch_size=64, shuffle=False)\n",
        "    unseen_loaders.append(unseen_loader)\n",
        "\n",
        "test_loader = DataLoader(test_set, batch_size=64, shuffle=False)\n",
        "\n",
        "# Initialize MembershipInferenceAttackNoPrivacy\n",
        "mia = MembershipInferenceAttackNoPrivacy(CIFAR10Classifier, AttackModel, device)\n",
        "\n",
        "# Train shadow models without differential privacy\n",
        "mia.train_shadow_models(seen_loaders, num_epochs=10)\n",
        "\n",
        "# Collect outputs for attack model\n",
        "mia.collect_outputs(seen_loaders, unseen_loaders)\n",
        "\n",
        "# Train attack models\n",
        "mia.train_attack_models(num_epochs=50)\n",
        "\n",
        "# Save the attack models\n",
        "mia.save_attack_models('attack_model')\n",
        "\n",
        "# Load the attack models (for future use)\n",
        "mia.load_attack_models('attack_model')"
      ],
      "metadata": {
        "id": "sCO3hTLpaCor",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "601e03d4-4afa-44f3-bc05-f09a9b274419"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch 1, Loss: 2.278943\n",
            "Epoch 2, Loss: 2.129886\n",
            "Epoch 3, Loss: 2.020780\n",
            "Epoch 4, Loss: 1.906599\n",
            "Epoch 5, Loss: 1.808966\n",
            "Epoch 6, Loss: 1.706399\n",
            "Epoch 7, Loss: 1.618856\n",
            "Epoch 8, Loss: 1.509206\n",
            "Epoch 9, Loss: 1.415810\n",
            "Epoch 10, Loss: 1.370505\n",
            "Shadow model 1 trained.\n",
            "Epoch 1, Loss: 2.293199\n",
            "Epoch 2, Loss: 2.166173\n",
            "Epoch 3, Loss: 2.083275\n",
            "Epoch 4, Loss: 1.954465\n",
            "Epoch 5, Loss: 1.879094\n",
            "Epoch 6, Loss: 1.779070\n",
            "Epoch 7, Loss: 1.660878\n",
            "Epoch 8, Loss: 1.590809\n",
            "Epoch 9, Loss: 1.516659\n",
            "Epoch 10, Loss: 1.369547\n",
            "Shadow model 2 trained.\n",
            "Epoch 1, Loss: 2.270345\n",
            "Epoch 2, Loss: 2.150567\n",
            "Epoch 3, Loss: 2.029976\n",
            "Epoch 4, Loss: 1.984311\n",
            "Epoch 5, Loss: 1.888012\n",
            "Epoch 6, Loss: 1.830041\n",
            "Epoch 7, Loss: 1.718209\n",
            "Epoch 8, Loss: 1.633460\n",
            "Epoch 9, Loss: 1.555427\n",
            "Epoch 10, Loss: 1.496844\n",
            "Shadow model 3 trained.\n",
            "Epoch 1, Loss: 2.292522\n",
            "Epoch 2, Loss: 2.175536\n",
            "Epoch 3, Loss: 2.071751\n",
            "Epoch 4, Loss: 1.978271\n",
            "Epoch 5, Loss: 1.963725\n",
            "Epoch 6, Loss: 1.808319\n",
            "Epoch 7, Loss: 1.712103\n",
            "Epoch 8, Loss: 1.635382\n",
            "Epoch 9, Loss: 1.620516\n",
            "Epoch 10, Loss: 1.473176\n",
            "Shadow model 4 trained.\n",
            "Epoch 1, Loss: 2.294186\n",
            "Epoch 2, Loss: 2.180228\n",
            "Epoch 3, Loss: 2.061990\n",
            "Epoch 4, Loss: 1.947810\n",
            "Epoch 5, Loss: 1.864503\n",
            "Epoch 6, Loss: 1.760971\n",
            "Epoch 7, Loss: 1.683673\n",
            "Epoch 8, Loss: 1.520539\n",
            "Epoch 9, Loss: 1.490576\n",
            "Epoch 10, Loss: 1.399202\n",
            "Shadow model 5 trained.\n",
            "Epoch 1, Loss: 2.277806\n",
            "Epoch 2, Loss: 2.161603\n",
            "Epoch 3, Loss: 2.059546\n",
            "Epoch 4, Loss: 1.984824\n",
            "Epoch 5, Loss: 1.865017\n",
            "Epoch 6, Loss: 1.808764\n",
            "Epoch 7, Loss: 1.736681\n",
            "Epoch 8, Loss: 1.630724\n",
            "Epoch 9, Loss: 1.498669\n",
            "Epoch 10, Loss: 1.451403\n",
            "Shadow model 6 trained.\n",
            "Epoch 1, Loss: 2.282780\n",
            "Epoch 2, Loss: 2.153042\n",
            "Epoch 3, Loss: 2.086903\n",
            "Epoch 4, Loss: 1.995298\n",
            "Epoch 5, Loss: 1.951013\n",
            "Epoch 6, Loss: 1.819114\n",
            "Epoch 7, Loss: 1.728551\n",
            "Epoch 8, Loss: 1.677418\n",
            "Epoch 9, Loss: 1.577930\n",
            "Epoch 10, Loss: 1.557087\n",
            "Shadow model 7 trained.\n",
            "Epoch 1, Loss: 2.264186\n",
            "Epoch 2, Loss: 2.103220\n",
            "Epoch 3, Loss: 2.007256\n",
            "Epoch 4, Loss: 1.902569\n",
            "Epoch 5, Loss: 1.756597\n",
            "Epoch 6, Loss: 1.656206\n",
            "Epoch 7, Loss: 1.592994\n",
            "Epoch 8, Loss: 1.474115\n",
            "Epoch 9, Loss: 1.330006\n",
            "Epoch 10, Loss: 1.324093\n",
            "Shadow model 8 trained.\n",
            "Epoch 1, Loss: 2.289199\n",
            "Epoch 2, Loss: 2.170303\n",
            "Epoch 3, Loss: 2.112120\n",
            "Epoch 4, Loss: 2.044645\n",
            "Epoch 5, Loss: 1.943685\n",
            "Epoch 6, Loss: 1.836897\n",
            "Epoch 7, Loss: 1.728337\n",
            "Epoch 8, Loss: 1.660714\n",
            "Epoch 9, Loss: 1.594773\n",
            "Epoch 10, Loss: 1.508635\n",
            "Shadow model 9 trained.\n",
            "Epoch 1, Loss: 2.300075\n",
            "Epoch 2, Loss: 2.184188\n",
            "Epoch 3, Loss: 2.093689\n",
            "Epoch 4, Loss: 1.992705\n",
            "Epoch 5, Loss: 1.944564\n",
            "Epoch 6, Loss: 1.820548\n",
            "Epoch 7, Loss: 1.757522\n",
            "Epoch 8, Loss: 1.683214\n",
            "Epoch 9, Loss: 1.585217\n",
            "Epoch 10, Loss: 1.480901\n",
            "Shadow model 10 trained.\n",
            "Epoch 1, Loss: 2.291659\n",
            "Epoch 2, Loss: 2.162012\n",
            "Epoch 3, Loss: 2.032877\n",
            "Epoch 4, Loss: 1.936753\n",
            "Epoch 5, Loss: 1.857264\n",
            "Epoch 6, Loss: 1.766471\n",
            "Epoch 7, Loss: 1.694715\n",
            "Epoch 8, Loss: 1.553608\n",
            "Epoch 9, Loss: 1.517701\n",
            "Epoch 10, Loss: 1.388768\n",
            "Shadow model 11 trained.\n",
            "Epoch 1, Loss: 2.278660\n",
            "Epoch 2, Loss: 2.099721\n",
            "Epoch 3, Loss: 1.981869\n",
            "Epoch 4, Loss: 1.893178\n",
            "Epoch 5, Loss: 1.754417\n",
            "Epoch 6, Loss: 1.666843\n",
            "Epoch 7, Loss: 1.586651\n",
            "Epoch 8, Loss: 1.470140\n",
            "Epoch 9, Loss: 1.421285\n",
            "Epoch 10, Loss: 1.282503\n",
            "Shadow model 12 trained.\n",
            "Epoch 1, Loss: 2.261949\n",
            "Epoch 2, Loss: 2.125101\n",
            "Epoch 3, Loss: 2.047557\n",
            "Epoch 4, Loss: 1.955548\n",
            "Epoch 5, Loss: 1.884860\n",
            "Epoch 6, Loss: 1.747236\n",
            "Epoch 7, Loss: 1.678453\n",
            "Epoch 8, Loss: 1.599715\n",
            "Epoch 9, Loss: 1.452203\n",
            "Epoch 10, Loss: 1.422403\n",
            "Shadow model 13 trained.\n",
            "Epoch 1, Loss: 2.279643\n",
            "Epoch 2, Loss: 2.133934\n",
            "Epoch 3, Loss: 2.048128\n",
            "Epoch 4, Loss: 1.939349\n",
            "Epoch 5, Loss: 1.799038\n",
            "Epoch 6, Loss: 1.698976\n",
            "Epoch 7, Loss: 1.583642\n",
            "Epoch 8, Loss: 1.484934\n",
            "Epoch 9, Loss: 1.419032\n",
            "Epoch 10, Loss: 1.305890\n",
            "Shadow model 14 trained.\n",
            "Epoch 1, Loss: 2.271506\n",
            "Epoch 2, Loss: 2.159641\n",
            "Epoch 3, Loss: 2.027858\n",
            "Epoch 4, Loss: 1.955326\n",
            "Epoch 5, Loss: 1.872521\n",
            "Epoch 6, Loss: 1.794350\n",
            "Epoch 7, Loss: 1.700187\n",
            "Epoch 8, Loss: 1.663344\n",
            "Epoch 9, Loss: 1.571143\n",
            "Epoch 10, Loss: 1.448736\n",
            "Shadow model 15 trained.\n",
            "Epoch 1, Loss: 2.273143\n",
            "Epoch 2, Loss: 2.126337\n",
            "Epoch 3, Loss: 2.014950\n",
            "Epoch 4, Loss: 1.921173\n",
            "Epoch 5, Loss: 1.839291\n",
            "Epoch 6, Loss: 1.760200\n",
            "Epoch 7, Loss: 1.636375\n",
            "Epoch 8, Loss: 1.557945\n",
            "Epoch 9, Loss: 1.492799\n",
            "Epoch 10, Loss: 1.348304\n",
            "Shadow model 16 trained.\n",
            "Epoch 1, Loss: 2.251662\n",
            "Epoch 2, Loss: 2.096714\n",
            "Epoch 3, Loss: 1.944151\n",
            "Epoch 4, Loss: 1.831958\n",
            "Epoch 5, Loss: 1.742312\n",
            "Epoch 6, Loss: 1.648695\n",
            "Epoch 7, Loss: 1.492764\n",
            "Epoch 8, Loss: 1.439956\n",
            "Epoch 9, Loss: 1.299598\n",
            "Epoch 10, Loss: 1.181648\n",
            "Shadow model 17 trained.\n",
            "Epoch 1, Loss: 2.273669\n",
            "Epoch 2, Loss: 2.128057\n",
            "Epoch 3, Loss: 2.044124\n",
            "Epoch 4, Loss: 1.977948\n",
            "Epoch 5, Loss: 1.884523\n",
            "Epoch 6, Loss: 1.809535\n",
            "Epoch 7, Loss: 1.678473\n",
            "Epoch 8, Loss: 1.598171\n",
            "Epoch 9, Loss: 1.489126\n",
            "Epoch 10, Loss: 1.431107\n",
            "Shadow model 18 trained.\n",
            "Epoch 1, Loss: 2.268053\n",
            "Epoch 2, Loss: 2.101328\n",
            "Epoch 3, Loss: 2.012688\n",
            "Epoch 4, Loss: 1.900374\n",
            "Epoch 5, Loss: 1.810473\n",
            "Epoch 6, Loss: 1.669272\n",
            "Epoch 7, Loss: 1.603718\n",
            "Epoch 8, Loss: 1.429985\n",
            "Epoch 9, Loss: 1.388830\n",
            "Epoch 10, Loss: 1.291186\n",
            "Shadow model 19 trained.\n",
            "Epoch 1, Loss: 2.275832\n",
            "Epoch 2, Loss: 2.166723\n",
            "Epoch 3, Loss: 2.030050\n",
            "Epoch 4, Loss: 1.915553\n",
            "Epoch 5, Loss: 1.807143\n",
            "Epoch 6, Loss: 1.750516\n",
            "Epoch 7, Loss: 1.645989\n",
            "Epoch 8, Loss: 1.526562\n",
            "Epoch 9, Loss: 1.475704\n",
            "Epoch 10, Loss: 1.328004\n",
            "Shadow model 20 trained.\n",
            "Epoch 1, Loss: 2.261585\n",
            "Epoch 2, Loss: 2.145752\n",
            "Epoch 3, Loss: 2.068592\n",
            "Epoch 4, Loss: 1.902896\n",
            "Epoch 5, Loss: 1.827733\n",
            "Epoch 6, Loss: 1.733626\n",
            "Epoch 7, Loss: 1.641822\n",
            "Epoch 8, Loss: 1.567652\n",
            "Epoch 9, Loss: 1.452991\n",
            "Epoch 10, Loss: 1.400920\n",
            "Shadow model 21 trained.\n",
            "Epoch 1, Loss: 2.231364\n",
            "Epoch 2, Loss: 2.118635\n",
            "Epoch 3, Loss: 1.993695\n",
            "Epoch 4, Loss: 1.929110\n",
            "Epoch 5, Loss: 1.869740\n",
            "Epoch 6, Loss: 1.757083\n",
            "Epoch 7, Loss: 1.659118\n",
            "Epoch 8, Loss: 1.577717\n",
            "Epoch 9, Loss: 1.492120\n",
            "Epoch 10, Loss: 1.416476\n",
            "Shadow model 22 trained.\n",
            "Epoch 1, Loss: 2.255171\n",
            "Epoch 2, Loss: 2.103590\n",
            "Epoch 3, Loss: 2.017128\n",
            "Epoch 4, Loss: 1.887580\n",
            "Epoch 5, Loss: 1.844590\n",
            "Epoch 6, Loss: 1.700985\n",
            "Epoch 7, Loss: 1.639919\n",
            "Epoch 8, Loss: 1.535280\n",
            "Epoch 9, Loss: 1.430735\n",
            "Epoch 10, Loss: 1.353405\n",
            "Shadow model 23 trained.\n",
            "Epoch 1, Loss: 2.283996\n",
            "Epoch 2, Loss: 2.167390\n",
            "Epoch 3, Loss: 2.053526\n",
            "Epoch 4, Loss: 2.009190\n",
            "Epoch 5, Loss: 1.872086\n",
            "Epoch 6, Loss: 1.784178\n",
            "Epoch 7, Loss: 1.707104\n",
            "Epoch 8, Loss: 1.608683\n",
            "Epoch 9, Loss: 1.563855\n",
            "Epoch 10, Loss: 1.464012\n",
            "Shadow model 24 trained.\n",
            "Epoch 1, Loss: 2.264677\n",
            "Epoch 2, Loss: 2.114600\n",
            "Epoch 3, Loss: 1.978996\n",
            "Epoch 4, Loss: 1.888387\n",
            "Epoch 5, Loss: 1.764153\n",
            "Epoch 6, Loss: 1.708486\n",
            "Epoch 7, Loss: 1.610606\n",
            "Epoch 8, Loss: 1.527119\n",
            "Epoch 9, Loss: 1.445561\n",
            "Epoch 10, Loss: 1.368106\n",
            "Shadow model 25 trained.\n",
            "Epoch 1, Loss: 2.277597\n",
            "Epoch 2, Loss: 2.103239\n",
            "Epoch 3, Loss: 1.978485\n",
            "Epoch 4, Loss: 1.964381\n",
            "Epoch 5, Loss: 1.817955\n",
            "Epoch 6, Loss: 1.771018\n",
            "Epoch 7, Loss: 1.677128\n",
            "Epoch 8, Loss: 1.597645\n",
            "Epoch 9, Loss: 1.472215\n",
            "Epoch 10, Loss: 1.435312\n",
            "Shadow model 26 trained.\n",
            "Epoch 1, Loss: 2.284343\n",
            "Epoch 2, Loss: 2.215195\n",
            "Epoch 3, Loss: 2.123084\n",
            "Epoch 4, Loss: 2.053442\n",
            "Epoch 5, Loss: 1.980590\n",
            "Epoch 6, Loss: 1.848792\n",
            "Epoch 7, Loss: 1.744924\n",
            "Epoch 8, Loss: 1.645631\n",
            "Epoch 9, Loss: 1.537781\n",
            "Epoch 10, Loss: 1.436604\n",
            "Shadow model 27 trained.\n",
            "Epoch 1, Loss: 2.294559\n",
            "Epoch 2, Loss: 2.193830\n",
            "Epoch 3, Loss: 2.059161\n",
            "Epoch 4, Loss: 1.968377\n",
            "Epoch 5, Loss: 1.866276\n",
            "Epoch 6, Loss: 1.821106\n",
            "Epoch 7, Loss: 1.716650\n",
            "Epoch 8, Loss: 1.637606\n",
            "Epoch 9, Loss: 1.574260\n",
            "Epoch 10, Loss: 1.525034\n",
            "Shadow model 28 trained.\n",
            "Epoch 1, Loss: 2.317997\n",
            "Epoch 2, Loss: 2.205256\n",
            "Epoch 3, Loss: 2.120555\n",
            "Epoch 4, Loss: 2.051216\n",
            "Epoch 5, Loss: 1.972053\n",
            "Epoch 6, Loss: 1.845880\n",
            "Epoch 7, Loss: 1.781099\n",
            "Epoch 8, Loss: 1.685418\n",
            "Epoch 9, Loss: 1.619631\n",
            "Epoch 10, Loss: 1.485886\n",
            "Shadow model 29 trained.\n",
            "Epoch 1, Loss: 2.268842\n",
            "Epoch 2, Loss: 2.149212\n",
            "Epoch 3, Loss: 2.025183\n",
            "Epoch 4, Loss: 1.921316\n",
            "Epoch 5, Loss: 1.850268\n",
            "Epoch 6, Loss: 1.719054\n",
            "Epoch 7, Loss: 1.630731\n",
            "Epoch 8, Loss: 1.519394\n",
            "Epoch 9, Loss: 1.427175\n",
            "Epoch 10, Loss: 1.318025\n",
            "Shadow model 30 trained.\n",
            "Epoch 1, Loss: 2.293597\n",
            "Epoch 2, Loss: 2.176912\n",
            "Epoch 3, Loss: 2.049087\n",
            "Epoch 4, Loss: 1.909324\n",
            "Epoch 5, Loss: 1.876589\n",
            "Epoch 6, Loss: 1.742819\n",
            "Epoch 7, Loss: 1.700483\n",
            "Epoch 8, Loss: 1.611309\n",
            "Epoch 9, Loss: 1.520243\n",
            "Epoch 10, Loss: 1.449024\n",
            "Shadow model 31 trained.\n",
            "Epoch 1, Loss: 2.279750\n",
            "Epoch 2, Loss: 2.162011\n",
            "Epoch 3, Loss: 2.054869\n",
            "Epoch 4, Loss: 1.965752\n",
            "Epoch 5, Loss: 1.878153\n",
            "Epoch 6, Loss: 1.821327\n",
            "Epoch 7, Loss: 1.709055\n",
            "Epoch 8, Loss: 1.580592\n",
            "Epoch 9, Loss: 1.525978\n",
            "Epoch 10, Loss: 1.390183\n",
            "Shadow model 32 trained.\n",
            "Epoch 1, Loss: 2.311303\n",
            "Epoch 2, Loss: 2.190093\n",
            "Epoch 3, Loss: 2.109726\n",
            "Epoch 4, Loss: 2.002192\n",
            "Epoch 5, Loss: 1.896993\n",
            "Epoch 6, Loss: 1.832315\n",
            "Epoch 7, Loss: 1.709623\n",
            "Epoch 8, Loss: 1.600662\n",
            "Epoch 9, Loss: 1.513227\n",
            "Epoch 10, Loss: 1.458464\n",
            "Shadow model 33 trained.\n",
            "Epoch 1, Loss: 2.280070\n",
            "Epoch 2, Loss: 2.104655\n",
            "Epoch 3, Loss: 1.999048\n",
            "Epoch 4, Loss: 1.903613\n",
            "Epoch 5, Loss: 1.782351\n",
            "Epoch 6, Loss: 1.690766\n",
            "Epoch 7, Loss: 1.630911\n",
            "Epoch 8, Loss: 1.496281\n",
            "Epoch 9, Loss: 1.436502\n",
            "Epoch 10, Loss: 1.339112\n",
            "Shadow model 34 trained.\n",
            "Epoch 1, Loss: 2.311684\n",
            "Epoch 2, Loss: 2.205596\n",
            "Epoch 3, Loss: 2.112960\n",
            "Epoch 4, Loss: 2.000670\n",
            "Epoch 5, Loss: 1.834876\n",
            "Epoch 6, Loss: 1.750755\n",
            "Epoch 7, Loss: 1.611418\n",
            "Epoch 8, Loss: 1.495699\n",
            "Epoch 9, Loss: 1.384715\n",
            "Epoch 10, Loss: 1.292120\n",
            "Shadow model 35 trained.\n",
            "Epoch 1, Loss: 2.273798\n",
            "Epoch 2, Loss: 2.156883\n",
            "Epoch 3, Loss: 1.996518\n",
            "Epoch 4, Loss: 1.936301\n",
            "Epoch 5, Loss: 1.837705\n",
            "Epoch 6, Loss: 1.733976\n",
            "Epoch 7, Loss: 1.647818\n",
            "Epoch 8, Loss: 1.543349\n",
            "Epoch 9, Loss: 1.482767\n",
            "Epoch 10, Loss: 1.347246\n",
            "Shadow model 36 trained.\n",
            "Epoch 1, Loss: 2.302162\n",
            "Epoch 2, Loss: 2.178655\n",
            "Epoch 3, Loss: 2.083584\n",
            "Epoch 4, Loss: 1.978688\n",
            "Epoch 5, Loss: 1.891295\n",
            "Epoch 6, Loss: 1.781024\n",
            "Epoch 7, Loss: 1.739806\n",
            "Epoch 8, Loss: 1.631243\n",
            "Epoch 9, Loss: 1.579760\n",
            "Epoch 10, Loss: 1.442979\n",
            "Shadow model 37 trained.\n",
            "Epoch 1, Loss: 2.272658\n",
            "Epoch 2, Loss: 2.149980\n",
            "Epoch 3, Loss: 2.044446\n",
            "Epoch 4, Loss: 1.986101\n",
            "Epoch 5, Loss: 1.906510\n",
            "Epoch 6, Loss: 1.798962\n",
            "Epoch 7, Loss: 1.680922\n",
            "Epoch 8, Loss: 1.607540\n",
            "Epoch 9, Loss: 1.536890\n",
            "Epoch 10, Loss: 1.458595\n",
            "Shadow model 38 trained.\n",
            "Epoch 1, Loss: 2.312493\n",
            "Epoch 2, Loss: 2.220449\n",
            "Epoch 3, Loss: 2.102352\n",
            "Epoch 4, Loss: 1.949687\n",
            "Epoch 5, Loss: 1.856956\n",
            "Epoch 6, Loss: 1.768447\n",
            "Epoch 7, Loss: 1.690501\n",
            "Epoch 8, Loss: 1.614917\n",
            "Epoch 9, Loss: 1.483505\n",
            "Epoch 10, Loss: 1.356671\n",
            "Shadow model 39 trained.\n",
            "Epoch 1, Loss: 2.288774\n",
            "Epoch 2, Loss: 2.158294\n",
            "Epoch 3, Loss: 2.035364\n",
            "Epoch 4, Loss: 1.947400\n",
            "Epoch 5, Loss: 1.834595\n",
            "Epoch 6, Loss: 1.743891\n",
            "Epoch 7, Loss: 1.667115\n",
            "Epoch 8, Loss: 1.545097\n",
            "Epoch 9, Loss: 1.478061\n",
            "Epoch 10, Loss: 1.473522\n",
            "Shadow model 40 trained.\n",
            "Epoch 1, Loss: 2.275688\n",
            "Epoch 2, Loss: 2.179722\n",
            "Epoch 3, Loss: 2.046819\n",
            "Epoch 4, Loss: 1.920947\n",
            "Epoch 5, Loss: 1.834673\n",
            "Epoch 6, Loss: 1.691660\n",
            "Epoch 7, Loss: 1.577998\n",
            "Epoch 8, Loss: 1.397691\n",
            "Epoch 9, Loss: 1.326949\n",
            "Epoch 10, Loss: 1.197744\n",
            "Shadow model 41 trained.\n",
            "Epoch 1, Loss: 2.262610\n",
            "Epoch 2, Loss: 2.070404\n",
            "Epoch 3, Loss: 1.985323\n",
            "Epoch 4, Loss: 1.862693\n",
            "Epoch 5, Loss: 1.726155\n",
            "Epoch 6, Loss: 1.624380\n",
            "Epoch 7, Loss: 1.482865\n",
            "Epoch 8, Loss: 1.389495\n",
            "Epoch 9, Loss: 1.261129\n",
            "Epoch 10, Loss: 1.186419\n",
            "Shadow model 42 trained.\n",
            "Epoch 1, Loss: 2.315354\n",
            "Epoch 2, Loss: 2.197717\n",
            "Epoch 3, Loss: 2.113752\n",
            "Epoch 4, Loss: 2.013998\n",
            "Epoch 5, Loss: 1.896490\n",
            "Epoch 6, Loss: 1.789524\n",
            "Epoch 7, Loss: 1.738425\n",
            "Epoch 8, Loss: 1.641593\n",
            "Epoch 9, Loss: 1.548786\n",
            "Epoch 10, Loss: 1.444915\n",
            "Shadow model 43 trained.\n",
            "Epoch 1, Loss: 2.267910\n",
            "Epoch 2, Loss: 2.124301\n",
            "Epoch 3, Loss: 2.032674\n",
            "Epoch 4, Loss: 1.949402\n",
            "Epoch 5, Loss: 1.850941\n",
            "Epoch 6, Loss: 1.738123\n",
            "Epoch 7, Loss: 1.640820\n",
            "Epoch 8, Loss: 1.579225\n",
            "Epoch 9, Loss: 1.488839\n",
            "Epoch 10, Loss: 1.406020\n",
            "Shadow model 44 trained.\n",
            "Epoch 1, Loss: 2.285303\n",
            "Epoch 2, Loss: 2.165659\n",
            "Epoch 3, Loss: 2.106206\n",
            "Epoch 4, Loss: 1.951121\n",
            "Epoch 5, Loss: 1.869179\n",
            "Epoch 6, Loss: 1.832022\n",
            "Epoch 7, Loss: 1.698648\n",
            "Epoch 8, Loss: 1.610461\n",
            "Epoch 9, Loss: 1.481503\n",
            "Epoch 10, Loss: 1.374154\n",
            "Shadow model 45 trained.\n",
            "Epoch 1, Loss: 2.292318\n",
            "Epoch 2, Loss: 2.161021\n",
            "Epoch 3, Loss: 2.043336\n",
            "Epoch 4, Loss: 1.898762\n",
            "Epoch 5, Loss: 1.832381\n",
            "Epoch 6, Loss: 1.771495\n",
            "Epoch 7, Loss: 1.624897\n",
            "Epoch 8, Loss: 1.559641\n",
            "Epoch 9, Loss: 1.490755\n",
            "Epoch 10, Loss: 1.369108\n",
            "Shadow model 46 trained.\n",
            "Epoch 1, Loss: 2.320546\n",
            "Epoch 2, Loss: 2.208159\n",
            "Epoch 3, Loss: 2.120036\n",
            "Epoch 4, Loss: 2.032951\n",
            "Epoch 5, Loss: 1.926340\n",
            "Epoch 6, Loss: 1.808593\n",
            "Epoch 7, Loss: 1.701900\n",
            "Epoch 8, Loss: 1.606734\n",
            "Epoch 9, Loss: 1.496750\n",
            "Epoch 10, Loss: 1.460591\n",
            "Shadow model 47 trained.\n",
            "Epoch 1, Loss: 2.236389\n",
            "Epoch 2, Loss: 2.094147\n",
            "Epoch 3, Loss: 2.012691\n",
            "Epoch 4, Loss: 1.911978\n",
            "Epoch 5, Loss: 1.844965\n",
            "Epoch 6, Loss: 1.702529\n",
            "Epoch 7, Loss: 1.621968\n",
            "Epoch 8, Loss: 1.536395\n",
            "Epoch 9, Loss: 1.498995\n",
            "Epoch 10, Loss: 1.346247\n",
            "Shadow model 48 trained.\n",
            "Epoch 1, Loss: 2.281085\n",
            "Epoch 2, Loss: 2.143851\n",
            "Epoch 3, Loss: 1.988048\n",
            "Epoch 4, Loss: 1.881028\n",
            "Epoch 5, Loss: 1.764179\n",
            "Epoch 6, Loss: 1.628598\n",
            "Epoch 7, Loss: 1.548751\n",
            "Epoch 8, Loss: 1.489892\n",
            "Epoch 9, Loss: 1.286860\n",
            "Epoch 10, Loss: 1.296147\n",
            "Shadow model 49 trained.\n",
            "Epoch 1, Loss: 2.271621\n",
            "Epoch 2, Loss: 2.114934\n",
            "Epoch 3, Loss: 2.002815\n",
            "Epoch 4, Loss: 1.903035\n",
            "Epoch 5, Loss: 1.856203\n",
            "Epoch 6, Loss: 1.726017\n",
            "Epoch 7, Loss: 1.624100\n",
            "Epoch 8, Loss: 1.594410\n",
            "Epoch 9, Loss: 1.480404\n",
            "Epoch 10, Loss: 1.390780\n",
            "Shadow model 50 trained.\n",
            "Epoch 1, Loss: 0.609343\n",
            "Epoch 2, Loss: 0.588391\n",
            "Epoch 3, Loss: 0.584439\n",
            "Epoch 4, Loss: 0.581374\n",
            "Epoch 5, Loss: 0.578959\n",
            "Epoch 6, Loss: 0.576824\n",
            "Epoch 7, Loss: 0.575387\n",
            "Epoch 8, Loss: 0.573267\n",
            "Epoch 9, Loss: 0.571824\n",
            "Epoch 10, Loss: 0.570065\n",
            "Epoch 11, Loss: 0.568624\n",
            "Epoch 12, Loss: 0.567288\n",
            "Epoch 13, Loss: 0.566094\n",
            "Epoch 14, Loss: 0.565251\n",
            "Epoch 15, Loss: 0.564008\n",
            "Epoch 16, Loss: 0.563337\n",
            "Epoch 17, Loss: 0.562836\n",
            "Epoch 18, Loss: 0.562038\n",
            "Epoch 19, Loss: 0.561691\n",
            "Epoch 20, Loss: 0.560985\n",
            "Epoch 21, Loss: 0.560896\n",
            "Epoch 22, Loss: 0.560598\n",
            "Epoch 23, Loss: 0.560006\n",
            "Epoch 24, Loss: 0.559270\n",
            "Epoch 25, Loss: 0.559186\n",
            "Epoch 26, Loss: 0.559062\n",
            "Epoch 27, Loss: 0.558771\n",
            "Epoch 28, Loss: 0.558615\n",
            "Epoch 29, Loss: 0.558410\n",
            "Epoch 30, Loss: 0.558228\n",
            "Epoch 31, Loss: 0.557929\n",
            "Epoch 32, Loss: 0.557705\n",
            "Epoch 33, Loss: 0.557383\n",
            "Epoch 34, Loss: 0.557115\n",
            "Epoch 35, Loss: 0.556975\n",
            "Epoch 36, Loss: 0.556872\n",
            "Epoch 37, Loss: 0.556384\n",
            "Epoch 38, Loss: 0.556402\n",
            "Epoch 39, Loss: 0.555827\n",
            "Epoch 40, Loss: 0.555766\n",
            "Epoch 41, Loss: 0.555911\n",
            "Epoch 42, Loss: 0.555447\n",
            "Epoch 43, Loss: 0.555041\n",
            "Epoch 44, Loss: 0.554840\n",
            "Epoch 45, Loss: 0.554415\n",
            "Epoch 46, Loss: 0.554585\n",
            "Epoch 47, Loss: 0.554173\n",
            "Epoch 48, Loss: 0.553998\n",
            "Epoch 49, Loss: 0.553804\n",
            "Epoch 50, Loss: 0.554000\n",
            "Attack model for class 0 trained.\n",
            "Epoch 1, Loss: 0.570937\n",
            "Epoch 2, Loss: 0.549951\n",
            "Epoch 3, Loss: 0.546792\n",
            "Epoch 4, Loss: 0.545564\n",
            "Epoch 5, Loss: 0.544701\n",
            "Epoch 6, Loss: 0.543319\n",
            "Epoch 7, Loss: 0.543040\n",
            "Epoch 8, Loss: 0.542032\n",
            "Epoch 9, Loss: 0.541294\n",
            "Epoch 10, Loss: 0.541033\n",
            "Epoch 11, Loss: 0.540421\n",
            "Epoch 12, Loss: 0.540007\n",
            "Epoch 13, Loss: 0.539264\n",
            "Epoch 14, Loss: 0.539209\n",
            "Epoch 15, Loss: 0.538872\n",
            "Epoch 16, Loss: 0.538311\n",
            "Epoch 17, Loss: 0.538001\n",
            "Epoch 18, Loss: 0.537656\n",
            "Epoch 19, Loss: 0.537891\n",
            "Epoch 20, Loss: 0.537586\n",
            "Epoch 21, Loss: 0.537441\n",
            "Epoch 22, Loss: 0.537122\n",
            "Epoch 23, Loss: 0.536656\n",
            "Epoch 24, Loss: 0.536504\n",
            "Epoch 25, Loss: 0.536451\n",
            "Epoch 26, Loss: 0.536004\n",
            "Epoch 27, Loss: 0.536143\n",
            "Epoch 28, Loss: 0.535991\n",
            "Epoch 29, Loss: 0.535665\n",
            "Epoch 30, Loss: 0.535840\n",
            "Epoch 31, Loss: 0.535646\n",
            "Epoch 32, Loss: 0.535200\n",
            "Epoch 33, Loss: 0.534889\n",
            "Epoch 34, Loss: 0.535052\n",
            "Epoch 35, Loss: 0.535010\n",
            "Epoch 36, Loss: 0.534347\n",
            "Epoch 37, Loss: 0.534583\n",
            "Epoch 38, Loss: 0.534529\n",
            "Epoch 39, Loss: 0.534425\n",
            "Epoch 40, Loss: 0.534203\n",
            "Epoch 41, Loss: 0.533968\n",
            "Epoch 42, Loss: 0.534004\n",
            "Epoch 43, Loss: 0.533592\n",
            "Epoch 44, Loss: 0.533499\n",
            "Epoch 45, Loss: 0.533571\n",
            "Epoch 46, Loss: 0.533407\n",
            "Epoch 47, Loss: 0.533457\n",
            "Epoch 48, Loss: 0.533400\n",
            "Epoch 49, Loss: 0.533150\n",
            "Epoch 50, Loss: 0.533026\n",
            "Attack model for class 1 trained.\n",
            "Epoch 1, Loss: 0.593880\n",
            "Epoch 2, Loss: 0.554992\n",
            "Epoch 3, Loss: 0.541154\n",
            "Epoch 4, Loss: 0.533853\n",
            "Epoch 5, Loss: 0.529976\n",
            "Epoch 6, Loss: 0.527277\n",
            "Epoch 7, Loss: 0.524536\n",
            "Epoch 8, Loss: 0.522388\n",
            "Epoch 9, Loss: 0.520422\n",
            "Epoch 10, Loss: 0.518335\n",
            "Epoch 11, Loss: 0.517135\n",
            "Epoch 12, Loss: 0.515530\n",
            "Epoch 13, Loss: 0.514049\n",
            "Epoch 14, Loss: 0.512534\n",
            "Epoch 15, Loss: 0.511968\n",
            "Epoch 16, Loss: 0.511074\n",
            "Epoch 17, Loss: 0.509737\n",
            "Epoch 18, Loss: 0.509294\n",
            "Epoch 19, Loss: 0.508612\n",
            "Epoch 20, Loss: 0.507720\n",
            "Epoch 21, Loss: 0.507674\n",
            "Epoch 22, Loss: 0.507636\n",
            "Epoch 23, Loss: 0.506371\n",
            "Epoch 24, Loss: 0.506353\n",
            "Epoch 25, Loss: 0.505894\n",
            "Epoch 26, Loss: 0.506286\n",
            "Epoch 27, Loss: 0.505527\n",
            "Epoch 28, Loss: 0.505526\n",
            "Epoch 29, Loss: 0.504839\n",
            "Epoch 30, Loss: 0.504915\n",
            "Epoch 31, Loss: 0.504854\n",
            "Epoch 32, Loss: 0.504157\n",
            "Epoch 33, Loss: 0.504208\n",
            "Epoch 34, Loss: 0.504160\n",
            "Epoch 35, Loss: 0.503997\n",
            "Epoch 36, Loss: 0.503840\n",
            "Epoch 37, Loss: 0.503818\n",
            "Epoch 38, Loss: 0.503328\n",
            "Epoch 39, Loss: 0.502312\n",
            "Epoch 40, Loss: 0.502870\n",
            "Epoch 41, Loss: 0.502885\n",
            "Epoch 42, Loss: 0.503059\n",
            "Epoch 43, Loss: 0.503039\n",
            "Epoch 44, Loss: 0.502368\n",
            "Epoch 45, Loss: 0.502789\n",
            "Epoch 46, Loss: 0.502729\n",
            "Epoch 47, Loss: 0.502481\n",
            "Epoch 48, Loss: 0.502557\n",
            "Epoch 49, Loss: 0.502156\n",
            "Epoch 50, Loss: 0.502162\n",
            "Attack model for class 2 trained.\n",
            "Epoch 1, Loss: 0.569328\n",
            "Epoch 2, Loss: 0.511767\n",
            "Epoch 3, Loss: 0.493221\n",
            "Epoch 4, Loss: 0.485680\n",
            "Epoch 5, Loss: 0.483148\n",
            "Epoch 6, Loss: 0.481102\n",
            "Epoch 7, Loss: 0.479873\n",
            "Epoch 8, Loss: 0.478575\n",
            "Epoch 9, Loss: 0.478110\n",
            "Epoch 10, Loss: 0.477258\n",
            "Epoch 11, Loss: 0.476151\n",
            "Epoch 12, Loss: 0.475460\n",
            "Epoch 13, Loss: 0.474856\n",
            "Epoch 14, Loss: 0.474249\n",
            "Epoch 15, Loss: 0.473031\n",
            "Epoch 16, Loss: 0.472557\n",
            "Epoch 17, Loss: 0.472483\n",
            "Epoch 18, Loss: 0.471609\n",
            "Epoch 19, Loss: 0.471492\n",
            "Epoch 20, Loss: 0.471083\n",
            "Epoch 21, Loss: 0.470629\n",
            "Epoch 22, Loss: 0.470312\n",
            "Epoch 23, Loss: 0.469544\n",
            "Epoch 24, Loss: 0.469344\n",
            "Epoch 25, Loss: 0.469137\n",
            "Epoch 26, Loss: 0.468700\n",
            "Epoch 27, Loss: 0.468637\n",
            "Epoch 28, Loss: 0.467528\n",
            "Epoch 29, Loss: 0.467889\n",
            "Epoch 30, Loss: 0.467573\n",
            "Epoch 31, Loss: 0.467392\n",
            "Epoch 32, Loss: 0.467260\n",
            "Epoch 33, Loss: 0.466952\n",
            "Epoch 34, Loss: 0.466962\n",
            "Epoch 35, Loss: 0.466835\n",
            "Epoch 36, Loss: 0.466500\n",
            "Epoch 37, Loss: 0.466271\n",
            "Epoch 38, Loss: 0.466527\n",
            "Epoch 39, Loss: 0.465980\n",
            "Epoch 40, Loss: 0.466356\n",
            "Epoch 41, Loss: 0.465597\n",
            "Epoch 42, Loss: 0.465253\n",
            "Epoch 43, Loss: 0.465793\n",
            "Epoch 44, Loss: 0.465231\n",
            "Epoch 45, Loss: 0.465452\n",
            "Epoch 46, Loss: 0.465035\n",
            "Epoch 47, Loss: 0.465138\n",
            "Epoch 48, Loss: 0.464918\n",
            "Epoch 49, Loss: 0.465040\n",
            "Epoch 50, Loss: 0.464486\n",
            "Attack model for class 3 trained.\n",
            "Epoch 1, Loss: 0.600309\n",
            "Epoch 2, Loss: 0.561833\n",
            "Epoch 3, Loss: 0.543920\n",
            "Epoch 4, Loss: 0.533496\n",
            "Epoch 5, Loss: 0.527576\n",
            "Epoch 6, Loss: 0.523466\n",
            "Epoch 7, Loss: 0.520283\n",
            "Epoch 8, Loss: 0.518130\n",
            "Epoch 9, Loss: 0.515472\n",
            "Epoch 10, Loss: 0.513489\n",
            "Epoch 11, Loss: 0.511721\n",
            "Epoch 12, Loss: 0.509676\n",
            "Epoch 13, Loss: 0.508257\n",
            "Epoch 14, Loss: 0.506909\n",
            "Epoch 15, Loss: 0.505426\n",
            "Epoch 16, Loss: 0.504440\n",
            "Epoch 17, Loss: 0.503214\n",
            "Epoch 18, Loss: 0.502334\n",
            "Epoch 19, Loss: 0.501339\n",
            "Epoch 20, Loss: 0.500140\n",
            "Epoch 21, Loss: 0.499940\n",
            "Epoch 22, Loss: 0.499476\n",
            "Epoch 23, Loss: 0.498862\n",
            "Epoch 24, Loss: 0.498562\n",
            "Epoch 25, Loss: 0.498032\n",
            "Epoch 26, Loss: 0.497521\n",
            "Epoch 27, Loss: 0.497164\n",
            "Epoch 28, Loss: 0.496930\n",
            "Epoch 29, Loss: 0.496354\n",
            "Epoch 30, Loss: 0.496339\n",
            "Epoch 31, Loss: 0.496358\n",
            "Epoch 32, Loss: 0.495334\n",
            "Epoch 33, Loss: 0.495452\n",
            "Epoch 34, Loss: 0.495472\n",
            "Epoch 35, Loss: 0.494695\n",
            "Epoch 36, Loss: 0.495440\n",
            "Epoch 37, Loss: 0.495006\n",
            "Epoch 38, Loss: 0.494973\n",
            "Epoch 39, Loss: 0.494723\n",
            "Epoch 40, Loss: 0.494611\n",
            "Epoch 41, Loss: 0.493874\n",
            "Epoch 42, Loss: 0.494021\n",
            "Epoch 43, Loss: 0.494263\n",
            "Epoch 44, Loss: 0.493870\n",
            "Epoch 45, Loss: 0.493266\n",
            "Epoch 46, Loss: 0.492990\n",
            "Epoch 47, Loss: 0.493179\n",
            "Epoch 48, Loss: 0.493334\n",
            "Epoch 49, Loss: 0.492751\n",
            "Epoch 50, Loss: 0.492620\n",
            "Attack model for class 4 trained.\n",
            "Epoch 1, Loss: 0.591203\n",
            "Epoch 2, Loss: 0.548193\n",
            "Epoch 3, Loss: 0.538627\n",
            "Epoch 4, Loss: 0.534296\n",
            "Epoch 5, Loss: 0.531688\n",
            "Epoch 6, Loss: 0.529725\n",
            "Epoch 7, Loss: 0.527444\n",
            "Epoch 8, Loss: 0.525888\n",
            "Epoch 9, Loss: 0.525343\n",
            "Epoch 10, Loss: 0.523607\n",
            "Epoch 11, Loss: 0.522555\n",
            "Epoch 12, Loss: 0.521445\n",
            "Epoch 13, Loss: 0.520756\n",
            "Epoch 14, Loss: 0.519426\n",
            "Epoch 15, Loss: 0.519267\n",
            "Epoch 16, Loss: 0.518488\n",
            "Epoch 17, Loss: 0.517708\n",
            "Epoch 18, Loss: 0.516669\n",
            "Epoch 19, Loss: 0.517032\n",
            "Epoch 20, Loss: 0.515979\n",
            "Epoch 21, Loss: 0.515777\n",
            "Epoch 22, Loss: 0.515339\n",
            "Epoch 23, Loss: 0.514878\n",
            "Epoch 24, Loss: 0.514666\n",
            "Epoch 25, Loss: 0.514485\n",
            "Epoch 26, Loss: 0.513774\n",
            "Epoch 27, Loss: 0.513570\n",
            "Epoch 28, Loss: 0.513243\n",
            "Epoch 29, Loss: 0.513055\n",
            "Epoch 30, Loss: 0.512942\n",
            "Epoch 31, Loss: 0.512747\n",
            "Epoch 32, Loss: 0.512635\n",
            "Epoch 33, Loss: 0.512503\n",
            "Epoch 34, Loss: 0.512393\n",
            "Epoch 35, Loss: 0.512097\n",
            "Epoch 36, Loss: 0.511932\n",
            "Epoch 37, Loss: 0.511761\n",
            "Epoch 38, Loss: 0.511731\n",
            "Epoch 39, Loss: 0.511612\n",
            "Epoch 40, Loss: 0.511575\n",
            "Epoch 41, Loss: 0.511160\n",
            "Epoch 42, Loss: 0.511333\n",
            "Epoch 43, Loss: 0.511304\n",
            "Epoch 44, Loss: 0.511272\n",
            "Epoch 45, Loss: 0.511176\n",
            "Epoch 46, Loss: 0.510813\n",
            "Epoch 47, Loss: 0.510781\n",
            "Epoch 48, Loss: 0.510744\n",
            "Epoch 49, Loss: 0.510942\n",
            "Epoch 50, Loss: 0.510887\n",
            "Attack model for class 5 trained.\n",
            "Epoch 1, Loss: 0.588600\n",
            "Epoch 2, Loss: 0.561541\n",
            "Epoch 3, Loss: 0.555532\n",
            "Epoch 4, Loss: 0.552692\n",
            "Epoch 5, Loss: 0.551715\n",
            "Epoch 6, Loss: 0.550146\n",
            "Epoch 7, Loss: 0.549216\n",
            "Epoch 8, Loss: 0.547878\n",
            "Epoch 9, Loss: 0.547434\n",
            "Epoch 10, Loss: 0.546070\n",
            "Epoch 11, Loss: 0.545665\n",
            "Epoch 12, Loss: 0.544786\n",
            "Epoch 13, Loss: 0.543580\n",
            "Epoch 14, Loss: 0.543272\n",
            "Epoch 15, Loss: 0.542736\n",
            "Epoch 16, Loss: 0.542060\n",
            "Epoch 17, Loss: 0.541543\n",
            "Epoch 18, Loss: 0.540622\n",
            "Epoch 19, Loss: 0.540160\n",
            "Epoch 20, Loss: 0.540130\n",
            "Epoch 21, Loss: 0.539620\n",
            "Epoch 22, Loss: 0.539254\n",
            "Epoch 23, Loss: 0.538836\n",
            "Epoch 24, Loss: 0.538764\n",
            "Epoch 25, Loss: 0.538566\n",
            "Epoch 26, Loss: 0.538300\n",
            "Epoch 27, Loss: 0.538099\n",
            "Epoch 28, Loss: 0.537607\n",
            "Epoch 29, Loss: 0.537651\n",
            "Epoch 30, Loss: 0.537318\n",
            "Epoch 31, Loss: 0.536970\n",
            "Epoch 32, Loss: 0.536874\n",
            "Epoch 33, Loss: 0.536925\n",
            "Epoch 34, Loss: 0.536376\n",
            "Epoch 35, Loss: 0.536431\n",
            "Epoch 36, Loss: 0.536220\n",
            "Epoch 37, Loss: 0.536110\n",
            "Epoch 38, Loss: 0.535745\n",
            "Epoch 39, Loss: 0.535788\n",
            "Epoch 40, Loss: 0.535401\n",
            "Epoch 41, Loss: 0.535454\n",
            "Epoch 42, Loss: 0.534687\n",
            "Epoch 43, Loss: 0.534905\n",
            "Epoch 44, Loss: 0.534596\n",
            "Epoch 45, Loss: 0.534927\n",
            "Epoch 46, Loss: 0.535154\n",
            "Epoch 47, Loss: 0.534682\n",
            "Epoch 48, Loss: 0.534887\n",
            "Epoch 49, Loss: 0.534363\n",
            "Epoch 50, Loss: 0.534521\n",
            "Attack model for class 6 trained.\n",
            "Epoch 1, Loss: 0.580335\n",
            "Epoch 2, Loss: 0.553147\n",
            "Epoch 3, Loss: 0.548280\n",
            "Epoch 4, Loss: 0.544919\n",
            "Epoch 5, Loss: 0.543631\n",
            "Epoch 6, Loss: 0.541744\n",
            "Epoch 7, Loss: 0.539750\n",
            "Epoch 8, Loss: 0.538335\n",
            "Epoch 9, Loss: 0.537987\n",
            "Epoch 10, Loss: 0.536740\n",
            "Epoch 11, Loss: 0.535889\n",
            "Epoch 12, Loss: 0.535036\n",
            "Epoch 13, Loss: 0.534686\n",
            "Epoch 14, Loss: 0.533813\n",
            "Epoch 15, Loss: 0.533582\n",
            "Epoch 16, Loss: 0.532944\n",
            "Epoch 17, Loss: 0.532775\n",
            "Epoch 18, Loss: 0.532481\n",
            "Epoch 19, Loss: 0.532239\n",
            "Epoch 20, Loss: 0.531964\n",
            "Epoch 21, Loss: 0.531357\n",
            "Epoch 22, Loss: 0.531720\n",
            "Epoch 23, Loss: 0.531426\n",
            "Epoch 24, Loss: 0.531238\n",
            "Epoch 25, Loss: 0.530957\n",
            "Epoch 26, Loss: 0.530858\n",
            "Epoch 27, Loss: 0.530286\n",
            "Epoch 28, Loss: 0.530343\n",
            "Epoch 29, Loss: 0.530666\n",
            "Epoch 30, Loss: 0.530352\n",
            "Epoch 31, Loss: 0.530404\n",
            "Epoch 32, Loss: 0.529934\n",
            "Epoch 33, Loss: 0.530015\n",
            "Epoch 34, Loss: 0.529728\n",
            "Epoch 35, Loss: 0.529938\n",
            "Epoch 36, Loss: 0.529300\n",
            "Epoch 37, Loss: 0.529572\n",
            "Epoch 38, Loss: 0.529519\n",
            "Epoch 39, Loss: 0.529203\n",
            "Epoch 40, Loss: 0.529417\n",
            "Epoch 41, Loss: 0.528924\n",
            "Epoch 42, Loss: 0.528911\n",
            "Epoch 43, Loss: 0.528893\n",
            "Epoch 44, Loss: 0.528865\n",
            "Epoch 45, Loss: 0.528649\n",
            "Epoch 46, Loss: 0.528679\n",
            "Epoch 47, Loss: 0.528484\n",
            "Epoch 48, Loss: 0.528506\n",
            "Epoch 49, Loss: 0.528368\n",
            "Epoch 50, Loss: 0.528350\n",
            "Attack model for class 7 trained.\n",
            "Epoch 1, Loss: 0.591427\n",
            "Epoch 2, Loss: 0.572834\n",
            "Epoch 3, Loss: 0.569492\n",
            "Epoch 4, Loss: 0.567468\n",
            "Epoch 5, Loss: 0.566176\n",
            "Epoch 6, Loss: 0.564884\n",
            "Epoch 7, Loss: 0.563750\n",
            "Epoch 8, Loss: 0.562791\n",
            "Epoch 9, Loss: 0.561767\n",
            "Epoch 10, Loss: 0.561134\n",
            "Epoch 11, Loss: 0.560180\n",
            "Epoch 12, Loss: 0.559603\n",
            "Epoch 13, Loss: 0.558533\n",
            "Epoch 14, Loss: 0.558398\n",
            "Epoch 15, Loss: 0.557737\n",
            "Epoch 16, Loss: 0.556893\n",
            "Epoch 17, Loss: 0.556611\n",
            "Epoch 18, Loss: 0.556217\n",
            "Epoch 19, Loss: 0.555924\n",
            "Epoch 20, Loss: 0.555238\n",
            "Epoch 21, Loss: 0.555108\n",
            "Epoch 22, Loss: 0.554404\n",
            "Epoch 23, Loss: 0.554171\n",
            "Epoch 24, Loss: 0.553764\n",
            "Epoch 25, Loss: 0.553723\n",
            "Epoch 26, Loss: 0.553147\n",
            "Epoch 27, Loss: 0.553130\n",
            "Epoch 28, Loss: 0.552435\n",
            "Epoch 29, Loss: 0.552321\n",
            "Epoch 30, Loss: 0.552171\n",
            "Epoch 31, Loss: 0.551854\n",
            "Epoch 32, Loss: 0.551525\n",
            "Epoch 33, Loss: 0.551733\n",
            "Epoch 34, Loss: 0.551159\n",
            "Epoch 35, Loss: 0.550817\n",
            "Epoch 36, Loss: 0.550910\n",
            "Epoch 37, Loss: 0.550865\n",
            "Epoch 38, Loss: 0.550450\n",
            "Epoch 39, Loss: 0.550162\n",
            "Epoch 40, Loss: 0.550035\n",
            "Epoch 41, Loss: 0.550138\n",
            "Epoch 42, Loss: 0.550075\n",
            "Epoch 43, Loss: 0.549783\n",
            "Epoch 44, Loss: 0.549666\n",
            "Epoch 45, Loss: 0.549468\n",
            "Epoch 46, Loss: 0.549657\n",
            "Epoch 47, Loss: 0.549001\n",
            "Epoch 48, Loss: 0.549193\n",
            "Epoch 49, Loss: 0.548686\n",
            "Epoch 50, Loss: 0.548950\n",
            "Attack model for class 8 trained.\n",
            "Epoch 1, Loss: 0.551505\n",
            "Epoch 2, Loss: 0.513083\n",
            "Epoch 3, Loss: 0.506639\n",
            "Epoch 4, Loss: 0.504851\n",
            "Epoch 5, Loss: 0.503182\n",
            "Epoch 6, Loss: 0.503040\n",
            "Epoch 7, Loss: 0.501789\n",
            "Epoch 8, Loss: 0.501440\n",
            "Epoch 9, Loss: 0.500551\n",
            "Epoch 10, Loss: 0.500542\n",
            "Epoch 11, Loss: 0.499886\n",
            "Epoch 12, Loss: 0.499142\n",
            "Epoch 13, Loss: 0.498717\n",
            "Epoch 14, Loss: 0.498025\n",
            "Epoch 15, Loss: 0.497886\n",
            "Epoch 16, Loss: 0.497719\n",
            "Epoch 17, Loss: 0.496889\n",
            "Epoch 18, Loss: 0.496771\n",
            "Epoch 19, Loss: 0.496378\n",
            "Epoch 20, Loss: 0.495915\n",
            "Epoch 21, Loss: 0.495970\n",
            "Epoch 22, Loss: 0.495820\n",
            "Epoch 23, Loss: 0.495406\n",
            "Epoch 24, Loss: 0.495022\n",
            "Epoch 25, Loss: 0.494856\n",
            "Epoch 26, Loss: 0.494571\n",
            "Epoch 27, Loss: 0.494021\n",
            "Epoch 28, Loss: 0.494346\n",
            "Epoch 29, Loss: 0.494024\n",
            "Epoch 30, Loss: 0.493713\n",
            "Epoch 31, Loss: 0.493794\n",
            "Epoch 32, Loss: 0.493633\n",
            "Epoch 33, Loss: 0.493472\n",
            "Epoch 34, Loss: 0.492906\n",
            "Epoch 35, Loss: 0.492951\n",
            "Epoch 36, Loss: 0.492911\n",
            "Epoch 37, Loss: 0.493115\n",
            "Epoch 38, Loss: 0.492761\n",
            "Epoch 39, Loss: 0.492739\n",
            "Epoch 40, Loss: 0.492702\n",
            "Epoch 41, Loss: 0.492446\n",
            "Epoch 42, Loss: 0.492415\n",
            "Epoch 43, Loss: 0.491831\n",
            "Epoch 44, Loss: 0.492311\n",
            "Epoch 45, Loss: 0.491911\n",
            "Epoch 46, Loss: 0.492032\n",
            "Epoch 47, Loss: 0.492087\n",
            "Epoch 48, Loss: 0.492030\n",
            "Epoch 49, Loss: 0.491950\n",
            "Epoch 50, Loss: 0.491604\n",
            "Attack model for class 9 trained.\n",
            "Attack model for class 0 saved to attack_model_class_0.pth\n",
            "Attack model for class 1 saved to attack_model_class_1.pth\n",
            "Attack model for class 2 saved to attack_model_class_2.pth\n",
            "Attack model for class 3 saved to attack_model_class_3.pth\n",
            "Attack model for class 4 saved to attack_model_class_4.pth\n",
            "Attack model for class 5 saved to attack_model_class_5.pth\n",
            "Attack model for class 6 saved to attack_model_class_6.pth\n",
            "Attack model for class 7 saved to attack_model_class_7.pth\n",
            "Attack model for class 8 saved to attack_model_class_8.pth\n",
            "Attack model for class 9 saved to attack_model_class_9.pth\n",
            "Attack model for class 0 loaded from attack_model_class_0.pth\n",
            "Attack model for class 1 loaded from attack_model_class_1.pth\n",
            "Attack model for class 2 loaded from attack_model_class_2.pth\n",
            "Attack model for class 3 loaded from attack_model_class_3.pth\n",
            "Attack model for class 4 loaded from attack_model_class_4.pth\n",
            "Attack model for class 5 loaded from attack_model_class_5.pth\n",
            "Attack model for class 6 loaded from attack_model_class_6.pth\n",
            "Attack model for class 7 loaded from attack_model_class_7.pth\n",
            "Attack model for class 8 loaded from attack_model_class_8.pth\n",
            "Attack model for class 9 loaded from attack_model_class_9.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Accuracies"
      ],
      "metadata": {
        "id": "SLO_nmHw1iUc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_base = CIFAR10Classifier()\n",
        "model_base.load_state_dict(torch.load('baseline_model.pth', map_location=device))\n",
        "model_base.to(device)\n",
        "\n",
        "accuracy_baseline = mia.evaluate_attack_model(train_loader_baseline, test_loader_baseline, model_base)\n",
        "print(f'Accuracy for Attacking to the Baseline Model :  {accuracy_baseline * 100:.2f}%')"
      ],
      "metadata": {
        "id": "7xpJvTOqaFWZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e987dcd1-389b-4cc0-e025-523e671c5858"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for Attacking to the Baseline Model :  77.37%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_module_prefix(state_dict):\n",
        "    new_state_dict = {}\n",
        "    for k, v in state_dict.items():\n",
        "        if k.startswith('_module.'):\n",
        "            new_state_dict[k[8:]] = v  # remove '_module.' prefix\n",
        "        else:\n",
        "            new_state_dict[k] = v\n",
        "    return new_state_dict\n",
        "\n",
        "model_private = CIFAR10Classifier().to(device)\n",
        "private_state_dict = torch.load('modified_model.pth', map_location=device)\n",
        "model_private.load_state_dict(remove_module_prefix(private_state_dict))\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model_private.to(device)\n",
        "\n",
        "accuracy_private = mia.evaluate_attack_model(train_loader_modified, test_loader_modified, model_private)\n",
        "print(f'Accuracy for Attacking to the Private Model :  {accuracy_private * 100:.2f}%')"
      ],
      "metadata": {
        "id": "ejgA8yg0aF72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3ae7707-4081-42e4-b691-f449e53ccdd5"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for Attacking to the Private Model :  61.13%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using Random Forest for Attacker Model"
      ],
      "metadata": {
        "id": "Sbl0AvupGb6T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### importing libraries"
      ],
      "metadata": {
        "id": "cQ8RO_8KHFgU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Subset, ConcatDataset\n",
        "from model import CIFAR10Classifier\n",
        "import torch.nn.functional as F\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "import joblib  # For saving/loading models"
      ],
      "metadata": {
        "id": "cwi_MYB7H12K"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attack Model without privacy"
      ],
      "metadata": {
        "id": "egsduyzELBfG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We defined a Class for Membership Inference Attack with No Privacy.\n"
      ],
      "metadata": {
        "id": "qg6YyUPSMEO8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MembershipInferenceAttackNoPrivacy:\n",
        "    def __init__(self, shadow_model_class, device='cpu'):\n",
        "        self.shadow_model_class = shadow_model_class\n",
        "        self.device = device\n",
        "        self.attack_models = {}\n",
        "\n",
        "    def train_shadow_models(self, seen_loaders, num_epochs=10, lr=1e-3):\n",
        "        self.shadow_models = [self.shadow_model_class().to(self.device) for _ in range(len(seen_loaders))]\n",
        "\n",
        "        for i, (shadow_model, seen_loader) in enumerate(zip(self.shadow_models, seen_loaders)):\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "            optimizer = optim.Adam(shadow_model.parameters(), lr=lr)\n",
        "            self.train_model(shadow_model, seen_loader, criterion, optimizer, num_epochs)\n",
        "            print(f'Shadow model {i+1} trained.')\n",
        "\n",
        "    def train_model(self, model, dataloader, criterion, optimizer, num_epochs):\n",
        "        model.train()\n",
        "        for epoch in range(num_epochs):\n",
        "            running_loss = 0.0\n",
        "            for inputs, labels in dataloader:\n",
        "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                running_loss += loss.item()\n",
        "            print(f'Epoch {epoch+1}, Loss: {running_loss/len(dataloader):.6f}')\n",
        "\n",
        "    def collect_outputs(self, seen_loaders, unseen_loaders):\n",
        "        self.attack_data = []\n",
        "        self.attack_labels = []\n",
        "\n",
        "        for shadow_model, seen_loader, unseen_loader in zip(self.shadow_models, seen_loaders, unseen_loaders):\n",
        "            self.collect_shadow_model_outputs(shadow_model, seen_loader, label=1)  # in\n",
        "            self.collect_shadow_model_outputs(shadow_model, unseen_loader, label=0)  # out\n",
        "\n",
        "        self.attack_data = torch.cat(self.attack_data).cpu().numpy()\n",
        "        self.attack_labels = torch.cat(self.attack_labels).cpu().numpy()\n",
        "\n",
        "    def collect_shadow_model_outputs(self, model, dataloader, label):\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in dataloader:\n",
        "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
        "                outputs = model(inputs)\n",
        "                probabilities = F.softmax(outputs, dim=1)\n",
        "                self.attack_data.append(torch.cat([probabilities, labels.unsqueeze(1).float()], dim=1))\n",
        "                self.attack_labels.append(torch.full((outputs.size(0),), label, dtype=torch.float))\n",
        "\n",
        "    def train_attack_models(self):\n",
        "        for class_label in range(10):  # Assuming 10 classes\n",
        "            class_indices = (self.attack_data[:, -1] == class_label)\n",
        "            class_data = self.attack_data[class_indices][:, :-1]  # Exclude the last column (class label)\n",
        "            class_labels = self.attack_labels[class_indices]  # Binary labels (in or out)\n",
        "\n",
        "            attack_model = RandomForestClassifier(n_estimators=50)\n",
        "            attack_model.fit(class_data, class_labels)\n",
        "\n",
        "            # Evaluate performance\n",
        "            train_predictions = attack_model.predict(class_data)\n",
        "            train_accuracy = accuracy_score(class_labels, train_predictions)\n",
        "            train_precision = precision_score(class_labels, train_predictions)\n",
        "            train_recall = recall_score(class_labels, train_predictions)\n",
        "            train_f1 = f1_score(class_labels, train_predictions)\n",
        "            train_confusion_matrix = confusion_matrix(class_labels, train_predictions)\n",
        "\n",
        "            print(f'Class {class_label}, Training Accuracy: {train_accuracy:.4f}')\n",
        "            print(f'Class {class_label}, Training Precision: {train_precision:.4f}')\n",
        "            print(f'Class {class_label}, Training Recall: {train_recall:.4f}')\n",
        "            print(f'Class {class_label}, Training F1 Score: {train_f1:.4f}')\n",
        "            print(f'Class {class_label}, Training Confusion Matrix:\\n {train_confusion_matrix}')\n",
        "\n",
        "            self.attack_models[class_label] = attack_model\n",
        "            print(f'Attack model for class {class_label} trained.')\n",
        "\n",
        "    def save_attack_models(self, path):\n",
        "        for class_label, model in self.attack_models.items():\n",
        "            joblib.dump(model, f'{path}_class_{class_label}.joblib')\n",
        "            print(f'Attack model for class {class_label} saved to {path}_class_{class_label}.joblib')\n",
        "\n",
        "    def load_attack_models(self, path):\n",
        "        for class_label in range(10):  # Assuming 10 classes\n",
        "            model = joblib.load(f'{path}_class_{class_label}.joblib')\n",
        "            self.attack_models[class_label] = model\n",
        "            print(f'Attack model for class {class_label} loaded from {path}_class_{class_label}.joblib')\n",
        "\n",
        "    def infer_membership(self, model, seen_loader, unseen_loader, seen_outputs, unseen_outputs, labels):\n",
        "        model_outputs = torch.cat([seen_outputs, unseen_outputs]).cpu().numpy()\n",
        "        labels = labels.cpu().numpy()\n",
        "\n",
        "        # print(\"In Infer : \")\n",
        "        # print(f\"model_outputs size: {model_outputs.shape}\")\n",
        "        # print(f\"labels size: {len(labels)}\")\n",
        "\n",
        "        memberships = []\n",
        "        for output, label in zip(model_outputs, labels):\n",
        "            class_label = int(label)\n",
        "            attack_model = self.attack_models[class_label]\n",
        "            membership_pred = attack_model.predict(output.reshape(1, -1))[0]\n",
        "            memberships.append(membership_pred)\n",
        "\n",
        "        # print(f\"memberships size: {len(memberships)}\")\n",
        "        return torch.tensor(memberships, device=self.device)\n",
        "\n",
        "    def evaluate_attack_model(self, seen_loader, unseen_loader, target_model):\n",
        "        seen_outputs, labels_seen = self.get_model_outputs(target_model, seen_loader)\n",
        "        unseen_outputs, labels_unseen = self.get_model_outputs(target_model, unseen_loader)\n",
        "\n",
        "        # print(f\"Seen outputs size: {seen_outputs.size()}\")\n",
        "        # print(f\"Unseen outputs size: {unseen_outputs.size()}\")\n",
        "\n",
        "        attack_data = torch.cat([seen_outputs, unseen_outputs]).to(self.device)\n",
        "        attack_labels = torch.cat([torch.ones(len(seen_outputs)), torch.zeros(len(unseen_outputs))]).to(self.device)\n",
        "        labels = torch.cat([labels_seen, labels_unseen]).to(self.device)\n",
        "\n",
        "        # print(f\"Attack data size: {attack_data.size()}\")\n",
        "        # print(f\"Attack labels size: {attack_labels.size()}\")\n",
        "\n",
        "        memberships = self.infer_membership(target_model, seen_loader, unseen_loader, seen_outputs, unseen_outputs, labels)\n",
        "        membership_preds = torch.tensor(memberships).float()\n",
        "        accuracy = (membership_preds == attack_labels).float().mean().item()\n",
        "        return accuracy\n",
        "\n",
        "    def get_model_outputs(self, model, dataloader):\n",
        "        model.eval()\n",
        "        outputs_list = []\n",
        "        labels_list = []\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in dataloader:\n",
        "                inputs = inputs.to(self.device)\n",
        "                outputs = model(inputs)\n",
        "                probabilities = F.softmax(outputs, dim=1)\n",
        "                outputs_list.append(probabilities)\n",
        "                labels_list.append(labels)\n",
        "        return torch.cat(outputs_list), torch.cat(labels_list)"
      ],
      "metadata": {
        "id": "0QjJCmaoGjks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Attack Model"
      ],
      "metadata": {
        "id": "jdQUy6maOdxu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Num of Shadow Models = 2 , Batch Size = 64 , Epochs = 10 , Num_Estimator = 50"
      ],
      "metadata": {
        "id": "ZyVN-Em7Osad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
        "train_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Split training data into 80% and 20%\n",
        "train_size = int(0.8 * len(train_set))\n",
        "remaining_size = len(train_set) - train_size\n",
        "train_subset, remaining_subset = torch.utils.data.random_split(train_set, [train_size, remaining_size])\n",
        "\n",
        "# Create disjoint loaders for seen data from 80% training data\n",
        "num_shadow_models = 2\n",
        "seen_size_per_model = train_size // num_shadow_models\n",
        "seen_loaders = []\n",
        "\n",
        "for i in range(num_shadow_models):\n",
        "    start_idx = i * seen_size_per_model\n",
        "    end_idx = (i + 1) * seen_size_per_model\n",
        "    seen_indices = torch.arange(start_idx, end_idx)\n",
        "    seen_train_set = Subset(train_subset, seen_indices)\n",
        "    seen_loader = DataLoader(seen_train_set, batch_size=64, shuffle=True)\n",
        "    seen_loaders.append(seen_loader)\n",
        "\n",
        "# Create concatenated unseen data from the remaining 20% of training data and the entire test set\n",
        "unseen_dataset = ConcatDataset([remaining_subset, test_set])\n",
        "unseen_size_per_model = len(unseen_dataset) // num_shadow_models\n",
        "unseen_loaders = []\n",
        "\n",
        "for i in range(num_shadow_models):\n",
        "    start_idx = i * unseen_size_per_model\n",
        "    end_idx = (i + 1) * unseen_size_per_model\n",
        "    unseen_indices = torch.arange(start_idx, end_idx)\n",
        "    unseen_subset = Subset(unseen_dataset, unseen_indices)\n",
        "    unseen_loader = DataLoader(unseen_subset, batch_size=64, shuffle=False)\n",
        "    unseen_loaders.append(unseen_loader)\n",
        "\n",
        "test_loader = DataLoader(test_set, batch_size=64, shuffle=False)\n",
        "\n",
        "# Initialize MembershipInferenceAttackNoPrivacy\n",
        "mia = MembershipInferenceAttackNoPrivacy(CIFAR10Classifier, device)\n",
        "\n",
        "# Train shadow models without differential privacy\n",
        "mia.train_shadow_models(seen_loaders, num_epochs=10)\n",
        "\n",
        "# Collect outputs for attack model\n",
        "mia.collect_outputs(seen_loaders, unseen_loaders)\n",
        "\n",
        "# Train attack models\n",
        "mia.train_attack_models()\n",
        "\n",
        "# Save the attack models\n",
        "mia.save_attack_models('attack_model')\n",
        "\n",
        "# Load the attack models (for future use)\n",
        "mia.load_attack_models('attack_model')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tP19gnecOakz",
        "outputId": "ce60e521-10d1-41e2-be38-2c31d096d452"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch 1, Loss: 1.857926\n",
            "Epoch 2, Loss: 1.580750\n",
            "Epoch 3, Loss: 1.457101\n",
            "Epoch 4, Loss: 1.382254\n",
            "Epoch 5, Loss: 1.333081\n",
            "Epoch 6, Loss: 1.277171\n",
            "Epoch 7, Loss: 1.232877\n",
            "Epoch 8, Loss: 1.187397\n",
            "Epoch 9, Loss: 1.145134\n",
            "Epoch 10, Loss: 1.113797\n",
            "Shadow model 1 trained.\n",
            "Epoch 1, Loss: 1.851497\n",
            "Epoch 2, Loss: 1.594829\n",
            "Epoch 3, Loss: 1.481197\n",
            "Epoch 4, Loss: 1.405747\n",
            "Epoch 5, Loss: 1.348338\n",
            "Epoch 6, Loss: 1.290058\n",
            "Epoch 7, Loss: 1.237226\n",
            "Epoch 8, Loss: 1.194154\n",
            "Epoch 9, Loss: 1.164091\n",
            "Epoch 10, Loss: 1.131637\n",
            "Shadow model 2 trained.\n",
            "Class 0, Training Accuracy: 0.7937\n",
            "Class 0, Training Precision: 0.7627\n",
            "Class 0, Training Recall: 1.0000\n",
            "Class 0, Training F1 Score: 0.8653\n",
            "Class 0, Training Confusion Matrix:\n",
            " [[ 784 1238]\n",
            " [   0 3978]]\n",
            "Attack model for class 0 trained.\n",
            "Class 1, Training Accuracy: 0.7837\n",
            "Class 1, Training Precision: 0.7553\n",
            "Class 1, Training Recall: 0.9998\n",
            "Class 1, Training F1 Score: 0.8605\n",
            "Class 1, Training Confusion Matrix:\n",
            " [[ 698 1297]\n",
            " [   1 4004]]\n",
            "Attack model for class 1 trained.\n",
            "Class 2, Training Accuracy: 0.8303\n",
            "Class 2, Training Precision: 0.7982\n",
            "Class 2, Training Recall: 0.9990\n",
            "Class 2, Training F1 Score: 0.8874\n",
            "Class 2, Training Confusion Matrix:\n",
            " [[ 972 1014]\n",
            " [   4 4010]]\n",
            "Attack model for class 2 trained.\n",
            "Class 3, Training Accuracy: 0.8168\n",
            "Class 3, Training Precision: 0.7849\n",
            "Class 3, Training Recall: 0.9998\n",
            "Class 3, Training F1 Score: 0.8794\n",
            "Class 3, Training Confusion Matrix:\n",
            " [[ 894 1098]\n",
            " [   1 4007]]\n",
            "Attack model for class 3 trained.\n",
            "Class 4, Training Accuracy: 0.8180\n",
            "Class 4, Training Precision: 0.7857\n",
            "Class 4, Training Recall: 1.0000\n",
            "Class 4, Training F1 Score: 0.8800\n",
            "Class 4, Training Confusion Matrix:\n",
            " [[ 905 1092]\n",
            " [   0 4003]]\n",
            "Attack model for class 4 trained.\n",
            "Class 5, Training Accuracy: 0.7955\n",
            "Class 5, Training Precision: 0.7661\n",
            "Class 5, Training Recall: 0.9995\n",
            "Class 5, Training F1 Score: 0.8674\n",
            "Class 5, Training Confusion Matrix:\n",
            " [[ 761 1225]\n",
            " [   2 4012]]\n",
            "Attack model for class 5 trained.\n",
            "Class 6, Training Accuracy: 0.7800\n",
            "Class 6, Training Precision: 0.7508\n",
            "Class 6, Training Recall: 1.0000\n",
            "Class 6, Training F1 Score: 0.8577\n",
            "Class 6, Training Confusion Matrix:\n",
            " [[ 702 1320]\n",
            " [   0 3978]]\n",
            "Attack model for class 6 trained.\n",
            "Class 7, Training Accuracy: 0.7797\n",
            "Class 7, Training Precision: 0.7532\n",
            "Class 7, Training Recall: 0.9993\n",
            "Class 7, Training F1 Score: 0.8590\n",
            "Class 7, Training Confusion Matrix:\n",
            " [[ 652 1319]\n",
            " [   3 4026]]\n",
            "Attack model for class 7 trained.\n",
            "Class 8, Training Accuracy: 0.7797\n",
            "Class 8, Training Precision: 0.7507\n",
            "Class 8, Training Recall: 0.9995\n",
            "Class 8, Training F1 Score: 0.8574\n",
            "Class 8, Training Confusion Matrix:\n",
            " [[ 704 1320]\n",
            " [   2 3974]]\n",
            "Attack model for class 8 trained.\n",
            "Class 9, Training Accuracy: 0.7958\n",
            "Class 9, Training Precision: 0.7655\n",
            "Class 9, Training Recall: 0.9995\n",
            "Class 9, Training F1 Score: 0.8670\n",
            "Class 9, Training Confusion Matrix:\n",
            " [[ 782 1223]\n",
            " [   2 3993]]\n",
            "Attack model for class 9 trained.\n",
            "Attack model for class 0 saved to attack_model_class_0.joblib\n",
            "Attack model for class 1 saved to attack_model_class_1.joblib\n",
            "Attack model for class 2 saved to attack_model_class_2.joblib\n",
            "Attack model for class 3 saved to attack_model_class_3.joblib\n",
            "Attack model for class 4 saved to attack_model_class_4.joblib\n",
            "Attack model for class 5 saved to attack_model_class_5.joblib\n",
            "Attack model for class 6 saved to attack_model_class_6.joblib\n",
            "Attack model for class 7 saved to attack_model_class_7.joblib\n",
            "Attack model for class 8 saved to attack_model_class_8.joblib\n",
            "Attack model for class 9 saved to attack_model_class_9.joblib\n",
            "Attack model for class 0 loaded from attack_model_class_0.joblib\n",
            "Attack model for class 1 loaded from attack_model_class_1.joblib\n",
            "Attack model for class 2 loaded from attack_model_class_2.joblib\n",
            "Attack model for class 3 loaded from attack_model_class_3.joblib\n",
            "Attack model for class 4 loaded from attack_model_class_4.joblib\n",
            "Attack model for class 5 loaded from attack_model_class_5.joblib\n",
            "Attack model for class 6 loaded from attack_model_class_6.joblib\n",
            "Attack model for class 7 loaded from attack_model_class_7.joblib\n",
            "Attack model for class 8 loaded from attack_model_class_8.joblib\n",
            "Attack model for class 9 loaded from attack_model_class_9.joblib\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Accuracies"
      ],
      "metadata": {
        "id": "eAqRQEB2PFbv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_base = CIFAR10Classifier()\n",
        "model_base.load_state_dict(torch.load('baseline_model.pth', map_location=device))\n",
        "model_base.to(device)\n",
        "\n",
        "accuracy_baseline = mia.evaluate_attack_model(train_loader_baseline, test_loader_baseline, model_base)\n",
        "print(f'Accuracy for Attacking to the Baseline Model :  {accuracy_baseline * 100:.2f}%')"
      ],
      "metadata": {
        "id": "VElYv0J4GnDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_module_prefix(state_dict):\n",
        "    new_state_dict = {}\n",
        "    for k, v in state_dict.items():\n",
        "        if k.startswith('_module.'):\n",
        "            new_state_dict[k[8:]] = v  # remove '_module.' prefix\n",
        "        else:\n",
        "            new_state_dict[k] = v\n",
        "    return new_state_dict\n",
        "\n",
        "model_private = CIFAR10Classifier().to(device)\n",
        "private_state_dict = torch.load('modified_model.pth', map_location=device)\n",
        "model_private.load_state_dict(remove_module_prefix(private_state_dict))\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model_private.to(device)\n",
        "\n",
        "accuracy_private = mia.evaluate_attack_model(train_loader_modified, test_loader_modified, model_private)\n",
        "print(f'Accuracy for Attacking to the Private Model :  {accuracy_private * 100:.2f}%')"
      ],
      "metadata": {
        "id": "4-6qhU8_GqX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Num of Shadow Models = 100 (in the article) , Batch Size = 64 , Epochs = 10"
      ],
      "metadata": {
        "id": "YhTRDZcMQXPu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
        "train_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Split training data into 80% and 20%\n",
        "train_size = int(0.8 * len(train_set))\n",
        "remaining_size = len(train_set) - train_size\n",
        "train_subset, remaining_subset = torch.utils.data.random_split(train_set, [train_size, remaining_size])\n",
        "\n",
        "# Create disjoint loaders for seen data from 80% training data\n",
        "num_shadow_models = 100\n",
        "seen_size_per_model = train_size // num_shadow_models\n",
        "seen_loaders = []\n",
        "\n",
        "for i in range(num_shadow_models):\n",
        "    start_idx = i * seen_size_per_model\n",
        "    end_idx = (i + 1) * seen_size_per_model\n",
        "    seen_indices = torch.arange(start_idx, end_idx)\n",
        "    seen_train_set = Subset(train_subset, seen_indices)\n",
        "    seen_loader = DataLoader(seen_train_set, batch_size=64, shuffle=True)\n",
        "    seen_loaders.append(seen_loader)\n",
        "\n",
        "# Create concatenated unseen data from the remaining 20% of training data and the entire test set\n",
        "unseen_dataset = ConcatDataset([remaining_subset, test_set])\n",
        "unseen_size_per_model = len(unseen_dataset) // num_shadow_models\n",
        "unseen_loaders = []\n",
        "\n",
        "for i in range(num_shadow_models):\n",
        "    start_idx = i * unseen_size_per_model\n",
        "    end_idx = (i + 1) * unseen_size_per_model\n",
        "    unseen_indices = torch.arange(start_idx, end_idx)\n",
        "    unseen_subset = Subset(unseen_dataset, unseen_indices)\n",
        "    unseen_loader = DataLoader(unseen_subset, batch_size=64, shuffle=False)\n",
        "    unseen_loaders.append(unseen_loader)\n",
        "\n",
        "test_loader = DataLoader(test_set, batch_size=64, shuffle=False)\n",
        "\n",
        "# Initialize MembershipInferenceAttackNoPrivacy\n",
        "mia = MembershipInferenceAttackNoPrivacy(CIFAR10Classifier, device)\n",
        "\n",
        "# Train shadow models without differential privacy\n",
        "mia.train_shadow_models(seen_loaders, num_epochs=10)\n",
        "\n",
        "# Collect outputs for attack model\n",
        "mia.collect_outputs(seen_loaders, unseen_loaders)\n",
        "\n",
        "# Train attack models\n",
        "mia.train_attack_models()\n",
        "\n",
        "# Save the attack models\n",
        "mia.save_attack_models('attack_model')\n",
        "\n",
        "# Load the attack models (for future use)\n",
        "mia.load_attack_models('attack_model')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9y5csZotQiFc",
        "outputId": "9d9351f8-72bc-47e3-c1e6-34bb6017ba96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch 1, Loss: 2.313570\n",
            "Epoch 2, Loss: 2.215772\n",
            "Epoch 3, Loss: 2.136619\n",
            "Epoch 4, Loss: 2.022963\n",
            "Epoch 5, Loss: 1.892865\n",
            "Epoch 6, Loss: 1.823914\n",
            "Epoch 7, Loss: 1.847620\n",
            "Epoch 8, Loss: 1.713441\n",
            "Epoch 9, Loss: 1.691665\n",
            "Epoch 10, Loss: 1.440415\n",
            "Shadow model 1 trained.\n",
            "Epoch 1, Loss: 2.289453\n",
            "Epoch 2, Loss: 2.182635\n",
            "Epoch 3, Loss: 2.058279\n",
            "Epoch 4, Loss: 1.982899\n",
            "Epoch 5, Loss: 1.876770\n",
            "Epoch 6, Loss: 1.814436\n",
            "Epoch 7, Loss: 1.757686\n",
            "Epoch 8, Loss: 1.624396\n",
            "Epoch 9, Loss: 1.585671\n",
            "Epoch 10, Loss: 1.470452\n",
            "Shadow model 2 trained.\n",
            "Epoch 1, Loss: 2.306730\n",
            "Epoch 2, Loss: 2.190963\n",
            "Epoch 3, Loss: 2.109275\n",
            "Epoch 4, Loss: 1.992381\n",
            "Epoch 5, Loss: 1.827165\n",
            "Epoch 6, Loss: 1.803251\n",
            "Epoch 7, Loss: 1.710317\n",
            "Epoch 8, Loss: 1.574142\n",
            "Epoch 9, Loss: 1.386884\n",
            "Epoch 10, Loss: 1.432828\n",
            "Shadow model 3 trained.\n",
            "Epoch 1, Loss: 2.287187\n",
            "Epoch 2, Loss: 2.112371\n",
            "Epoch 3, Loss: 2.085701\n",
            "Epoch 4, Loss: 1.936148\n",
            "Epoch 5, Loss: 1.773429\n",
            "Epoch 6, Loss: 1.720464\n",
            "Epoch 7, Loss: 1.657607\n",
            "Epoch 8, Loss: 1.525095\n",
            "Epoch 9, Loss: 1.435730\n",
            "Epoch 10, Loss: 1.437448\n",
            "Shadow model 4 trained.\n",
            "Epoch 1, Loss: 2.328375\n",
            "Epoch 2, Loss: 2.201972\n",
            "Epoch 3, Loss: 2.112524\n",
            "Epoch 4, Loss: 2.034598\n",
            "Epoch 5, Loss: 1.910319\n",
            "Epoch 6, Loss: 1.905467\n",
            "Epoch 7, Loss: 1.798461\n",
            "Epoch 8, Loss: 1.717725\n",
            "Epoch 9, Loss: 1.645087\n",
            "Epoch 10, Loss: 1.539000\n",
            "Shadow model 5 trained.\n",
            "Epoch 1, Loss: 2.312306\n",
            "Epoch 2, Loss: 2.226612\n",
            "Epoch 3, Loss: 2.162354\n",
            "Epoch 4, Loss: 2.063996\n",
            "Epoch 5, Loss: 1.968246\n",
            "Epoch 6, Loss: 1.962971\n",
            "Epoch 7, Loss: 1.876544\n",
            "Epoch 8, Loss: 1.747317\n",
            "Epoch 9, Loss: 1.672257\n",
            "Epoch 10, Loss: 1.593239\n",
            "Shadow model 6 trained.\n",
            "Epoch 1, Loss: 2.301866\n",
            "Epoch 2, Loss: 2.221269\n",
            "Epoch 3, Loss: 2.109515\n",
            "Epoch 4, Loss: 2.021818\n",
            "Epoch 5, Loss: 1.959489\n",
            "Epoch 6, Loss: 1.858981\n",
            "Epoch 7, Loss: 1.748825\n",
            "Epoch 8, Loss: 1.689972\n",
            "Epoch 9, Loss: 1.584329\n",
            "Epoch 10, Loss: 1.495978\n",
            "Shadow model 7 trained.\n",
            "Epoch 1, Loss: 2.328307\n",
            "Epoch 2, Loss: 2.175875\n",
            "Epoch 3, Loss: 2.132060\n",
            "Epoch 4, Loss: 2.032607\n",
            "Epoch 5, Loss: 1.987517\n",
            "Epoch 6, Loss: 1.884547\n",
            "Epoch 7, Loss: 1.783266\n",
            "Epoch 8, Loss: 1.653779\n",
            "Epoch 9, Loss: 1.571851\n",
            "Epoch 10, Loss: 1.591361\n",
            "Shadow model 8 trained.\n",
            "Epoch 1, Loss: 2.339946\n",
            "Epoch 2, Loss: 2.237171\n",
            "Epoch 3, Loss: 2.171013\n",
            "Epoch 4, Loss: 2.119569\n",
            "Epoch 5, Loss: 2.072364\n",
            "Epoch 6, Loss: 1.918696\n",
            "Epoch 7, Loss: 1.922339\n",
            "Epoch 8, Loss: 1.836345\n",
            "Epoch 9, Loss: 1.747525\n",
            "Epoch 10, Loss: 1.653995\n",
            "Shadow model 9 trained.\n",
            "Epoch 1, Loss: 2.341596\n",
            "Epoch 2, Loss: 2.247378\n",
            "Epoch 3, Loss: 2.206980\n",
            "Epoch 4, Loss: 2.115139\n",
            "Epoch 5, Loss: 2.030663\n",
            "Epoch 6, Loss: 2.049196\n",
            "Epoch 7, Loss: 1.906092\n",
            "Epoch 8, Loss: 1.866216\n",
            "Epoch 9, Loss: 1.780465\n",
            "Epoch 10, Loss: 1.637899\n",
            "Shadow model 10 trained.\n",
            "Epoch 1, Loss: 2.325069\n",
            "Epoch 2, Loss: 2.231868\n",
            "Epoch 3, Loss: 2.103822\n",
            "Epoch 4, Loss: 2.038995\n",
            "Epoch 5, Loss: 1.917056\n",
            "Epoch 6, Loss: 1.793054\n",
            "Epoch 7, Loss: 1.744575\n",
            "Epoch 8, Loss: 1.634901\n",
            "Epoch 9, Loss: 1.600628\n",
            "Epoch 10, Loss: 1.470186\n",
            "Shadow model 11 trained.\n",
            "Epoch 1, Loss: 2.301801\n",
            "Epoch 2, Loss: 2.202252\n",
            "Epoch 3, Loss: 2.075509\n",
            "Epoch 4, Loss: 1.993312\n",
            "Epoch 5, Loss: 1.900255\n",
            "Epoch 6, Loss: 1.886183\n",
            "Epoch 7, Loss: 1.758055\n",
            "Epoch 8, Loss: 1.677992\n",
            "Epoch 9, Loss: 1.546143\n",
            "Epoch 10, Loss: 1.407365\n",
            "Shadow model 12 trained.\n",
            "Epoch 1, Loss: 2.315208\n",
            "Epoch 2, Loss: 2.217013\n",
            "Epoch 3, Loss: 2.144563\n",
            "Epoch 4, Loss: 2.013228\n",
            "Epoch 5, Loss: 1.961712\n",
            "Epoch 6, Loss: 1.893407\n",
            "Epoch 7, Loss: 1.769016\n",
            "Epoch 8, Loss: 1.716155\n",
            "Epoch 9, Loss: 1.736593\n",
            "Epoch 10, Loss: 1.442425\n",
            "Shadow model 13 trained.\n",
            "Epoch 1, Loss: 2.310509\n",
            "Epoch 2, Loss: 2.236020\n",
            "Epoch 3, Loss: 2.144327\n",
            "Epoch 4, Loss: 2.109687\n",
            "Epoch 5, Loss: 1.983621\n",
            "Epoch 6, Loss: 1.946191\n",
            "Epoch 7, Loss: 1.898537\n",
            "Epoch 8, Loss: 1.745130\n",
            "Epoch 9, Loss: 1.710342\n",
            "Epoch 10, Loss: 1.669434\n",
            "Shadow model 14 trained.\n",
            "Epoch 1, Loss: 2.253453\n",
            "Epoch 2, Loss: 2.124982\n",
            "Epoch 3, Loss: 2.058945\n",
            "Epoch 4, Loss: 1.991751\n",
            "Epoch 5, Loss: 1.877996\n",
            "Epoch 6, Loss: 1.764898\n",
            "Epoch 7, Loss: 1.755720\n",
            "Epoch 8, Loss: 1.650726\n",
            "Epoch 9, Loss: 1.534699\n",
            "Epoch 10, Loss: 1.576962\n",
            "Shadow model 15 trained.\n",
            "Epoch 1, Loss: 2.321978\n",
            "Epoch 2, Loss: 2.244198\n",
            "Epoch 3, Loss: 2.157612\n",
            "Epoch 4, Loss: 2.097245\n",
            "Epoch 5, Loss: 1.981684\n",
            "Epoch 6, Loss: 1.852091\n",
            "Epoch 7, Loss: 1.822632\n",
            "Epoch 8, Loss: 1.715289\n",
            "Epoch 9, Loss: 1.527495\n",
            "Epoch 10, Loss: 1.362597\n",
            "Shadow model 16 trained.\n",
            "Epoch 1, Loss: 2.295278\n",
            "Epoch 2, Loss: 2.187305\n",
            "Epoch 3, Loss: 2.030567\n",
            "Epoch 4, Loss: 1.939708\n",
            "Epoch 5, Loss: 1.856638\n",
            "Epoch 6, Loss: 1.773827\n",
            "Epoch 7, Loss: 1.667271\n",
            "Epoch 8, Loss: 1.598859\n",
            "Epoch 9, Loss: 1.524965\n",
            "Epoch 10, Loss: 1.357864\n",
            "Shadow model 17 trained.\n",
            "Epoch 1, Loss: 2.291689\n",
            "Epoch 2, Loss: 2.199458\n",
            "Epoch 3, Loss: 2.157784\n",
            "Epoch 4, Loss: 1.968110\n",
            "Epoch 5, Loss: 1.909562\n",
            "Epoch 6, Loss: 1.870516\n",
            "Epoch 7, Loss: 1.767909\n",
            "Epoch 8, Loss: 1.635725\n",
            "Epoch 9, Loss: 1.680869\n",
            "Epoch 10, Loss: 1.482216\n",
            "Shadow model 18 trained.\n",
            "Epoch 1, Loss: 2.323473\n",
            "Epoch 2, Loss: 2.212989\n",
            "Epoch 3, Loss: 2.162044\n",
            "Epoch 4, Loss: 2.048978\n",
            "Epoch 5, Loss: 1.914231\n",
            "Epoch 6, Loss: 1.790091\n",
            "Epoch 7, Loss: 1.714571\n",
            "Epoch 8, Loss: 1.636019\n",
            "Epoch 9, Loss: 1.398921\n",
            "Epoch 10, Loss: 1.359566\n",
            "Shadow model 19 trained.\n",
            "Epoch 1, Loss: 2.307652\n",
            "Epoch 2, Loss: 2.218180\n",
            "Epoch 3, Loss: 2.136645\n",
            "Epoch 4, Loss: 2.017433\n",
            "Epoch 5, Loss: 1.951368\n",
            "Epoch 6, Loss: 1.892326\n",
            "Epoch 7, Loss: 1.782079\n",
            "Epoch 8, Loss: 1.716510\n",
            "Epoch 9, Loss: 1.589522\n",
            "Epoch 10, Loss: 1.434690\n",
            "Shadow model 20 trained.\n",
            "Epoch 1, Loss: 2.308637\n",
            "Epoch 2, Loss: 2.195321\n",
            "Epoch 3, Loss: 2.091731\n",
            "Epoch 4, Loss: 2.023221\n",
            "Epoch 5, Loss: 1.945074\n",
            "Epoch 6, Loss: 1.910405\n",
            "Epoch 7, Loss: 1.829975\n",
            "Epoch 8, Loss: 1.733025\n",
            "Epoch 9, Loss: 1.648574\n",
            "Epoch 10, Loss: 1.502316\n",
            "Shadow model 21 trained.\n",
            "Epoch 1, Loss: 2.325484\n",
            "Epoch 2, Loss: 2.237458\n",
            "Epoch 3, Loss: 2.171802\n",
            "Epoch 4, Loss: 2.075430\n",
            "Epoch 5, Loss: 1.988507\n",
            "Epoch 6, Loss: 1.970114\n",
            "Epoch 7, Loss: 1.862665\n",
            "Epoch 8, Loss: 1.762014\n",
            "Epoch 9, Loss: 1.652197\n",
            "Epoch 10, Loss: 1.663334\n",
            "Shadow model 22 trained.\n",
            "Epoch 1, Loss: 2.350352\n",
            "Epoch 2, Loss: 2.230263\n",
            "Epoch 3, Loss: 2.129784\n",
            "Epoch 4, Loss: 2.096400\n",
            "Epoch 5, Loss: 1.993230\n",
            "Epoch 6, Loss: 1.943786\n",
            "Epoch 7, Loss: 1.941895\n",
            "Epoch 8, Loss: 1.824529\n",
            "Epoch 9, Loss: 1.848726\n",
            "Epoch 10, Loss: 1.690929\n",
            "Shadow model 23 trained.\n",
            "Epoch 1, Loss: 2.317251\n",
            "Epoch 2, Loss: 2.187347\n",
            "Epoch 3, Loss: 2.112856\n",
            "Epoch 4, Loss: 2.055618\n",
            "Epoch 5, Loss: 1.978223\n",
            "Epoch 6, Loss: 1.965027\n",
            "Epoch 7, Loss: 1.911340\n",
            "Epoch 8, Loss: 1.755856\n",
            "Epoch 9, Loss: 1.693109\n",
            "Epoch 10, Loss: 1.602251\n",
            "Shadow model 24 trained.\n",
            "Epoch 1, Loss: 2.317806\n",
            "Epoch 2, Loss: 2.207694\n",
            "Epoch 3, Loss: 2.118267\n",
            "Epoch 4, Loss: 2.046603\n",
            "Epoch 5, Loss: 1.955269\n",
            "Epoch 6, Loss: 1.944818\n",
            "Epoch 7, Loss: 1.871058\n",
            "Epoch 8, Loss: 1.879711\n",
            "Epoch 9, Loss: 1.713292\n",
            "Epoch 10, Loss: 1.636917\n",
            "Shadow model 25 trained.\n",
            "Epoch 1, Loss: 2.344101\n",
            "Epoch 2, Loss: 2.248033\n",
            "Epoch 3, Loss: 2.191907\n",
            "Epoch 4, Loss: 2.073988\n",
            "Epoch 5, Loss: 1.986938\n",
            "Epoch 6, Loss: 1.891440\n",
            "Epoch 7, Loss: 1.778066\n",
            "Epoch 8, Loss: 1.681780\n",
            "Epoch 9, Loss: 1.586215\n",
            "Epoch 10, Loss: 1.544843\n",
            "Shadow model 26 trained.\n",
            "Epoch 1, Loss: 2.312765\n",
            "Epoch 2, Loss: 2.178761\n",
            "Epoch 3, Loss: 2.145787\n",
            "Epoch 4, Loss: 2.040561\n",
            "Epoch 5, Loss: 1.983574\n",
            "Epoch 6, Loss: 1.912702\n",
            "Epoch 7, Loss: 1.781195\n",
            "Epoch 8, Loss: 1.720407\n",
            "Epoch 9, Loss: 1.798762\n",
            "Epoch 10, Loss: 1.636925\n",
            "Shadow model 27 trained.\n",
            "Epoch 1, Loss: 2.276510\n",
            "Epoch 2, Loss: 2.160242\n",
            "Epoch 3, Loss: 2.021141\n",
            "Epoch 4, Loss: 1.939377\n",
            "Epoch 5, Loss: 1.863859\n",
            "Epoch 6, Loss: 1.686678\n",
            "Epoch 7, Loss: 1.594040\n",
            "Epoch 8, Loss: 1.628426\n",
            "Epoch 9, Loss: 1.499564\n",
            "Epoch 10, Loss: 1.492748\n",
            "Shadow model 28 trained.\n",
            "Epoch 1, Loss: 2.312442\n",
            "Epoch 2, Loss: 2.219143\n",
            "Epoch 3, Loss: 2.132660\n",
            "Epoch 4, Loss: 2.072080\n",
            "Epoch 5, Loss: 1.947367\n",
            "Epoch 6, Loss: 1.953382\n",
            "Epoch 7, Loss: 1.769888\n",
            "Epoch 8, Loss: 1.744089\n",
            "Epoch 9, Loss: 1.665538\n",
            "Epoch 10, Loss: 1.584676\n",
            "Shadow model 29 trained.\n",
            "Epoch 1, Loss: 2.317824\n",
            "Epoch 2, Loss: 2.220662\n",
            "Epoch 3, Loss: 2.124384\n",
            "Epoch 4, Loss: 2.068251\n",
            "Epoch 5, Loss: 1.955236\n",
            "Epoch 6, Loss: 1.957275\n",
            "Epoch 7, Loss: 1.766454\n",
            "Epoch 8, Loss: 1.614435\n",
            "Epoch 9, Loss: 1.572262\n",
            "Epoch 10, Loss: 1.449097\n",
            "Shadow model 30 trained.\n",
            "Epoch 1, Loss: 2.281836\n",
            "Epoch 2, Loss: 2.194709\n",
            "Epoch 3, Loss: 2.038326\n",
            "Epoch 4, Loss: 1.982424\n",
            "Epoch 5, Loss: 1.826248\n",
            "Epoch 6, Loss: 1.850755\n",
            "Epoch 7, Loss: 1.662474\n",
            "Epoch 8, Loss: 1.688488\n",
            "Epoch 9, Loss: 1.586601\n",
            "Epoch 10, Loss: 1.443839\n",
            "Shadow model 31 trained.\n",
            "Epoch 1, Loss: 2.306162\n",
            "Epoch 2, Loss: 2.119077\n",
            "Epoch 3, Loss: 2.046072\n",
            "Epoch 4, Loss: 1.944081\n",
            "Epoch 5, Loss: 1.879821\n",
            "Epoch 6, Loss: 1.783286\n",
            "Epoch 7, Loss: 1.756160\n",
            "Epoch 8, Loss: 1.690570\n",
            "Epoch 9, Loss: 1.530020\n",
            "Epoch 10, Loss: 1.484591\n",
            "Shadow model 32 trained.\n",
            "Epoch 1, Loss: 2.288443\n",
            "Epoch 2, Loss: 2.142724\n",
            "Epoch 3, Loss: 2.082074\n",
            "Epoch 4, Loss: 1.959678\n",
            "Epoch 5, Loss: 1.873774\n",
            "Epoch 6, Loss: 1.759314\n",
            "Epoch 7, Loss: 1.664563\n",
            "Epoch 8, Loss: 1.653234\n",
            "Epoch 9, Loss: 1.525213\n",
            "Epoch 10, Loss: 1.397231\n",
            "Shadow model 33 trained.\n",
            "Epoch 1, Loss: 2.306734\n",
            "Epoch 2, Loss: 2.203130\n",
            "Epoch 3, Loss: 2.106733\n",
            "Epoch 4, Loss: 1.997589\n",
            "Epoch 5, Loss: 1.941145\n",
            "Epoch 6, Loss: 1.794714\n",
            "Epoch 7, Loss: 1.741521\n",
            "Epoch 8, Loss: 1.743600\n",
            "Epoch 9, Loss: 1.618628\n",
            "Epoch 10, Loss: 1.529218\n",
            "Shadow model 34 trained.\n",
            "Epoch 1, Loss: 2.307303\n",
            "Epoch 2, Loss: 2.201743\n",
            "Epoch 3, Loss: 2.080131\n",
            "Epoch 4, Loss: 1.947338\n",
            "Epoch 5, Loss: 1.849872\n",
            "Epoch 6, Loss: 1.802396\n",
            "Epoch 7, Loss: 1.719256\n",
            "Epoch 8, Loss: 1.573354\n",
            "Epoch 9, Loss: 1.419074\n",
            "Epoch 10, Loss: 1.347983\n",
            "Shadow model 35 trained.\n",
            "Epoch 1, Loss: 2.301586\n",
            "Epoch 2, Loss: 2.224271\n",
            "Epoch 3, Loss: 2.169786\n",
            "Epoch 4, Loss: 2.068710\n",
            "Epoch 5, Loss: 1.971424\n",
            "Epoch 6, Loss: 1.960445\n",
            "Epoch 7, Loss: 1.979894\n",
            "Epoch 8, Loss: 1.837091\n",
            "Epoch 9, Loss: 1.764566\n",
            "Epoch 10, Loss: 1.703548\n",
            "Shadow model 36 trained.\n",
            "Epoch 1, Loss: 2.294419\n",
            "Epoch 2, Loss: 2.130849\n",
            "Epoch 3, Loss: 2.086392\n",
            "Epoch 4, Loss: 2.014735\n",
            "Epoch 5, Loss: 1.968381\n",
            "Epoch 6, Loss: 1.858627\n",
            "Epoch 7, Loss: 1.809212\n",
            "Epoch 8, Loss: 1.740713\n",
            "Epoch 9, Loss: 1.598148\n",
            "Epoch 10, Loss: 1.559914\n",
            "Shadow model 37 trained.\n",
            "Epoch 1, Loss: 2.312961\n",
            "Epoch 2, Loss: 2.190085\n",
            "Epoch 3, Loss: 2.063100\n",
            "Epoch 4, Loss: 1.983428\n",
            "Epoch 5, Loss: 1.823996\n",
            "Epoch 6, Loss: 1.841177\n",
            "Epoch 7, Loss: 1.679840\n",
            "Epoch 8, Loss: 1.564189\n",
            "Epoch 9, Loss: 1.488186\n",
            "Epoch 10, Loss: 1.466476\n",
            "Shadow model 38 trained.\n",
            "Epoch 1, Loss: 2.302771\n",
            "Epoch 2, Loss: 2.201606\n",
            "Epoch 3, Loss: 2.062376\n",
            "Epoch 4, Loss: 1.949616\n",
            "Epoch 5, Loss: 1.906144\n",
            "Epoch 6, Loss: 1.913372\n",
            "Epoch 7, Loss: 1.780319\n",
            "Epoch 8, Loss: 1.614970\n",
            "Epoch 9, Loss: 1.632217\n",
            "Epoch 10, Loss: 1.454456\n",
            "Shadow model 39 trained.\n",
            "Epoch 1, Loss: 2.299735\n",
            "Epoch 2, Loss: 2.176178\n",
            "Epoch 3, Loss: 2.144808\n",
            "Epoch 4, Loss: 2.039186\n",
            "Epoch 5, Loss: 1.958636\n",
            "Epoch 6, Loss: 1.863618\n",
            "Epoch 7, Loss: 1.734778\n",
            "Epoch 8, Loss: 1.664368\n",
            "Epoch 9, Loss: 1.535959\n",
            "Epoch 10, Loss: 1.389867\n",
            "Shadow model 40 trained.\n",
            "Epoch 1, Loss: 2.295239\n",
            "Epoch 2, Loss: 2.205423\n",
            "Epoch 3, Loss: 2.154122\n",
            "Epoch 4, Loss: 2.035679\n",
            "Epoch 5, Loss: 1.934834\n",
            "Epoch 6, Loss: 1.799104\n",
            "Epoch 7, Loss: 1.712436\n",
            "Epoch 8, Loss: 1.604943\n",
            "Epoch 9, Loss: 1.556187\n",
            "Epoch 10, Loss: 1.452862\n",
            "Shadow model 41 trained.\n",
            "Epoch 1, Loss: 2.316891\n",
            "Epoch 2, Loss: 2.173079\n",
            "Epoch 3, Loss: 2.061544\n",
            "Epoch 4, Loss: 1.994954\n",
            "Epoch 5, Loss: 1.950378\n",
            "Epoch 6, Loss: 1.867866\n",
            "Epoch 7, Loss: 1.758562\n",
            "Epoch 8, Loss: 1.654613\n",
            "Epoch 9, Loss: 1.522486\n",
            "Epoch 10, Loss: 1.523042\n",
            "Shadow model 42 trained.\n",
            "Epoch 1, Loss: 2.343469\n",
            "Epoch 2, Loss: 2.198642\n",
            "Epoch 3, Loss: 2.119264\n",
            "Epoch 4, Loss: 1.991724\n",
            "Epoch 5, Loss: 1.934438\n",
            "Epoch 6, Loss: 1.803520\n",
            "Epoch 7, Loss: 1.818159\n",
            "Epoch 8, Loss: 1.729282\n",
            "Epoch 9, Loss: 1.737742\n",
            "Epoch 10, Loss: 1.568460\n",
            "Shadow model 43 trained.\n",
            "Epoch 1, Loss: 2.335620\n",
            "Epoch 2, Loss: 2.261826\n",
            "Epoch 3, Loss: 2.201661\n",
            "Epoch 4, Loss: 2.117260\n",
            "Epoch 5, Loss: 2.010600\n",
            "Epoch 6, Loss: 1.956605\n",
            "Epoch 7, Loss: 1.842667\n",
            "Epoch 8, Loss: 1.823999\n",
            "Epoch 9, Loss: 1.729896\n",
            "Epoch 10, Loss: 1.610660\n",
            "Shadow model 44 trained.\n",
            "Epoch 1, Loss: 2.311028\n",
            "Epoch 2, Loss: 2.189443\n",
            "Epoch 3, Loss: 2.098871\n",
            "Epoch 4, Loss: 1.963388\n",
            "Epoch 5, Loss: 1.866555\n",
            "Epoch 6, Loss: 1.894651\n",
            "Epoch 7, Loss: 1.811925\n",
            "Epoch 8, Loss: 1.622101\n",
            "Epoch 9, Loss: 1.583144\n",
            "Epoch 10, Loss: 1.516410\n",
            "Shadow model 45 trained.\n",
            "Epoch 1, Loss: 2.311731\n",
            "Epoch 2, Loss: 2.204377\n",
            "Epoch 3, Loss: 2.113881\n",
            "Epoch 4, Loss: 2.039654\n",
            "Epoch 5, Loss: 1.983656\n",
            "Epoch 6, Loss: 1.823234\n",
            "Epoch 7, Loss: 1.816661\n",
            "Epoch 8, Loss: 1.711180\n",
            "Epoch 9, Loss: 1.680993\n",
            "Epoch 10, Loss: 1.540111\n",
            "Shadow model 46 trained.\n",
            "Epoch 1, Loss: 2.323179\n",
            "Epoch 2, Loss: 2.194972\n",
            "Epoch 3, Loss: 2.100081\n",
            "Epoch 4, Loss: 2.057938\n",
            "Epoch 5, Loss: 1.998421\n",
            "Epoch 6, Loss: 1.902290\n",
            "Epoch 7, Loss: 1.806386\n",
            "Epoch 8, Loss: 1.799521\n",
            "Epoch 9, Loss: 1.632356\n",
            "Epoch 10, Loss: 1.662965\n",
            "Shadow model 47 trained.\n",
            "Epoch 1, Loss: 2.296744\n",
            "Epoch 2, Loss: 2.190121\n",
            "Epoch 3, Loss: 2.091876\n",
            "Epoch 4, Loss: 1.955456\n",
            "Epoch 5, Loss: 1.912481\n",
            "Epoch 6, Loss: 1.777069\n",
            "Epoch 7, Loss: 1.679700\n",
            "Epoch 8, Loss: 1.546602\n",
            "Epoch 9, Loss: 1.496590\n",
            "Epoch 10, Loss: 1.405090\n",
            "Shadow model 48 trained.\n",
            "Epoch 1, Loss: 2.277090\n",
            "Epoch 2, Loss: 2.190378\n",
            "Epoch 3, Loss: 2.023587\n",
            "Epoch 4, Loss: 1.963056\n",
            "Epoch 5, Loss: 1.901327\n",
            "Epoch 6, Loss: 1.835651\n",
            "Epoch 7, Loss: 1.658910\n",
            "Epoch 8, Loss: 1.587896\n",
            "Epoch 9, Loss: 1.485067\n",
            "Epoch 10, Loss: 1.356551\n",
            "Shadow model 49 trained.\n",
            "Epoch 1, Loss: 2.295761\n",
            "Epoch 2, Loss: 2.169942\n",
            "Epoch 3, Loss: 2.131096\n",
            "Epoch 4, Loss: 2.038995\n",
            "Epoch 5, Loss: 1.971150\n",
            "Epoch 6, Loss: 1.831169\n",
            "Epoch 7, Loss: 1.749474\n",
            "Epoch 8, Loss: 1.702156\n",
            "Epoch 9, Loss: 1.670746\n",
            "Epoch 10, Loss: 1.613565\n",
            "Shadow model 50 trained.\n",
            "Epoch 1, Loss: 2.323762\n",
            "Epoch 2, Loss: 2.213360\n",
            "Epoch 3, Loss: 2.093423\n",
            "Epoch 4, Loss: 2.008661\n",
            "Epoch 5, Loss: 1.905977\n",
            "Epoch 6, Loss: 1.828668\n",
            "Epoch 7, Loss: 1.723224\n",
            "Epoch 8, Loss: 1.668883\n",
            "Epoch 9, Loss: 1.516362\n",
            "Epoch 10, Loss: 1.515084\n",
            "Shadow model 51 trained.\n",
            "Epoch 1, Loss: 2.279201\n",
            "Epoch 2, Loss: 2.141629\n",
            "Epoch 3, Loss: 2.060505\n",
            "Epoch 4, Loss: 2.048739\n",
            "Epoch 5, Loss: 1.902553\n",
            "Epoch 6, Loss: 1.796628\n",
            "Epoch 7, Loss: 1.783798\n",
            "Epoch 8, Loss: 1.683931\n",
            "Epoch 9, Loss: 1.732223\n",
            "Epoch 10, Loss: 1.595789\n",
            "Shadow model 52 trained.\n",
            "Epoch 1, Loss: 2.298667\n",
            "Epoch 2, Loss: 2.129658\n",
            "Epoch 3, Loss: 2.069623\n",
            "Epoch 4, Loss: 2.009168\n",
            "Epoch 5, Loss: 1.934026\n",
            "Epoch 6, Loss: 1.900999\n",
            "Epoch 7, Loss: 1.835634\n",
            "Epoch 8, Loss: 1.760954\n",
            "Epoch 9, Loss: 1.668366\n",
            "Epoch 10, Loss: 1.527678\n",
            "Shadow model 53 trained.\n",
            "Epoch 1, Loss: 2.297361\n",
            "Epoch 2, Loss: 2.202104\n",
            "Epoch 3, Loss: 2.132591\n",
            "Epoch 4, Loss: 2.050378\n",
            "Epoch 5, Loss: 1.988903\n",
            "Epoch 6, Loss: 1.928792\n",
            "Epoch 7, Loss: 1.878361\n",
            "Epoch 8, Loss: 1.743831\n",
            "Epoch 9, Loss: 1.656805\n",
            "Epoch 10, Loss: 1.542152\n",
            "Shadow model 54 trained.\n",
            "Epoch 1, Loss: 2.262945\n",
            "Epoch 2, Loss: 2.115145\n",
            "Epoch 3, Loss: 2.131982\n",
            "Epoch 4, Loss: 1.991632\n",
            "Epoch 5, Loss: 1.904377\n",
            "Epoch 6, Loss: 1.916986\n",
            "Epoch 7, Loss: 1.768162\n",
            "Epoch 8, Loss: 1.687938\n",
            "Epoch 9, Loss: 1.600555\n",
            "Epoch 10, Loss: 1.541623\n",
            "Shadow model 55 trained.\n",
            "Epoch 1, Loss: 2.341698\n",
            "Epoch 2, Loss: 2.230616\n",
            "Epoch 3, Loss: 2.160890\n",
            "Epoch 4, Loss: 2.029135\n",
            "Epoch 5, Loss: 1.908340\n",
            "Epoch 6, Loss: 1.895937\n",
            "Epoch 7, Loss: 1.806051\n",
            "Epoch 8, Loss: 1.705039\n",
            "Epoch 9, Loss: 1.568952\n",
            "Epoch 10, Loss: 1.440604\n",
            "Shadow model 56 trained.\n",
            "Epoch 1, Loss: 2.314856\n",
            "Epoch 2, Loss: 2.206630\n",
            "Epoch 3, Loss: 2.030243\n",
            "Epoch 4, Loss: 1.976226\n",
            "Epoch 5, Loss: 1.879684\n",
            "Epoch 6, Loss: 1.813248\n",
            "Epoch 7, Loss: 1.652385\n",
            "Epoch 8, Loss: 1.624634\n",
            "Epoch 9, Loss: 1.573611\n",
            "Epoch 10, Loss: 1.382407\n",
            "Shadow model 57 trained.\n",
            "Epoch 1, Loss: 2.314179\n",
            "Epoch 2, Loss: 2.161607\n",
            "Epoch 3, Loss: 2.061797\n",
            "Epoch 4, Loss: 1.980861\n",
            "Epoch 5, Loss: 1.903772\n",
            "Epoch 6, Loss: 1.784623\n",
            "Epoch 7, Loss: 1.748479\n",
            "Epoch 8, Loss: 1.588443\n",
            "Epoch 9, Loss: 1.603525\n",
            "Epoch 10, Loss: 1.529356\n",
            "Shadow model 58 trained.\n",
            "Epoch 1, Loss: 2.312821\n",
            "Epoch 2, Loss: 2.261776\n",
            "Epoch 3, Loss: 2.150101\n",
            "Epoch 4, Loss: 2.050967\n",
            "Epoch 5, Loss: 2.041754\n",
            "Epoch 6, Loss: 1.921048\n",
            "Epoch 7, Loss: 1.792184\n",
            "Epoch 8, Loss: 1.727834\n",
            "Epoch 9, Loss: 1.620489\n",
            "Epoch 10, Loss: 1.500362\n",
            "Shadow model 59 trained.\n",
            "Epoch 1, Loss: 2.320785\n",
            "Epoch 2, Loss: 2.219558\n",
            "Epoch 3, Loss: 2.174356\n",
            "Epoch 4, Loss: 1.993518\n",
            "Epoch 5, Loss: 1.914537\n",
            "Epoch 6, Loss: 1.892520\n",
            "Epoch 7, Loss: 1.784484\n",
            "Epoch 8, Loss: 1.725802\n",
            "Epoch 9, Loss: 1.621284\n",
            "Epoch 10, Loss: 1.563090\n",
            "Shadow model 60 trained.\n",
            "Epoch 1, Loss: 2.316439\n",
            "Epoch 2, Loss: 2.211833\n",
            "Epoch 3, Loss: 2.122872\n",
            "Epoch 4, Loss: 2.056978\n",
            "Epoch 5, Loss: 1.943786\n",
            "Epoch 6, Loss: 1.876875\n",
            "Epoch 7, Loss: 1.749941\n",
            "Epoch 8, Loss: 1.793134\n",
            "Epoch 9, Loss: 1.622903\n",
            "Epoch 10, Loss: 1.523472\n",
            "Shadow model 61 trained.\n",
            "Epoch 1, Loss: 2.298009\n",
            "Epoch 2, Loss: 2.177344\n",
            "Epoch 3, Loss: 2.076774\n",
            "Epoch 4, Loss: 1.967974\n",
            "Epoch 5, Loss: 1.893452\n",
            "Epoch 6, Loss: 1.851178\n",
            "Epoch 7, Loss: 1.746038\n",
            "Epoch 8, Loss: 1.606604\n",
            "Epoch 9, Loss: 1.492165\n",
            "Epoch 10, Loss: 1.431116\n",
            "Shadow model 62 trained.\n",
            "Epoch 1, Loss: 2.311799\n",
            "Epoch 2, Loss: 2.190055\n",
            "Epoch 3, Loss: 2.068784\n",
            "Epoch 4, Loss: 2.034819\n",
            "Epoch 5, Loss: 1.970209\n",
            "Epoch 6, Loss: 1.900456\n",
            "Epoch 7, Loss: 1.790276\n",
            "Epoch 8, Loss: 1.655689\n",
            "Epoch 9, Loss: 1.660865\n",
            "Epoch 10, Loss: 1.605532\n",
            "Shadow model 63 trained.\n",
            "Epoch 1, Loss: 2.311733\n",
            "Epoch 2, Loss: 2.150367\n",
            "Epoch 3, Loss: 2.120852\n",
            "Epoch 4, Loss: 1.991943\n",
            "Epoch 5, Loss: 1.918013\n",
            "Epoch 6, Loss: 1.785879\n",
            "Epoch 7, Loss: 1.839202\n",
            "Epoch 8, Loss: 1.719800\n",
            "Epoch 9, Loss: 1.690181\n",
            "Epoch 10, Loss: 1.613198\n",
            "Shadow model 64 trained.\n",
            "Epoch 1, Loss: 2.344964\n",
            "Epoch 2, Loss: 2.214745\n",
            "Epoch 3, Loss: 2.120686\n",
            "Epoch 4, Loss: 2.055895\n",
            "Epoch 5, Loss: 1.956583\n",
            "Epoch 6, Loss: 1.839477\n",
            "Epoch 7, Loss: 1.746683\n",
            "Epoch 8, Loss: 1.728056\n",
            "Epoch 9, Loss: 1.631422\n",
            "Epoch 10, Loss: 1.611642\n",
            "Shadow model 65 trained.\n",
            "Epoch 1, Loss: 2.322822\n",
            "Epoch 2, Loss: 2.214849\n",
            "Epoch 3, Loss: 2.145724\n",
            "Epoch 4, Loss: 2.085479\n",
            "Epoch 5, Loss: 2.020141\n",
            "Epoch 6, Loss: 1.954919\n",
            "Epoch 7, Loss: 1.852740\n",
            "Epoch 8, Loss: 1.722895\n",
            "Epoch 9, Loss: 1.686133\n",
            "Epoch 10, Loss: 1.547894\n",
            "Shadow model 66 trained.\n",
            "Epoch 1, Loss: 2.316168\n",
            "Epoch 2, Loss: 2.194797\n",
            "Epoch 3, Loss: 2.152904\n",
            "Epoch 4, Loss: 2.062270\n",
            "Epoch 5, Loss: 1.941116\n",
            "Epoch 6, Loss: 1.813241\n",
            "Epoch 7, Loss: 1.815389\n",
            "Epoch 8, Loss: 1.720145\n",
            "Epoch 9, Loss: 1.586101\n",
            "Epoch 10, Loss: 1.537633\n",
            "Shadow model 67 trained.\n",
            "Epoch 1, Loss: 2.328967\n",
            "Epoch 2, Loss: 2.261700\n",
            "Epoch 3, Loss: 2.162489\n",
            "Epoch 4, Loss: 2.085982\n",
            "Epoch 5, Loss: 1.992882\n",
            "Epoch 6, Loss: 1.921920\n",
            "Epoch 7, Loss: 1.927733\n",
            "Epoch 8, Loss: 1.777061\n",
            "Epoch 9, Loss: 1.813417\n",
            "Epoch 10, Loss: 1.665386\n",
            "Shadow model 68 trained.\n",
            "Epoch 1, Loss: 2.312836\n",
            "Epoch 2, Loss: 2.246625\n",
            "Epoch 3, Loss: 2.128166\n",
            "Epoch 4, Loss: 2.137100\n",
            "Epoch 5, Loss: 2.022785\n",
            "Epoch 6, Loss: 1.942182\n",
            "Epoch 7, Loss: 1.862865\n",
            "Epoch 8, Loss: 1.799674\n",
            "Epoch 9, Loss: 1.650680\n",
            "Epoch 10, Loss: 1.523177\n",
            "Shadow model 69 trained.\n",
            "Epoch 1, Loss: 2.325236\n",
            "Epoch 2, Loss: 2.246139\n",
            "Epoch 3, Loss: 2.153715\n",
            "Epoch 4, Loss: 2.068473\n",
            "Epoch 5, Loss: 2.004846\n",
            "Epoch 6, Loss: 1.956599\n",
            "Epoch 7, Loss: 1.836097\n",
            "Epoch 8, Loss: 1.772021\n",
            "Epoch 9, Loss: 1.684139\n",
            "Epoch 10, Loss: 1.644469\n",
            "Shadow model 70 trained.\n",
            "Epoch 1, Loss: 2.309321\n",
            "Epoch 2, Loss: 2.138979\n",
            "Epoch 3, Loss: 2.125168\n",
            "Epoch 4, Loss: 2.028688\n",
            "Epoch 5, Loss: 1.993291\n",
            "Epoch 6, Loss: 1.852004\n",
            "Epoch 7, Loss: 1.836509\n",
            "Epoch 8, Loss: 1.685365\n",
            "Epoch 9, Loss: 1.667546\n",
            "Epoch 10, Loss: 1.573732\n",
            "Shadow model 71 trained.\n",
            "Epoch 1, Loss: 2.347560\n",
            "Epoch 2, Loss: 2.229004\n",
            "Epoch 3, Loss: 2.120489\n",
            "Epoch 4, Loss: 2.109230\n",
            "Epoch 5, Loss: 2.021506\n",
            "Epoch 6, Loss: 1.967413\n",
            "Epoch 7, Loss: 1.883493\n",
            "Epoch 8, Loss: 1.790397\n",
            "Epoch 9, Loss: 1.679580\n",
            "Epoch 10, Loss: 1.594605\n",
            "Shadow model 72 trained.\n",
            "Epoch 1, Loss: 2.277604\n",
            "Epoch 2, Loss: 2.150975\n",
            "Epoch 3, Loss: 2.053333\n",
            "Epoch 4, Loss: 1.908004\n",
            "Epoch 5, Loss: 1.913025\n",
            "Epoch 6, Loss: 1.703337\n",
            "Epoch 7, Loss: 1.582074\n",
            "Epoch 8, Loss: 1.520487\n",
            "Epoch 9, Loss: 1.441403\n",
            "Epoch 10, Loss: 1.417343\n",
            "Shadow model 73 trained.\n",
            "Epoch 1, Loss: 2.290296\n",
            "Epoch 2, Loss: 2.133225\n",
            "Epoch 3, Loss: 2.068839\n",
            "Epoch 4, Loss: 1.950229\n",
            "Epoch 5, Loss: 1.808085\n",
            "Epoch 6, Loss: 1.844542\n",
            "Epoch 7, Loss: 1.755531\n",
            "Epoch 8, Loss: 1.686775\n",
            "Epoch 9, Loss: 1.589916\n",
            "Epoch 10, Loss: 1.485624\n",
            "Shadow model 74 trained.\n",
            "Epoch 1, Loss: 2.340229\n",
            "Epoch 2, Loss: 2.224214\n",
            "Epoch 3, Loss: 2.209641\n",
            "Epoch 4, Loss: 2.106167\n",
            "Epoch 5, Loss: 2.045905\n",
            "Epoch 6, Loss: 1.995140\n",
            "Epoch 7, Loss: 1.876637\n",
            "Epoch 8, Loss: 1.857143\n",
            "Epoch 9, Loss: 1.796065\n",
            "Epoch 10, Loss: 1.717176\n",
            "Shadow model 75 trained.\n",
            "Epoch 1, Loss: 2.304665\n",
            "Epoch 2, Loss: 2.172657\n",
            "Epoch 3, Loss: 2.072604\n",
            "Epoch 4, Loss: 2.061233\n",
            "Epoch 5, Loss: 1.996402\n",
            "Epoch 6, Loss: 1.919481\n",
            "Epoch 7, Loss: 1.832039\n",
            "Epoch 8, Loss: 1.818989\n",
            "Epoch 9, Loss: 1.681365\n",
            "Epoch 10, Loss: 1.638680\n",
            "Shadow model 76 trained.\n",
            "Epoch 1, Loss: 2.271342\n",
            "Epoch 2, Loss: 2.187818\n",
            "Epoch 3, Loss: 2.080132\n",
            "Epoch 4, Loss: 1.990899\n",
            "Epoch 5, Loss: 1.923845\n",
            "Epoch 6, Loss: 1.796480\n",
            "Epoch 7, Loss: 1.666795\n",
            "Epoch 8, Loss: 1.578034\n",
            "Epoch 9, Loss: 1.542548\n",
            "Epoch 10, Loss: 1.454300\n",
            "Shadow model 77 trained.\n",
            "Epoch 1, Loss: 2.315245\n",
            "Epoch 2, Loss: 2.191396\n",
            "Epoch 3, Loss: 2.132736\n",
            "Epoch 4, Loss: 2.028522\n",
            "Epoch 5, Loss: 1.933975\n",
            "Epoch 6, Loss: 1.867950\n",
            "Epoch 7, Loss: 1.812230\n",
            "Epoch 8, Loss: 1.719640\n",
            "Epoch 9, Loss: 1.598143\n",
            "Epoch 10, Loss: 1.588576\n",
            "Shadow model 78 trained.\n",
            "Epoch 1, Loss: 2.258986\n",
            "Epoch 2, Loss: 2.166540\n",
            "Epoch 3, Loss: 2.054499\n",
            "Epoch 4, Loss: 1.993327\n",
            "Epoch 5, Loss: 1.992910\n",
            "Epoch 6, Loss: 1.875445\n",
            "Epoch 7, Loss: 1.758482\n",
            "Epoch 8, Loss: 1.711932\n",
            "Epoch 9, Loss: 1.655263\n",
            "Epoch 10, Loss: 1.476319\n",
            "Shadow model 79 trained.\n",
            "Epoch 1, Loss: 2.300293\n",
            "Epoch 2, Loss: 2.137392\n",
            "Epoch 3, Loss: 2.063649\n",
            "Epoch 4, Loss: 1.977483\n",
            "Epoch 5, Loss: 1.867559\n",
            "Epoch 6, Loss: 1.885278\n",
            "Epoch 7, Loss: 1.791386\n",
            "Epoch 8, Loss: 1.656453\n",
            "Epoch 9, Loss: 1.586312\n",
            "Epoch 10, Loss: 1.460074\n",
            "Shadow model 80 trained.\n",
            "Epoch 1, Loss: 2.266946\n",
            "Epoch 2, Loss: 2.169611\n",
            "Epoch 3, Loss: 2.079599\n",
            "Epoch 4, Loss: 1.986657\n",
            "Epoch 5, Loss: 1.906153\n",
            "Epoch 6, Loss: 1.730037\n",
            "Epoch 7, Loss: 1.631454\n",
            "Epoch 8, Loss: 1.542801\n",
            "Epoch 9, Loss: 1.445200\n",
            "Epoch 10, Loss: 1.411711\n",
            "Shadow model 81 trained.\n",
            "Epoch 1, Loss: 2.305925\n",
            "Epoch 2, Loss: 2.190445\n",
            "Epoch 3, Loss: 2.167106\n",
            "Epoch 4, Loss: 2.060151\n",
            "Epoch 5, Loss: 2.010572\n",
            "Epoch 6, Loss: 1.993099\n",
            "Epoch 7, Loss: 1.942657\n",
            "Epoch 8, Loss: 1.874107\n",
            "Epoch 9, Loss: 1.800556\n",
            "Epoch 10, Loss: 1.748218\n",
            "Shadow model 82 trained.\n",
            "Epoch 1, Loss: 2.318469\n",
            "Epoch 2, Loss: 2.235576\n",
            "Epoch 3, Loss: 2.115269\n",
            "Epoch 4, Loss: 2.073510\n",
            "Epoch 5, Loss: 2.013202\n",
            "Epoch 6, Loss: 1.841843\n",
            "Epoch 7, Loss: 1.832589\n",
            "Epoch 8, Loss: 1.625102\n",
            "Epoch 9, Loss: 1.534544\n",
            "Epoch 10, Loss: 1.618172\n",
            "Shadow model 83 trained.\n",
            "Epoch 1, Loss: 2.310804\n",
            "Epoch 2, Loss: 2.189347\n",
            "Epoch 3, Loss: 2.128613\n",
            "Epoch 4, Loss: 2.019864\n",
            "Epoch 5, Loss: 1.944328\n",
            "Epoch 6, Loss: 1.905896\n",
            "Epoch 7, Loss: 1.745641\n",
            "Epoch 8, Loss: 1.688361\n",
            "Epoch 9, Loss: 1.602525\n",
            "Epoch 10, Loss: 1.562721\n",
            "Shadow model 84 trained.\n",
            "Epoch 1, Loss: 2.317669\n",
            "Epoch 2, Loss: 2.231969\n",
            "Epoch 3, Loss: 2.159264\n",
            "Epoch 4, Loss: 2.065039\n",
            "Epoch 5, Loss: 2.032503\n",
            "Epoch 6, Loss: 1.930797\n",
            "Epoch 7, Loss: 1.868893\n",
            "Epoch 8, Loss: 1.756347\n",
            "Epoch 9, Loss: 1.691174\n",
            "Epoch 10, Loss: 1.563856\n",
            "Shadow model 85 trained.\n",
            "Epoch 1, Loss: 2.302275\n",
            "Epoch 2, Loss: 2.185261\n",
            "Epoch 3, Loss: 2.070216\n",
            "Epoch 4, Loss: 1.963518\n",
            "Epoch 5, Loss: 1.899197\n",
            "Epoch 6, Loss: 1.751790\n",
            "Epoch 7, Loss: 1.648400\n",
            "Epoch 8, Loss: 1.551409\n",
            "Epoch 9, Loss: 1.411937\n",
            "Epoch 10, Loss: 1.330821\n",
            "Shadow model 86 trained.\n",
            "Epoch 1, Loss: 2.306047\n",
            "Epoch 2, Loss: 2.164256\n",
            "Epoch 3, Loss: 2.111420\n",
            "Epoch 4, Loss: 2.009552\n",
            "Epoch 5, Loss: 1.856511\n",
            "Epoch 6, Loss: 1.801903\n",
            "Epoch 7, Loss: 1.721650\n",
            "Epoch 8, Loss: 1.568791\n",
            "Epoch 9, Loss: 1.525632\n",
            "Epoch 10, Loss: 1.406161\n",
            "Shadow model 87 trained.\n",
            "Epoch 1, Loss: 2.282417\n",
            "Epoch 2, Loss: 2.158380\n",
            "Epoch 3, Loss: 2.027434\n",
            "Epoch 4, Loss: 1.912256\n",
            "Epoch 5, Loss: 1.871239\n",
            "Epoch 6, Loss: 1.745351\n",
            "Epoch 7, Loss: 1.638758\n",
            "Epoch 8, Loss: 1.545390\n",
            "Epoch 9, Loss: 1.450747\n",
            "Epoch 10, Loss: 1.332703\n",
            "Shadow model 88 trained.\n",
            "Epoch 1, Loss: 2.305646\n",
            "Epoch 2, Loss: 2.196132\n",
            "Epoch 3, Loss: 2.092893\n",
            "Epoch 4, Loss: 2.018864\n",
            "Epoch 5, Loss: 1.929826\n",
            "Epoch 6, Loss: 1.848309\n",
            "Epoch 7, Loss: 1.801094\n",
            "Epoch 8, Loss: 1.663709\n",
            "Epoch 9, Loss: 1.601636\n",
            "Epoch 10, Loss: 1.472187\n",
            "Shadow model 89 trained.\n",
            "Epoch 1, Loss: 2.290069\n",
            "Epoch 2, Loss: 2.120493\n",
            "Epoch 3, Loss: 2.021421\n",
            "Epoch 4, Loss: 1.955544\n",
            "Epoch 5, Loss: 1.890069\n",
            "Epoch 6, Loss: 1.832473\n",
            "Epoch 7, Loss: 1.801924\n",
            "Epoch 8, Loss: 1.699119\n",
            "Epoch 9, Loss: 1.657644\n",
            "Epoch 10, Loss: 1.519834\n",
            "Shadow model 90 trained.\n",
            "Epoch 1, Loss: 2.325360\n",
            "Epoch 2, Loss: 2.197559\n",
            "Epoch 3, Loss: 2.061095\n",
            "Epoch 4, Loss: 2.004148\n",
            "Epoch 5, Loss: 2.013458\n",
            "Epoch 6, Loss: 1.925345\n",
            "Epoch 7, Loss: 1.891256\n",
            "Epoch 8, Loss: 1.860186\n",
            "Epoch 9, Loss: 1.754596\n",
            "Epoch 10, Loss: 1.706415\n",
            "Shadow model 91 trained.\n",
            "Epoch 1, Loss: 2.307413\n",
            "Epoch 2, Loss: 2.192565\n",
            "Epoch 3, Loss: 2.080936\n",
            "Epoch 4, Loss: 2.029637\n",
            "Epoch 5, Loss: 1.952768\n",
            "Epoch 6, Loss: 1.855867\n",
            "Epoch 7, Loss: 1.753086\n",
            "Epoch 8, Loss: 1.608159\n",
            "Epoch 9, Loss: 1.655267\n",
            "Epoch 10, Loss: 1.609379\n",
            "Shadow model 92 trained.\n",
            "Epoch 1, Loss: 2.302558\n",
            "Epoch 2, Loss: 2.179914\n",
            "Epoch 3, Loss: 2.077503\n",
            "Epoch 4, Loss: 2.059477\n",
            "Epoch 5, Loss: 1.959606\n",
            "Epoch 6, Loss: 1.809998\n",
            "Epoch 7, Loss: 1.856970\n",
            "Epoch 8, Loss: 1.754248\n",
            "Epoch 9, Loss: 1.650203\n",
            "Epoch 10, Loss: 1.620677\n",
            "Shadow model 93 trained.\n",
            "Epoch 1, Loss: 2.271678\n",
            "Epoch 2, Loss: 2.181501\n",
            "Epoch 3, Loss: 2.074797\n",
            "Epoch 4, Loss: 1.958237\n",
            "Epoch 5, Loss: 1.878037\n",
            "Epoch 6, Loss: 1.717717\n",
            "Epoch 7, Loss: 1.631725\n",
            "Epoch 8, Loss: 1.579033\n",
            "Epoch 9, Loss: 1.428806\n",
            "Epoch 10, Loss: 1.317018\n",
            "Shadow model 94 trained.\n",
            "Epoch 1, Loss: 2.270078\n",
            "Epoch 2, Loss: 2.182200\n",
            "Epoch 3, Loss: 2.067606\n",
            "Epoch 4, Loss: 2.039214\n",
            "Epoch 5, Loss: 1.947385\n",
            "Epoch 6, Loss: 1.784614\n",
            "Epoch 7, Loss: 1.762105\n",
            "Epoch 8, Loss: 1.699299\n",
            "Epoch 9, Loss: 1.623267\n",
            "Epoch 10, Loss: 1.592974\n",
            "Shadow model 95 trained.\n",
            "Epoch 1, Loss: 2.304659\n",
            "Epoch 2, Loss: 2.198237\n",
            "Epoch 3, Loss: 2.130474\n",
            "Epoch 4, Loss: 2.056446\n",
            "Epoch 5, Loss: 2.019592\n",
            "Epoch 6, Loss: 1.956747\n",
            "Epoch 7, Loss: 1.877124\n",
            "Epoch 8, Loss: 1.749625\n",
            "Epoch 9, Loss: 1.750973\n",
            "Epoch 10, Loss: 1.585300\n",
            "Shadow model 96 trained.\n",
            "Epoch 1, Loss: 2.298694\n",
            "Epoch 2, Loss: 2.220472\n",
            "Epoch 3, Loss: 2.152344\n",
            "Epoch 4, Loss: 2.045537\n",
            "Epoch 5, Loss: 1.917160\n",
            "Epoch 6, Loss: 1.923855\n",
            "Epoch 7, Loss: 1.759849\n",
            "Epoch 8, Loss: 1.713102\n",
            "Epoch 9, Loss: 1.616993\n",
            "Epoch 10, Loss: 1.515607\n",
            "Shadow model 97 trained.\n",
            "Epoch 1, Loss: 2.280432\n",
            "Epoch 2, Loss: 2.145094\n",
            "Epoch 3, Loss: 2.023893\n",
            "Epoch 4, Loss: 1.863071\n",
            "Epoch 5, Loss: 1.806057\n",
            "Epoch 6, Loss: 1.706431\n",
            "Epoch 7, Loss: 1.554126\n",
            "Epoch 8, Loss: 1.568131\n",
            "Epoch 9, Loss: 1.428152\n",
            "Epoch 10, Loss: 1.308086\n",
            "Shadow model 98 trained.\n",
            "Epoch 1, Loss: 2.307113\n",
            "Epoch 2, Loss: 2.162530\n",
            "Epoch 3, Loss: 2.095524\n",
            "Epoch 4, Loss: 2.008433\n",
            "Epoch 5, Loss: 1.933269\n",
            "Epoch 6, Loss: 1.861610\n",
            "Epoch 7, Loss: 1.814847\n",
            "Epoch 8, Loss: 1.800672\n",
            "Epoch 9, Loss: 1.618683\n",
            "Epoch 10, Loss: 1.474045\n",
            "Shadow model 99 trained.\n",
            "Epoch 1, Loss: 2.357566\n",
            "Epoch 2, Loss: 2.220339\n",
            "Epoch 3, Loss: 2.153274\n",
            "Epoch 4, Loss: 2.086251\n",
            "Epoch 5, Loss: 1.972332\n",
            "Epoch 6, Loss: 1.948999\n",
            "Epoch 7, Loss: 1.867033\n",
            "Epoch 8, Loss: 1.803429\n",
            "Epoch 9, Loss: 1.718113\n",
            "Epoch 10, Loss: 1.657023\n",
            "Shadow model 100 trained.\n",
            "Class 0, Training Accuracy: 1.0000\n",
            "Class 0, Training Precision: 1.0000\n",
            "Class 0, Training Recall: 1.0000\n",
            "Class 0, Training F1 Score: 1.0000\n",
            "Class 0, Training Confusion Matrix:\n",
            " [[1946    0]\n",
            " [   0 4054]]\n",
            "Attack model for class 0 trained.\n",
            "Class 1, Training Accuracy: 1.0000\n",
            "Class 1, Training Precision: 1.0000\n",
            "Class 1, Training Recall: 1.0000\n",
            "Class 1, Training F1 Score: 1.0000\n",
            "Class 1, Training Confusion Matrix:\n",
            " [[2007    0]\n",
            " [   0 3993]]\n",
            "Attack model for class 1 trained.\n",
            "Class 2, Training Accuracy: 0.9995\n",
            "Class 2, Training Precision: 0.9993\n",
            "Class 2, Training Recall: 1.0000\n",
            "Class 2, Training F1 Score: 0.9996\n",
            "Class 2, Training Confusion Matrix:\n",
            " [[1987    3]\n",
            " [   0 4010]]\n",
            "Attack model for class 2 trained.\n",
            "Class 3, Training Accuracy: 1.0000\n",
            "Class 3, Training Precision: 1.0000\n",
            "Class 3, Training Recall: 1.0000\n",
            "Class 3, Training F1 Score: 1.0000\n",
            "Class 3, Training Confusion Matrix:\n",
            " [[2015    0]\n",
            " [   0 3985]]\n",
            "Attack model for class 3 trained.\n",
            "Class 4, Training Accuracy: 0.9998\n",
            "Class 4, Training Precision: 0.9998\n",
            "Class 4, Training Recall: 1.0000\n",
            "Class 4, Training F1 Score: 0.9999\n",
            "Class 4, Training Confusion Matrix:\n",
            " [[1984    1]\n",
            " [   0 4015]]\n",
            "Attack model for class 4 trained.\n",
            "Class 5, Training Accuracy: 1.0000\n",
            "Class 5, Training Precision: 1.0000\n",
            "Class 5, Training Recall: 1.0000\n",
            "Class 5, Training F1 Score: 1.0000\n",
            "Class 5, Training Confusion Matrix:\n",
            " [[2010    0]\n",
            " [   0 3990]]\n",
            "Attack model for class 5 trained.\n",
            "Class 6, Training Accuracy: 0.9997\n",
            "Class 6, Training Precision: 0.9995\n",
            "Class 6, Training Recall: 1.0000\n",
            "Class 6, Training F1 Score: 0.9997\n",
            "Class 6, Training Confusion Matrix:\n",
            " [[2033    2]\n",
            " [   0 3965]]\n",
            "Attack model for class 6 trained.\n",
            "Class 7, Training Accuracy: 0.9998\n",
            "Class 7, Training Precision: 0.9997\n",
            "Class 7, Training Recall: 1.0000\n",
            "Class 7, Training F1 Score: 0.9999\n",
            "Class 7, Training Confusion Matrix:\n",
            " [[2002    1]\n",
            " [   0 3997]]\n",
            "Attack model for class 7 trained.\n",
            "Class 8, Training Accuracy: 1.0000\n",
            "Class 8, Training Precision: 1.0000\n",
            "Class 8, Training Recall: 1.0000\n",
            "Class 8, Training F1 Score: 1.0000\n",
            "Class 8, Training Confusion Matrix:\n",
            " [[1996    0]\n",
            " [   0 4004]]\n",
            "Attack model for class 8 trained.\n",
            "Class 9, Training Accuracy: 1.0000\n",
            "Class 9, Training Precision: 1.0000\n",
            "Class 9, Training Recall: 1.0000\n",
            "Class 9, Training F1 Score: 1.0000\n",
            "Class 9, Training Confusion Matrix:\n",
            " [[2013    0]\n",
            " [   0 3987]]\n",
            "Attack model for class 9 trained.\n",
            "Attack model for class 0 saved to attack_model_class_0.joblib\n",
            "Attack model for class 1 saved to attack_model_class_1.joblib\n",
            "Attack model for class 2 saved to attack_model_class_2.joblib\n",
            "Attack model for class 3 saved to attack_model_class_3.joblib\n",
            "Attack model for class 4 saved to attack_model_class_4.joblib\n",
            "Attack model for class 5 saved to attack_model_class_5.joblib\n",
            "Attack model for class 6 saved to attack_model_class_6.joblib\n",
            "Attack model for class 7 saved to attack_model_class_7.joblib\n",
            "Attack model for class 8 saved to attack_model_class_8.joblib\n",
            "Attack model for class 9 saved to attack_model_class_9.joblib\n",
            "Attack model for class 0 loaded from attack_model_class_0.joblib\n",
            "Attack model for class 1 loaded from attack_model_class_1.joblib\n",
            "Attack model for class 2 loaded from attack_model_class_2.joblib\n",
            "Attack model for class 3 loaded from attack_model_class_3.joblib\n",
            "Attack model for class 4 loaded from attack_model_class_4.joblib\n",
            "Attack model for class 5 loaded from attack_model_class_5.joblib\n",
            "Attack model for class 6 loaded from attack_model_class_6.joblib\n",
            "Attack model for class 7 loaded from attack_model_class_7.joblib\n",
            "Attack model for class 8 loaded from attack_model_class_8.joblib\n",
            "Attack model for class 9 loaded from attack_model_class_9.joblib\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Accuracies"
      ],
      "metadata": {
        "id": "hbWDP9nXQv1u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_base = CIFAR10Classifier()\n",
        "model_base.load_state_dict(torch.load('baseline_model.pth', map_location=device))\n",
        "model_base.to(device)\n",
        "\n",
        "accuracy_baseline = mia.evaluate_attack_model(train_loader_baseline, test_loader_baseline, model_base)\n",
        "print(f'Accuracy for Attacking to the Baseline Model :  {accuracy_baseline * 100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEzCnbY3QmJF",
        "outputId": "2586708e-0268-4189-c994-f63af8cab74e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for Attacking to the Baseline Model :  74.69%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_module_prefix(state_dict):\n",
        "    new_state_dict = {}\n",
        "    for k, v in state_dict.items():\n",
        "        if k.startswith('_module.'):\n",
        "            new_state_dict[k[8:]] = v  # remove '_module.' prefix\n",
        "        else:\n",
        "            new_state_dict[k] = v\n",
        "    return new_state_dict\n",
        "\n",
        "model_private = CIFAR10Classifier().to(device)\n",
        "private_state_dict = torch.load('modified_model.pth', map_location=device)\n",
        "model_private.load_state_dict(remove_module_prefix(private_state_dict))\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model_private.to(device)\n",
        "\n",
        "accuracy_private = mia.evaluate_attack_model(train_loader_modified, test_loader_modified, model_private)\n",
        "print(f'Accuracy for Attacking to the Private Model :  {accuracy_private * 100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9MyGDUIBQnwO",
        "outputId": "cf6e698c-19d9-4ab8-ce7d-a365da852601"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for Attacking to the Private Model :  60.33%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Num of Shadow Models = 50 , Batch Size = 64 , Epochs = 20"
      ],
      "metadata": {
        "id": "DoiXM5UDbHuU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
        "train_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Split training data into 80% and 20%\n",
        "train_size = int(0.8 * len(train_set))\n",
        "remaining_size = len(train_set) - train_size\n",
        "train_subset, remaining_subset = torch.utils.data.random_split(train_set, [train_size, remaining_size])\n",
        "\n",
        "# Create disjoint loaders for seen data from 80% training data\n",
        "num_shadow_models = 50\n",
        "seen_size_per_model = train_size // num_shadow_models\n",
        "seen_loaders = []\n",
        "\n",
        "for i in range(num_shadow_models):\n",
        "    start_idx = i * seen_size_per_model\n",
        "    end_idx = (i + 1) * seen_size_per_model\n",
        "    seen_indices = torch.arange(start_idx, end_idx)\n",
        "    seen_train_set = Subset(train_subset, seen_indices)\n",
        "    seen_loader = DataLoader(seen_train_set, batch_size=64, shuffle=True)\n",
        "    seen_loaders.append(seen_loader)\n",
        "\n",
        "# Create concatenated unseen data from the remaining 20% of training data and the entire test set\n",
        "unseen_dataset = ConcatDataset([remaining_subset, test_set])\n",
        "unseen_size_per_model = len(unseen_dataset) // num_shadow_models\n",
        "unseen_loaders = []\n",
        "\n",
        "for i in range(num_shadow_models):\n",
        "    start_idx = i * unseen_size_per_model\n",
        "    end_idx = (i + 1) * unseen_size_per_model\n",
        "    unseen_indices = torch.arange(start_idx, end_idx)\n",
        "    unseen_subset = Subset(unseen_dataset, unseen_indices)\n",
        "    unseen_loader = DataLoader(unseen_subset, batch_size=64, shuffle=False)\n",
        "    unseen_loaders.append(unseen_loader)\n",
        "\n",
        "test_loader = DataLoader(test_set, batch_size=64, shuffle=False)\n",
        "\n",
        "# Initialize MembershipInferenceAttackNoPrivacy\n",
        "mia = MembershipInferenceAttackNoPrivacy(CIFAR10Classifier, device)\n",
        "\n",
        "# Train shadow models without differential privacy\n",
        "mia.train_shadow_models(seen_loaders, num_epochs=20)\n",
        "\n",
        "# Collect outputs for attack model\n",
        "mia.collect_outputs(seen_loaders, unseen_loaders)\n",
        "\n",
        "# Train attack models\n",
        "mia.train_attack_models()\n",
        "\n",
        "# Save the attack models\n",
        "mia.save_attack_models('attack_model')\n",
        "\n",
        "# Load the attack models (for future use)\n",
        "mia.load_attack_models('attack_model')"
      ],
      "metadata": {
        "id": "hA-MChE0bH3k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42ad917f-d67f-48d7-f592-0b864405245d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch 1, Loss: 2.290871\n",
            "Epoch 2, Loss: 2.152292\n",
            "Epoch 3, Loss: 2.041647\n",
            "Epoch 4, Loss: 1.993175\n",
            "Epoch 5, Loss: 1.859375\n",
            "Epoch 6, Loss: 1.771865\n",
            "Epoch 7, Loss: 1.679776\n",
            "Epoch 8, Loss: 1.577215\n",
            "Epoch 9, Loss: 1.488882\n",
            "Epoch 10, Loss: 1.408353\n",
            "Epoch 11, Loss: 1.321452\n",
            "Epoch 12, Loss: 1.248266\n",
            "Epoch 13, Loss: 1.161570\n",
            "Epoch 14, Loss: 1.059208\n",
            "Epoch 15, Loss: 1.024660\n",
            "Epoch 16, Loss: 0.969470\n",
            "Epoch 17, Loss: 0.887253\n",
            "Epoch 18, Loss: 0.803858\n",
            "Epoch 19, Loss: 0.815177\n",
            "Epoch 20, Loss: 0.728020\n",
            "Shadow model 1 trained.\n",
            "Epoch 1, Loss: 2.278039\n",
            "Epoch 2, Loss: 2.108988\n",
            "Epoch 3, Loss: 1.991950\n",
            "Epoch 4, Loss: 1.860687\n",
            "Epoch 5, Loss: 1.774709\n",
            "Epoch 6, Loss: 1.627685\n",
            "Epoch 7, Loss: 1.544432\n",
            "Epoch 8, Loss: 1.429359\n",
            "Epoch 9, Loss: 1.317914\n",
            "Epoch 10, Loss: 1.223228\n",
            "Epoch 11, Loss: 1.142226\n",
            "Epoch 12, Loss: 1.054439\n",
            "Epoch 13, Loss: 1.039334\n",
            "Epoch 14, Loss: 0.914852\n",
            "Epoch 15, Loss: 0.866836\n",
            "Epoch 16, Loss: 0.818027\n",
            "Epoch 17, Loss: 0.763148\n",
            "Epoch 18, Loss: 0.712251\n",
            "Epoch 19, Loss: 0.648623\n",
            "Epoch 20, Loss: 0.609620\n",
            "Shadow model 2 trained.\n",
            "Epoch 1, Loss: 2.278240\n",
            "Epoch 2, Loss: 2.142209\n",
            "Epoch 3, Loss: 2.031545\n",
            "Epoch 4, Loss: 1.925753\n",
            "Epoch 5, Loss: 1.840856\n",
            "Epoch 6, Loss: 1.779210\n",
            "Epoch 7, Loss: 1.679920\n",
            "Epoch 8, Loss: 1.617977\n",
            "Epoch 9, Loss: 1.544518\n",
            "Epoch 10, Loss: 1.432131\n",
            "Epoch 11, Loss: 1.401953\n",
            "Epoch 12, Loss: 1.259174\n",
            "Epoch 13, Loss: 1.195803\n",
            "Epoch 14, Loss: 1.125626\n",
            "Epoch 15, Loss: 1.083021\n",
            "Epoch 16, Loss: 1.010166\n",
            "Epoch 17, Loss: 1.012540\n",
            "Epoch 18, Loss: 0.903966\n",
            "Epoch 19, Loss: 0.907689\n",
            "Epoch 20, Loss: 0.822309\n",
            "Shadow model 3 trained.\n",
            "Epoch 1, Loss: 2.257319\n",
            "Epoch 2, Loss: 2.117171\n",
            "Epoch 3, Loss: 1.996647\n",
            "Epoch 4, Loss: 1.933146\n",
            "Epoch 5, Loss: 1.789245\n",
            "Epoch 6, Loss: 1.657753\n",
            "Epoch 7, Loss: 1.543850\n",
            "Epoch 8, Loss: 1.495168\n",
            "Epoch 9, Loss: 1.407748\n",
            "Epoch 10, Loss: 1.359974\n",
            "Epoch 11, Loss: 1.209762\n",
            "Epoch 12, Loss: 1.102567\n",
            "Epoch 13, Loss: 1.073350\n",
            "Epoch 14, Loss: 0.945180\n",
            "Epoch 15, Loss: 0.916030\n",
            "Epoch 16, Loss: 0.841696\n",
            "Epoch 17, Loss: 0.772928\n",
            "Epoch 18, Loss: 0.704604\n",
            "Epoch 19, Loss: 0.695364\n",
            "Epoch 20, Loss: 0.620745\n",
            "Shadow model 4 trained.\n",
            "Epoch 1, Loss: 2.292882\n",
            "Epoch 2, Loss: 2.161904\n",
            "Epoch 3, Loss: 2.062881\n",
            "Epoch 4, Loss: 1.969591\n",
            "Epoch 5, Loss: 1.840017\n",
            "Epoch 6, Loss: 1.704200\n",
            "Epoch 7, Loss: 1.664064\n",
            "Epoch 8, Loss: 1.523417\n",
            "Epoch 9, Loss: 1.366669\n",
            "Epoch 10, Loss: 1.366108\n",
            "Epoch 11, Loss: 1.221427\n",
            "Epoch 12, Loss: 1.111123\n",
            "Epoch 13, Loss: 1.038099\n",
            "Epoch 14, Loss: 1.015998\n",
            "Epoch 15, Loss: 0.933064\n",
            "Epoch 16, Loss: 0.860220\n",
            "Epoch 17, Loss: 0.772156\n",
            "Epoch 18, Loss: 0.711714\n",
            "Epoch 19, Loss: 0.695514\n",
            "Epoch 20, Loss: 0.618401\n",
            "Shadow model 5 trained.\n",
            "Epoch 1, Loss: 2.271186\n",
            "Epoch 2, Loss: 2.140587\n",
            "Epoch 3, Loss: 2.047373\n",
            "Epoch 4, Loss: 1.958256\n",
            "Epoch 5, Loss: 1.885480\n",
            "Epoch 6, Loss: 1.802103\n",
            "Epoch 7, Loss: 1.647925\n",
            "Epoch 8, Loss: 1.576911\n",
            "Epoch 9, Loss: 1.457371\n",
            "Epoch 10, Loss: 1.392081\n",
            "Epoch 11, Loss: 1.290873\n",
            "Epoch 12, Loss: 1.262096\n",
            "Epoch 13, Loss: 1.093433\n",
            "Epoch 14, Loss: 1.047042\n",
            "Epoch 15, Loss: 0.970975\n",
            "Epoch 16, Loss: 0.869206\n",
            "Epoch 17, Loss: 0.847204\n",
            "Epoch 18, Loss: 0.726984\n",
            "Epoch 19, Loss: 0.711783\n",
            "Epoch 20, Loss: 0.739372\n",
            "Shadow model 6 trained.\n",
            "Epoch 1, Loss: 2.277834\n",
            "Epoch 2, Loss: 2.151062\n",
            "Epoch 3, Loss: 2.074496\n",
            "Epoch 4, Loss: 1.971429\n",
            "Epoch 5, Loss: 1.854564\n",
            "Epoch 6, Loss: 1.782696\n",
            "Epoch 7, Loss: 1.718958\n",
            "Epoch 8, Loss: 1.631142\n",
            "Epoch 9, Loss: 1.516861\n",
            "Epoch 10, Loss: 1.435016\n",
            "Epoch 11, Loss: 1.375340\n",
            "Epoch 12, Loss: 1.295827\n",
            "Epoch 13, Loss: 1.227490\n",
            "Epoch 14, Loss: 1.102395\n",
            "Epoch 15, Loss: 1.095528\n",
            "Epoch 16, Loss: 1.077208\n",
            "Epoch 17, Loss: 0.953497\n",
            "Epoch 18, Loss: 0.835878\n",
            "Epoch 19, Loss: 0.860439\n",
            "Epoch 20, Loss: 0.800374\n",
            "Shadow model 7 trained.\n",
            "Epoch 1, Loss: 2.289824\n",
            "Epoch 2, Loss: 2.178810\n",
            "Epoch 3, Loss: 2.050549\n",
            "Epoch 4, Loss: 1.929666\n",
            "Epoch 5, Loss: 1.822390\n",
            "Epoch 6, Loss: 1.722279\n",
            "Epoch 7, Loss: 1.629156\n",
            "Epoch 8, Loss: 1.500505\n",
            "Epoch 9, Loss: 1.409244\n",
            "Epoch 10, Loss: 1.348659\n",
            "Epoch 11, Loss: 1.263166\n",
            "Epoch 12, Loss: 1.120128\n",
            "Epoch 13, Loss: 1.093842\n",
            "Epoch 14, Loss: 1.038235\n",
            "Epoch 15, Loss: 0.898673\n",
            "Epoch 16, Loss: 0.841858\n",
            "Epoch 17, Loss: 0.799087\n",
            "Epoch 18, Loss: 0.747767\n",
            "Epoch 19, Loss: 0.684818\n",
            "Epoch 20, Loss: 0.685991\n",
            "Shadow model 8 trained.\n",
            "Epoch 1, Loss: 2.283004\n",
            "Epoch 2, Loss: 2.135616\n",
            "Epoch 3, Loss: 2.027576\n",
            "Epoch 4, Loss: 1.890380\n",
            "Epoch 5, Loss: 1.825102\n",
            "Epoch 6, Loss: 1.719608\n",
            "Epoch 7, Loss: 1.592610\n",
            "Epoch 8, Loss: 1.490610\n",
            "Epoch 9, Loss: 1.422874\n",
            "Epoch 10, Loss: 1.338492\n",
            "Epoch 11, Loss: 1.215331\n",
            "Epoch 12, Loss: 1.204073\n",
            "Epoch 13, Loss: 1.114117\n",
            "Epoch 14, Loss: 1.009788\n",
            "Epoch 15, Loss: 0.954872\n",
            "Epoch 16, Loss: 0.903216\n",
            "Epoch 17, Loss: 0.863619\n",
            "Epoch 18, Loss: 0.772054\n",
            "Epoch 19, Loss: 0.714090\n",
            "Epoch 20, Loss: 0.698900\n",
            "Shadow model 9 trained.\n",
            "Epoch 1, Loss: 2.301965\n",
            "Epoch 2, Loss: 2.186222\n",
            "Epoch 3, Loss: 2.089050\n",
            "Epoch 4, Loss: 1.942540\n",
            "Epoch 5, Loss: 1.851115\n",
            "Epoch 6, Loss: 1.783835\n",
            "Epoch 7, Loss: 1.703188\n",
            "Epoch 8, Loss: 1.619100\n",
            "Epoch 9, Loss: 1.479190\n",
            "Epoch 10, Loss: 1.424128\n",
            "Epoch 11, Loss: 1.321403\n",
            "Epoch 12, Loss: 1.264094\n",
            "Epoch 13, Loss: 1.170999\n",
            "Epoch 14, Loss: 1.081541\n",
            "Epoch 15, Loss: 0.974403\n",
            "Epoch 16, Loss: 0.978898\n",
            "Epoch 17, Loss: 0.901143\n",
            "Epoch 18, Loss: 0.864292\n",
            "Epoch 19, Loss: 0.843164\n",
            "Epoch 20, Loss: 0.751180\n",
            "Shadow model 10 trained.\n",
            "Epoch 1, Loss: 2.263544\n",
            "Epoch 2, Loss: 2.131544\n",
            "Epoch 3, Loss: 2.030056\n",
            "Epoch 4, Loss: 1.894479\n",
            "Epoch 5, Loss: 1.832560\n",
            "Epoch 6, Loss: 1.733007\n",
            "Epoch 7, Loss: 1.639807\n",
            "Epoch 8, Loss: 1.579858\n",
            "Epoch 9, Loss: 1.450101\n",
            "Epoch 10, Loss: 1.388841\n",
            "Epoch 11, Loss: 1.314298\n",
            "Epoch 12, Loss: 1.184289\n",
            "Epoch 13, Loss: 1.143593\n",
            "Epoch 14, Loss: 1.072905\n",
            "Epoch 15, Loss: 1.015072\n",
            "Epoch 16, Loss: 0.996677\n",
            "Epoch 17, Loss: 0.878844\n",
            "Epoch 18, Loss: 0.837729\n",
            "Epoch 19, Loss: 0.772596\n",
            "Epoch 20, Loss: 0.723754\n",
            "Shadow model 11 trained.\n",
            "Epoch 1, Loss: 2.227642\n",
            "Epoch 2, Loss: 2.044709\n",
            "Epoch 3, Loss: 1.871814\n",
            "Epoch 4, Loss: 1.752563\n",
            "Epoch 5, Loss: 1.647210\n",
            "Epoch 6, Loss: 1.505582\n",
            "Epoch 7, Loss: 1.408027\n",
            "Epoch 8, Loss: 1.316498\n",
            "Epoch 9, Loss: 1.215550\n",
            "Epoch 10, Loss: 1.162997\n",
            "Epoch 11, Loss: 1.100205\n",
            "Epoch 12, Loss: 1.019083\n",
            "Epoch 13, Loss: 0.914885\n",
            "Epoch 14, Loss: 0.834010\n",
            "Epoch 15, Loss: 0.799981\n",
            "Epoch 16, Loss: 0.728015\n",
            "Epoch 17, Loss: 0.720699\n",
            "Epoch 18, Loss: 0.642878\n",
            "Epoch 19, Loss: 0.568814\n",
            "Epoch 20, Loss: 0.517217\n",
            "Shadow model 12 trained.\n",
            "Epoch 1, Loss: 2.278242\n",
            "Epoch 2, Loss: 2.157040\n",
            "Epoch 3, Loss: 2.048707\n",
            "Epoch 4, Loss: 1.976249\n",
            "Epoch 5, Loss: 1.888722\n",
            "Epoch 6, Loss: 1.742975\n",
            "Epoch 7, Loss: 1.643675\n",
            "Epoch 8, Loss: 1.575787\n",
            "Epoch 9, Loss: 1.483626\n",
            "Epoch 10, Loss: 1.376938\n",
            "Epoch 11, Loss: 1.300702\n",
            "Epoch 12, Loss: 1.208068\n",
            "Epoch 13, Loss: 1.110670\n",
            "Epoch 14, Loss: 1.053178\n",
            "Epoch 15, Loss: 0.965886\n",
            "Epoch 16, Loss: 0.884024\n",
            "Epoch 17, Loss: 0.894940\n",
            "Epoch 18, Loss: 0.816066\n",
            "Epoch 19, Loss: 0.783088\n",
            "Epoch 20, Loss: 0.717705\n",
            "Shadow model 13 trained.\n",
            "Epoch 1, Loss: 2.283626\n",
            "Epoch 2, Loss: 2.144605\n",
            "Epoch 3, Loss: 2.024832\n",
            "Epoch 4, Loss: 1.943757\n",
            "Epoch 5, Loss: 1.812801\n",
            "Epoch 6, Loss: 1.690755\n",
            "Epoch 7, Loss: 1.584660\n",
            "Epoch 8, Loss: 1.447495\n",
            "Epoch 9, Loss: 1.378080\n",
            "Epoch 10, Loss: 1.264678\n",
            "Epoch 11, Loss: 1.167482\n",
            "Epoch 12, Loss: 1.098651\n",
            "Epoch 13, Loss: 0.981527\n",
            "Epoch 14, Loss: 0.899958\n",
            "Epoch 15, Loss: 0.843222\n",
            "Epoch 16, Loss: 0.752379\n",
            "Epoch 17, Loss: 0.689851\n",
            "Epoch 18, Loss: 0.625516\n",
            "Epoch 19, Loss: 0.592863\n",
            "Epoch 20, Loss: 0.528501\n",
            "Shadow model 14 trained.\n",
            "Epoch 1, Loss: 2.280142\n",
            "Epoch 2, Loss: 2.142381\n",
            "Epoch 3, Loss: 2.037245\n",
            "Epoch 4, Loss: 1.956476\n",
            "Epoch 5, Loss: 1.851725\n",
            "Epoch 6, Loss: 1.770215\n",
            "Epoch 7, Loss: 1.717299\n",
            "Epoch 8, Loss: 1.615789\n",
            "Epoch 9, Loss: 1.548997\n",
            "Epoch 10, Loss: 1.412043\n",
            "Epoch 11, Loss: 1.273414\n",
            "Epoch 12, Loss: 1.262462\n",
            "Epoch 13, Loss: 1.139135\n",
            "Epoch 14, Loss: 1.053208\n",
            "Epoch 15, Loss: 1.031003\n",
            "Epoch 16, Loss: 0.934441\n",
            "Epoch 17, Loss: 0.861810\n",
            "Epoch 18, Loss: 0.809732\n",
            "Epoch 19, Loss: 0.767004\n",
            "Epoch 20, Loss: 0.728174\n",
            "Shadow model 15 trained.\n",
            "Epoch 1, Loss: 2.258866\n",
            "Epoch 2, Loss: 2.115378\n",
            "Epoch 3, Loss: 2.013244\n",
            "Epoch 4, Loss: 1.917253\n",
            "Epoch 5, Loss: 1.802991\n",
            "Epoch 6, Loss: 1.718700\n",
            "Epoch 7, Loss: 1.618095\n",
            "Epoch 8, Loss: 1.505761\n",
            "Epoch 9, Loss: 1.420540\n",
            "Epoch 10, Loss: 1.378761\n",
            "Epoch 11, Loss: 1.257502\n",
            "Epoch 12, Loss: 1.138759\n",
            "Epoch 13, Loss: 1.069015\n",
            "Epoch 14, Loss: 1.021236\n",
            "Epoch 15, Loss: 0.969277\n",
            "Epoch 16, Loss: 0.881465\n",
            "Epoch 17, Loss: 0.839456\n",
            "Epoch 18, Loss: 0.800944\n",
            "Epoch 19, Loss: 0.730823\n",
            "Epoch 20, Loss: 0.677946\n",
            "Shadow model 16 trained.\n",
            "Epoch 1, Loss: 2.288557\n",
            "Epoch 2, Loss: 2.185107\n",
            "Epoch 3, Loss: 2.102625\n",
            "Epoch 4, Loss: 1.941585\n",
            "Epoch 5, Loss: 1.885074\n",
            "Epoch 6, Loss: 1.784934\n",
            "Epoch 7, Loss: 1.653491\n",
            "Epoch 8, Loss: 1.539546\n",
            "Epoch 9, Loss: 1.494916\n",
            "Epoch 10, Loss: 1.402477\n",
            "Epoch 11, Loss: 1.294135\n",
            "Epoch 12, Loss: 1.141237\n",
            "Epoch 13, Loss: 1.187461\n",
            "Epoch 14, Loss: 1.059014\n",
            "Epoch 15, Loss: 1.040959\n",
            "Epoch 16, Loss: 0.922088\n",
            "Epoch 17, Loss: 0.861797\n",
            "Epoch 18, Loss: 0.776409\n",
            "Epoch 19, Loss: 0.764757\n",
            "Epoch 20, Loss: 0.665485\n",
            "Shadow model 17 trained.\n",
            "Epoch 1, Loss: 2.319630\n",
            "Epoch 2, Loss: 2.186420\n",
            "Epoch 3, Loss: 2.090219\n",
            "Epoch 4, Loss: 1.948820\n",
            "Epoch 5, Loss: 1.817543\n",
            "Epoch 6, Loss: 1.712957\n",
            "Epoch 7, Loss: 1.610222\n",
            "Epoch 8, Loss: 1.512257\n",
            "Epoch 9, Loss: 1.471943\n",
            "Epoch 10, Loss: 1.371765\n",
            "Epoch 11, Loss: 1.210637\n",
            "Epoch 12, Loss: 1.141020\n",
            "Epoch 13, Loss: 1.031505\n",
            "Epoch 14, Loss: 0.971835\n",
            "Epoch 15, Loss: 0.907376\n",
            "Epoch 16, Loss: 0.873527\n",
            "Epoch 17, Loss: 0.817091\n",
            "Epoch 18, Loss: 0.753968\n",
            "Epoch 19, Loss: 0.739315\n",
            "Epoch 20, Loss: 0.635678\n",
            "Shadow model 18 trained.\n",
            "Epoch 1, Loss: 2.304049\n",
            "Epoch 2, Loss: 2.180178\n",
            "Epoch 3, Loss: 2.106066\n",
            "Epoch 4, Loss: 2.014375\n",
            "Epoch 5, Loss: 1.931930\n",
            "Epoch 6, Loss: 1.834435\n",
            "Epoch 7, Loss: 1.759222\n",
            "Epoch 8, Loss: 1.652602\n",
            "Epoch 9, Loss: 1.570248\n",
            "Epoch 10, Loss: 1.410741\n",
            "Epoch 11, Loss: 1.349915\n",
            "Epoch 12, Loss: 1.187610\n",
            "Epoch 13, Loss: 1.144612\n",
            "Epoch 14, Loss: 1.096833\n",
            "Epoch 15, Loss: 1.021596\n",
            "Epoch 16, Loss: 0.935049\n",
            "Epoch 17, Loss: 0.870951\n",
            "Epoch 18, Loss: 0.804667\n",
            "Epoch 19, Loss: 0.777373\n",
            "Epoch 20, Loss: 0.733410\n",
            "Shadow model 19 trained.\n",
            "Epoch 1, Loss: 2.312862\n",
            "Epoch 2, Loss: 2.216915\n",
            "Epoch 3, Loss: 2.118561\n",
            "Epoch 4, Loss: 1.971073\n",
            "Epoch 5, Loss: 1.852100\n",
            "Epoch 6, Loss: 1.761312\n",
            "Epoch 7, Loss: 1.650770\n",
            "Epoch 8, Loss: 1.574721\n",
            "Epoch 9, Loss: 1.444468\n",
            "Epoch 10, Loss: 1.341110\n",
            "Epoch 11, Loss: 1.298628\n",
            "Epoch 12, Loss: 1.179972\n",
            "Epoch 13, Loss: 1.106828\n",
            "Epoch 14, Loss: 1.040711\n",
            "Epoch 15, Loss: 1.036471\n",
            "Epoch 16, Loss: 0.939691\n",
            "Epoch 17, Loss: 0.861034\n",
            "Epoch 18, Loss: 0.782914\n",
            "Epoch 19, Loss: 0.796452\n",
            "Epoch 20, Loss: 0.746613\n",
            "Shadow model 20 trained.\n",
            "Epoch 1, Loss: 2.285048\n",
            "Epoch 2, Loss: 2.153863\n",
            "Epoch 3, Loss: 2.086821\n",
            "Epoch 4, Loss: 1.954634\n",
            "Epoch 5, Loss: 1.873686\n",
            "Epoch 6, Loss: 1.750114\n",
            "Epoch 7, Loss: 1.663925\n",
            "Epoch 8, Loss: 1.579723\n",
            "Epoch 9, Loss: 1.486295\n",
            "Epoch 10, Loss: 1.371365\n",
            "Epoch 11, Loss: 1.287040\n",
            "Epoch 12, Loss: 1.180062\n",
            "Epoch 13, Loss: 1.176032\n",
            "Epoch 14, Loss: 1.069216\n",
            "Epoch 15, Loss: 0.949206\n",
            "Epoch 16, Loss: 0.906448\n",
            "Epoch 17, Loss: 0.870723\n",
            "Epoch 18, Loss: 0.855526\n",
            "Epoch 19, Loss: 0.830091\n",
            "Epoch 20, Loss: 0.754066\n",
            "Shadow model 21 trained.\n",
            "Epoch 1, Loss: 2.266225\n",
            "Epoch 2, Loss: 2.156204\n",
            "Epoch 3, Loss: 2.074748\n",
            "Epoch 4, Loss: 1.968855\n",
            "Epoch 5, Loss: 1.898354\n",
            "Epoch 6, Loss: 1.859465\n",
            "Epoch 7, Loss: 1.734258\n",
            "Epoch 8, Loss: 1.646522\n",
            "Epoch 9, Loss: 1.574656\n",
            "Epoch 10, Loss: 1.484360\n",
            "Epoch 11, Loss: 1.417665\n",
            "Epoch 12, Loss: 1.306885\n",
            "Epoch 13, Loss: 1.268157\n",
            "Epoch 14, Loss: 1.195019\n",
            "Epoch 15, Loss: 1.115652\n",
            "Epoch 16, Loss: 1.051049\n",
            "Epoch 17, Loss: 0.988097\n",
            "Epoch 18, Loss: 0.909792\n",
            "Epoch 19, Loss: 0.891493\n",
            "Epoch 20, Loss: 0.852680\n",
            "Shadow model 22 trained.\n",
            "Epoch 1, Loss: 2.252729\n",
            "Epoch 2, Loss: 2.073200\n",
            "Epoch 3, Loss: 1.953558\n",
            "Epoch 4, Loss: 1.909796\n",
            "Epoch 5, Loss: 1.800794\n",
            "Epoch 6, Loss: 1.745004\n",
            "Epoch 7, Loss: 1.660623\n",
            "Epoch 8, Loss: 1.582804\n",
            "Epoch 9, Loss: 1.523639\n",
            "Epoch 10, Loss: 1.453791\n",
            "Epoch 11, Loss: 1.358107\n",
            "Epoch 12, Loss: 1.288841\n",
            "Epoch 13, Loss: 1.242654\n",
            "Epoch 14, Loss: 1.136527\n",
            "Epoch 15, Loss: 1.073021\n",
            "Epoch 16, Loss: 1.081435\n",
            "Epoch 17, Loss: 0.995181\n",
            "Epoch 18, Loss: 0.986117\n",
            "Epoch 19, Loss: 0.917295\n",
            "Epoch 20, Loss: 0.900044\n",
            "Shadow model 23 trained.\n",
            "Epoch 1, Loss: 2.287255\n",
            "Epoch 2, Loss: 2.152791\n",
            "Epoch 3, Loss: 2.086024\n",
            "Epoch 4, Loss: 2.007687\n",
            "Epoch 5, Loss: 1.955125\n",
            "Epoch 6, Loss: 1.853867\n",
            "Epoch 7, Loss: 1.692996\n",
            "Epoch 8, Loss: 1.652460\n",
            "Epoch 9, Loss: 1.522611\n",
            "Epoch 10, Loss: 1.482122\n",
            "Epoch 11, Loss: 1.409521\n",
            "Epoch 12, Loss: 1.243126\n",
            "Epoch 13, Loss: 1.202274\n",
            "Epoch 14, Loss: 1.098631\n",
            "Epoch 15, Loss: 1.086209\n",
            "Epoch 16, Loss: 0.991726\n",
            "Epoch 17, Loss: 0.992212\n",
            "Epoch 18, Loss: 0.879100\n",
            "Epoch 19, Loss: 0.825647\n",
            "Epoch 20, Loss: 0.794097\n",
            "Shadow model 24 trained.\n",
            "Epoch 1, Loss: 2.277887\n",
            "Epoch 2, Loss: 2.144404\n",
            "Epoch 3, Loss: 2.046851\n",
            "Epoch 4, Loss: 1.940056\n",
            "Epoch 5, Loss: 1.803843\n",
            "Epoch 6, Loss: 1.706070\n",
            "Epoch 7, Loss: 1.548736\n",
            "Epoch 8, Loss: 1.450470\n",
            "Epoch 9, Loss: 1.348466\n",
            "Epoch 10, Loss: 1.283736\n",
            "Epoch 11, Loss: 1.213088\n",
            "Epoch 12, Loss: 1.121797\n",
            "Epoch 13, Loss: 1.027800\n",
            "Epoch 14, Loss: 0.948734\n",
            "Epoch 15, Loss: 0.892323\n",
            "Epoch 16, Loss: 0.779901\n",
            "Epoch 17, Loss: 0.726020\n",
            "Epoch 18, Loss: 0.755641\n",
            "Epoch 19, Loss: 0.669529\n",
            "Epoch 20, Loss: 0.631371\n",
            "Shadow model 25 trained.\n",
            "Epoch 1, Loss: 2.288281\n",
            "Epoch 2, Loss: 2.163371\n",
            "Epoch 3, Loss: 2.021363\n",
            "Epoch 4, Loss: 1.947985\n",
            "Epoch 5, Loss: 1.857581\n",
            "Epoch 6, Loss: 1.691274\n",
            "Epoch 7, Loss: 1.543916\n",
            "Epoch 8, Loss: 1.469188\n",
            "Epoch 9, Loss: 1.357078\n",
            "Epoch 10, Loss: 1.264081\n",
            "Epoch 11, Loss: 1.166182\n",
            "Epoch 12, Loss: 1.079756\n",
            "Epoch 13, Loss: 0.974460\n",
            "Epoch 14, Loss: 0.924511\n",
            "Epoch 15, Loss: 0.824182\n",
            "Epoch 16, Loss: 0.774672\n",
            "Epoch 17, Loss: 0.730198\n",
            "Epoch 18, Loss: 0.648872\n",
            "Epoch 19, Loss: 0.634972\n",
            "Epoch 20, Loss: 0.569533\n",
            "Shadow model 26 trained.\n",
            "Epoch 1, Loss: 2.277639\n",
            "Epoch 2, Loss: 2.111010\n",
            "Epoch 3, Loss: 2.004228\n",
            "Epoch 4, Loss: 1.886971\n",
            "Epoch 5, Loss: 1.837708\n",
            "Epoch 6, Loss: 1.676084\n",
            "Epoch 7, Loss: 1.590554\n",
            "Epoch 8, Loss: 1.457925\n",
            "Epoch 9, Loss: 1.361063\n",
            "Epoch 10, Loss: 1.279746\n",
            "Epoch 11, Loss: 1.191039\n",
            "Epoch 12, Loss: 1.048465\n",
            "Epoch 13, Loss: 0.967932\n",
            "Epoch 14, Loss: 0.944277\n",
            "Epoch 15, Loss: 0.866956\n",
            "Epoch 16, Loss: 0.787166\n",
            "Epoch 17, Loss: 0.798161\n",
            "Epoch 18, Loss: 0.715438\n",
            "Epoch 19, Loss: 0.652698\n",
            "Epoch 20, Loss: 0.638384\n",
            "Shadow model 27 trained.\n",
            "Epoch 1, Loss: 2.316184\n",
            "Epoch 2, Loss: 2.197834\n",
            "Epoch 3, Loss: 2.090850\n",
            "Epoch 4, Loss: 1.981772\n",
            "Epoch 5, Loss: 1.912389\n",
            "Epoch 6, Loss: 1.809096\n",
            "Epoch 7, Loss: 1.732601\n",
            "Epoch 8, Loss: 1.650804\n",
            "Epoch 9, Loss: 1.558839\n",
            "Epoch 10, Loss: 1.461071\n",
            "Epoch 11, Loss: 1.412651\n",
            "Epoch 12, Loss: 1.333594\n",
            "Epoch 13, Loss: 1.285696\n",
            "Epoch 14, Loss: 1.213524\n",
            "Epoch 15, Loss: 1.150700\n",
            "Epoch 16, Loss: 1.061315\n",
            "Epoch 17, Loss: 0.967116\n",
            "Epoch 18, Loss: 0.960626\n",
            "Epoch 19, Loss: 0.950597\n",
            "Epoch 20, Loss: 0.819030\n",
            "Shadow model 28 trained.\n",
            "Epoch 1, Loss: 2.291939\n",
            "Epoch 2, Loss: 2.164514\n",
            "Epoch 3, Loss: 2.042570\n",
            "Epoch 4, Loss: 1.979824\n",
            "Epoch 5, Loss: 1.930537\n",
            "Epoch 6, Loss: 1.821405\n",
            "Epoch 7, Loss: 1.747756\n",
            "Epoch 8, Loss: 1.589906\n",
            "Epoch 9, Loss: 1.521437\n",
            "Epoch 10, Loss: 1.468293\n",
            "Epoch 11, Loss: 1.375685\n",
            "Epoch 12, Loss: 1.268216\n",
            "Epoch 13, Loss: 1.153986\n",
            "Epoch 14, Loss: 1.147002\n",
            "Epoch 15, Loss: 1.074504\n",
            "Epoch 16, Loss: 1.043751\n",
            "Epoch 17, Loss: 0.957882\n",
            "Epoch 18, Loss: 0.930151\n",
            "Epoch 19, Loss: 0.839571\n",
            "Epoch 20, Loss: 0.784153\n",
            "Shadow model 29 trained.\n",
            "Epoch 1, Loss: 2.293581\n",
            "Epoch 2, Loss: 2.176486\n",
            "Epoch 3, Loss: 2.095184\n",
            "Epoch 4, Loss: 2.013281\n",
            "Epoch 5, Loss: 1.908321\n",
            "Epoch 6, Loss: 1.827905\n",
            "Epoch 7, Loss: 1.700863\n",
            "Epoch 8, Loss: 1.634059\n",
            "Epoch 9, Loss: 1.574637\n",
            "Epoch 10, Loss: 1.479778\n",
            "Epoch 11, Loss: 1.441371\n",
            "Epoch 12, Loss: 1.367617\n",
            "Epoch 13, Loss: 1.289567\n",
            "Epoch 14, Loss: 1.240782\n",
            "Epoch 15, Loss: 1.183142\n",
            "Epoch 16, Loss: 1.148743\n",
            "Epoch 17, Loss: 1.019550\n",
            "Epoch 18, Loss: 0.937338\n",
            "Epoch 19, Loss: 0.999778\n",
            "Epoch 20, Loss: 0.925850\n",
            "Shadow model 30 trained.\n",
            "Epoch 1, Loss: 2.267072\n",
            "Epoch 2, Loss: 2.040175\n",
            "Epoch 3, Loss: 1.916730\n",
            "Epoch 4, Loss: 1.801822\n",
            "Epoch 5, Loss: 1.696120\n",
            "Epoch 6, Loss: 1.573215\n",
            "Epoch 7, Loss: 1.469310\n",
            "Epoch 8, Loss: 1.346447\n",
            "Epoch 9, Loss: 1.289234\n",
            "Epoch 10, Loss: 1.211572\n",
            "Epoch 11, Loss: 1.135866\n",
            "Epoch 12, Loss: 1.047277\n",
            "Epoch 13, Loss: 0.979606\n",
            "Epoch 14, Loss: 0.938860\n",
            "Epoch 15, Loss: 0.813231\n",
            "Epoch 16, Loss: 0.750269\n",
            "Epoch 17, Loss: 0.659584\n",
            "Epoch 18, Loss: 0.698324\n",
            "Epoch 19, Loss: 0.634565\n",
            "Epoch 20, Loss: 0.538350\n",
            "Shadow model 31 trained.\n",
            "Epoch 1, Loss: 2.280381\n",
            "Epoch 2, Loss: 2.133787\n",
            "Epoch 3, Loss: 2.046643\n",
            "Epoch 4, Loss: 1.977279\n",
            "Epoch 5, Loss: 1.865796\n",
            "Epoch 6, Loss: 1.767703\n",
            "Epoch 7, Loss: 1.678880\n",
            "Epoch 8, Loss: 1.588800\n",
            "Epoch 9, Loss: 1.514458\n",
            "Epoch 10, Loss: 1.399982\n",
            "Epoch 11, Loss: 1.342133\n",
            "Epoch 12, Loss: 1.263341\n",
            "Epoch 13, Loss: 1.245622\n",
            "Epoch 14, Loss: 1.110745\n",
            "Epoch 15, Loss: 1.031509\n",
            "Epoch 16, Loss: 0.999098\n",
            "Epoch 17, Loss: 0.942642\n",
            "Epoch 18, Loss: 0.820922\n",
            "Epoch 19, Loss: 0.831340\n",
            "Epoch 20, Loss: 0.738405\n",
            "Shadow model 32 trained.\n",
            "Epoch 1, Loss: 2.280293\n",
            "Epoch 2, Loss: 2.142938\n",
            "Epoch 3, Loss: 2.052102\n",
            "Epoch 4, Loss: 1.934224\n",
            "Epoch 5, Loss: 1.819077\n",
            "Epoch 6, Loss: 1.754923\n",
            "Epoch 7, Loss: 1.656885\n",
            "Epoch 8, Loss: 1.516603\n",
            "Epoch 9, Loss: 1.450418\n",
            "Epoch 10, Loss: 1.376345\n",
            "Epoch 11, Loss: 1.309015\n",
            "Epoch 12, Loss: 1.212183\n",
            "Epoch 13, Loss: 1.104827\n",
            "Epoch 14, Loss: 1.045252\n",
            "Epoch 15, Loss: 0.995856\n",
            "Epoch 16, Loss: 0.896062\n",
            "Epoch 17, Loss: 0.897918\n",
            "Epoch 18, Loss: 0.783233\n",
            "Epoch 19, Loss: 0.726684\n",
            "Epoch 20, Loss: 0.693391\n",
            "Shadow model 33 trained.\n",
            "Epoch 1, Loss: 2.275958\n",
            "Epoch 2, Loss: 2.131745\n",
            "Epoch 3, Loss: 2.021844\n",
            "Epoch 4, Loss: 1.939371\n",
            "Epoch 5, Loss: 1.838057\n",
            "Epoch 6, Loss: 1.740494\n",
            "Epoch 7, Loss: 1.587393\n",
            "Epoch 8, Loss: 1.474538\n",
            "Epoch 9, Loss: 1.390931\n",
            "Epoch 10, Loss: 1.244131\n",
            "Epoch 11, Loss: 1.252887\n",
            "Epoch 12, Loss: 1.107564\n",
            "Epoch 13, Loss: 1.046332\n",
            "Epoch 14, Loss: 1.020709\n",
            "Epoch 15, Loss: 0.952223\n",
            "Epoch 16, Loss: 0.825778\n",
            "Epoch 17, Loss: 0.770274\n",
            "Epoch 18, Loss: 0.743770\n",
            "Epoch 19, Loss: 0.690848\n",
            "Epoch 20, Loss: 0.657254\n",
            "Shadow model 34 trained.\n",
            "Epoch 1, Loss: 2.313947\n",
            "Epoch 2, Loss: 2.181736\n",
            "Epoch 3, Loss: 2.104363\n",
            "Epoch 4, Loss: 1.974253\n",
            "Epoch 5, Loss: 1.916070\n",
            "Epoch 6, Loss: 1.866845\n",
            "Epoch 7, Loss: 1.775042\n",
            "Epoch 8, Loss: 1.662154\n",
            "Epoch 9, Loss: 1.646677\n",
            "Epoch 10, Loss: 1.568819\n",
            "Epoch 11, Loss: 1.466327\n",
            "Epoch 12, Loss: 1.469371\n",
            "Epoch 13, Loss: 1.317874\n",
            "Epoch 14, Loss: 1.229099\n",
            "Epoch 15, Loss: 1.125808\n",
            "Epoch 16, Loss: 1.084151\n",
            "Epoch 17, Loss: 1.038078\n",
            "Epoch 18, Loss: 0.987390\n",
            "Epoch 19, Loss: 0.950747\n",
            "Epoch 20, Loss: 0.851063\n",
            "Shadow model 35 trained.\n",
            "Epoch 1, Loss: 2.278755\n",
            "Epoch 2, Loss: 2.132875\n",
            "Epoch 3, Loss: 2.007988\n",
            "Epoch 4, Loss: 1.926572\n",
            "Epoch 5, Loss: 1.798956\n",
            "Epoch 6, Loss: 1.675955\n",
            "Epoch 7, Loss: 1.615726\n",
            "Epoch 8, Loss: 1.519861\n",
            "Epoch 9, Loss: 1.383787\n",
            "Epoch 10, Loss: 1.359717\n",
            "Epoch 11, Loss: 1.245001\n",
            "Epoch 12, Loss: 1.191986\n",
            "Epoch 13, Loss: 1.060480\n",
            "Epoch 14, Loss: 1.015930\n",
            "Epoch 15, Loss: 0.960390\n",
            "Epoch 16, Loss: 0.855240\n",
            "Epoch 17, Loss: 0.778010\n",
            "Epoch 18, Loss: 0.753943\n",
            "Epoch 19, Loss: 0.715205\n",
            "Epoch 20, Loss: 0.643162\n",
            "Shadow model 36 trained.\n",
            "Epoch 1, Loss: 2.247874\n",
            "Epoch 2, Loss: 2.121463\n",
            "Epoch 3, Loss: 1.972496\n",
            "Epoch 4, Loss: 1.882829\n",
            "Epoch 5, Loss: 1.777700\n",
            "Epoch 6, Loss: 1.665258\n",
            "Epoch 7, Loss: 1.609995\n",
            "Epoch 8, Loss: 1.471931\n",
            "Epoch 9, Loss: 1.341384\n",
            "Epoch 10, Loss: 1.342017\n",
            "Epoch 11, Loss: 1.184559\n",
            "Epoch 12, Loss: 1.163310\n",
            "Epoch 13, Loss: 1.112994\n",
            "Epoch 14, Loss: 1.031924\n",
            "Epoch 15, Loss: 1.003843\n",
            "Epoch 16, Loss: 0.929439\n",
            "Epoch 17, Loss: 0.873734\n",
            "Epoch 18, Loss: 0.915943\n",
            "Epoch 19, Loss: 0.807459\n",
            "Epoch 20, Loss: 0.708497\n",
            "Shadow model 37 trained.\n",
            "Epoch 1, Loss: 2.249110\n",
            "Epoch 2, Loss: 2.135750\n",
            "Epoch 3, Loss: 2.004336\n",
            "Epoch 4, Loss: 1.914317\n",
            "Epoch 5, Loss: 1.849123\n",
            "Epoch 6, Loss: 1.692255\n",
            "Epoch 7, Loss: 1.628739\n",
            "Epoch 8, Loss: 1.574034\n",
            "Epoch 9, Loss: 1.464755\n",
            "Epoch 10, Loss: 1.369478\n",
            "Epoch 11, Loss: 1.316362\n",
            "Epoch 12, Loss: 1.235705\n",
            "Epoch 13, Loss: 1.158290\n",
            "Epoch 14, Loss: 1.007989\n",
            "Epoch 15, Loss: 1.053385\n",
            "Epoch 16, Loss: 0.957658\n",
            "Epoch 17, Loss: 0.959942\n",
            "Epoch 18, Loss: 0.888953\n",
            "Epoch 19, Loss: 0.860948\n",
            "Epoch 20, Loss: 0.731864\n",
            "Shadow model 38 trained.\n",
            "Epoch 1, Loss: 2.299735\n",
            "Epoch 2, Loss: 2.198687\n",
            "Epoch 3, Loss: 2.103143\n",
            "Epoch 4, Loss: 2.004759\n",
            "Epoch 5, Loss: 1.949743\n",
            "Epoch 6, Loss: 1.857276\n",
            "Epoch 7, Loss: 1.691926\n",
            "Epoch 8, Loss: 1.637570\n",
            "Epoch 9, Loss: 1.533260\n",
            "Epoch 10, Loss: 1.418216\n",
            "Epoch 11, Loss: 1.326196\n",
            "Epoch 12, Loss: 1.276066\n",
            "Epoch 13, Loss: 1.195078\n",
            "Epoch 14, Loss: 1.030078\n",
            "Epoch 15, Loss: 1.017403\n",
            "Epoch 16, Loss: 0.955370\n",
            "Epoch 17, Loss: 0.867039\n",
            "Epoch 18, Loss: 0.869978\n",
            "Epoch 19, Loss: 0.787984\n",
            "Epoch 20, Loss: 0.746699\n",
            "Shadow model 39 trained.\n",
            "Epoch 1, Loss: 2.267233\n",
            "Epoch 2, Loss: 2.099190\n",
            "Epoch 3, Loss: 1.983313\n",
            "Epoch 4, Loss: 1.882068\n",
            "Epoch 5, Loss: 1.780799\n",
            "Epoch 6, Loss: 1.694853\n",
            "Epoch 7, Loss: 1.609980\n",
            "Epoch 8, Loss: 1.484104\n",
            "Epoch 9, Loss: 1.465803\n",
            "Epoch 10, Loss: 1.363530\n",
            "Epoch 11, Loss: 1.223555\n",
            "Epoch 12, Loss: 1.163909\n",
            "Epoch 13, Loss: 1.089143\n",
            "Epoch 14, Loss: 1.054851\n",
            "Epoch 15, Loss: 1.001458\n",
            "Epoch 16, Loss: 0.884040\n",
            "Epoch 17, Loss: 0.832767\n",
            "Epoch 18, Loss: 0.847338\n",
            "Epoch 19, Loss: 0.754477\n",
            "Epoch 20, Loss: 0.691859\n",
            "Shadow model 40 trained.\n",
            "Epoch 1, Loss: 2.287215\n",
            "Epoch 2, Loss: 2.142681\n",
            "Epoch 3, Loss: 2.007658\n",
            "Epoch 4, Loss: 1.913925\n",
            "Epoch 5, Loss: 1.755810\n",
            "Epoch 6, Loss: 1.632415\n",
            "Epoch 7, Loss: 1.570472\n",
            "Epoch 8, Loss: 1.469049\n",
            "Epoch 9, Loss: 1.395089\n",
            "Epoch 10, Loss: 1.249489\n",
            "Epoch 11, Loss: 1.217288\n",
            "Epoch 12, Loss: 1.101729\n",
            "Epoch 13, Loss: 1.024997\n",
            "Epoch 14, Loss: 0.890055\n",
            "Epoch 15, Loss: 0.897961\n",
            "Epoch 16, Loss: 0.809775\n",
            "Epoch 17, Loss: 0.771465\n",
            "Epoch 18, Loss: 0.703735\n",
            "Epoch 19, Loss: 0.673822\n",
            "Epoch 20, Loss: 0.640797\n",
            "Shadow model 41 trained.\n",
            "Epoch 1, Loss: 2.282562\n",
            "Epoch 2, Loss: 2.156204\n",
            "Epoch 3, Loss: 2.046789\n",
            "Epoch 4, Loss: 1.970939\n",
            "Epoch 5, Loss: 1.890281\n",
            "Epoch 6, Loss: 1.774294\n",
            "Epoch 7, Loss: 1.699090\n",
            "Epoch 8, Loss: 1.583697\n",
            "Epoch 9, Loss: 1.548912\n",
            "Epoch 10, Loss: 1.464809\n",
            "Epoch 11, Loss: 1.397821\n",
            "Epoch 12, Loss: 1.330317\n",
            "Epoch 13, Loss: 1.260930\n",
            "Epoch 14, Loss: 1.174150\n",
            "Epoch 15, Loss: 1.140322\n",
            "Epoch 16, Loss: 1.042802\n",
            "Epoch 17, Loss: 0.982867\n",
            "Epoch 18, Loss: 0.917424\n",
            "Epoch 19, Loss: 0.896964\n",
            "Epoch 20, Loss: 0.828169\n",
            "Shadow model 42 trained.\n",
            "Epoch 1, Loss: 2.283296\n",
            "Epoch 2, Loss: 2.148406\n",
            "Epoch 3, Loss: 2.016573\n",
            "Epoch 4, Loss: 1.924172\n",
            "Epoch 5, Loss: 1.844532\n",
            "Epoch 6, Loss: 1.748437\n",
            "Epoch 7, Loss: 1.647102\n",
            "Epoch 8, Loss: 1.559615\n",
            "Epoch 9, Loss: 1.443984\n",
            "Epoch 10, Loss: 1.357576\n",
            "Epoch 11, Loss: 1.253314\n",
            "Epoch 12, Loss: 1.157310\n",
            "Epoch 13, Loss: 1.143709\n",
            "Epoch 14, Loss: 1.083936\n",
            "Epoch 15, Loss: 1.005279\n",
            "Epoch 16, Loss: 0.914261\n",
            "Epoch 17, Loss: 0.900026\n",
            "Epoch 18, Loss: 0.859532\n",
            "Epoch 19, Loss: 0.775219\n",
            "Epoch 20, Loss: 0.750188\n",
            "Shadow model 43 trained.\n",
            "Epoch 1, Loss: 2.285039\n",
            "Epoch 2, Loss: 2.153570\n",
            "Epoch 3, Loss: 2.044765\n",
            "Epoch 4, Loss: 1.944265\n",
            "Epoch 5, Loss: 1.831493\n",
            "Epoch 6, Loss: 1.836820\n",
            "Epoch 7, Loss: 1.732568\n",
            "Epoch 8, Loss: 1.688750\n",
            "Epoch 9, Loss: 1.498682\n",
            "Epoch 10, Loss: 1.509714\n",
            "Epoch 11, Loss: 1.410102\n",
            "Epoch 12, Loss: 1.394185\n",
            "Epoch 13, Loss: 1.258040\n",
            "Epoch 14, Loss: 1.233435\n",
            "Epoch 15, Loss: 1.206719\n",
            "Epoch 16, Loss: 1.086142\n",
            "Epoch 17, Loss: 1.051753\n",
            "Epoch 18, Loss: 0.954847\n",
            "Epoch 19, Loss: 0.912049\n",
            "Epoch 20, Loss: 0.895112\n",
            "Shadow model 44 trained.\n",
            "Epoch 1, Loss: 2.306059\n",
            "Epoch 2, Loss: 2.218251\n",
            "Epoch 3, Loss: 2.111349\n",
            "Epoch 4, Loss: 2.062437\n",
            "Epoch 5, Loss: 1.967922\n",
            "Epoch 6, Loss: 1.904366\n",
            "Epoch 7, Loss: 1.787124\n",
            "Epoch 8, Loss: 1.684046\n",
            "Epoch 9, Loss: 1.586830\n",
            "Epoch 10, Loss: 1.477475\n",
            "Epoch 11, Loss: 1.446155\n",
            "Epoch 12, Loss: 1.386707\n",
            "Epoch 13, Loss: 1.347865\n",
            "Epoch 14, Loss: 1.290413\n",
            "Epoch 15, Loss: 1.199780\n",
            "Epoch 16, Loss: 1.158907\n",
            "Epoch 17, Loss: 1.033980\n",
            "Epoch 18, Loss: 1.036119\n",
            "Epoch 19, Loss: 1.013207\n",
            "Epoch 20, Loss: 0.926201\n",
            "Shadow model 45 trained.\n",
            "Epoch 1, Loss: 2.301298\n",
            "Epoch 2, Loss: 2.188998\n",
            "Epoch 3, Loss: 2.102291\n",
            "Epoch 4, Loss: 2.015389\n",
            "Epoch 5, Loss: 1.958042\n",
            "Epoch 6, Loss: 1.907271\n",
            "Epoch 7, Loss: 1.821410\n",
            "Epoch 8, Loss: 1.714548\n",
            "Epoch 9, Loss: 1.616600\n",
            "Epoch 10, Loss: 1.536593\n",
            "Epoch 11, Loss: 1.518168\n",
            "Epoch 12, Loss: 1.425450\n",
            "Epoch 13, Loss: 1.295474\n",
            "Epoch 14, Loss: 1.228334\n",
            "Epoch 15, Loss: 1.190396\n",
            "Epoch 16, Loss: 1.122600\n",
            "Epoch 17, Loss: 1.078026\n",
            "Epoch 18, Loss: 0.986726\n",
            "Epoch 19, Loss: 0.944158\n",
            "Epoch 20, Loss: 0.896022\n",
            "Shadow model 46 trained.\n",
            "Epoch 1, Loss: 2.295406\n",
            "Epoch 2, Loss: 2.132232\n",
            "Epoch 3, Loss: 2.039448\n",
            "Epoch 4, Loss: 1.899019\n",
            "Epoch 5, Loss: 1.835687\n",
            "Epoch 6, Loss: 1.714768\n",
            "Epoch 7, Loss: 1.606365\n",
            "Epoch 8, Loss: 1.463328\n",
            "Epoch 9, Loss: 1.399305\n",
            "Epoch 10, Loss: 1.298597\n",
            "Epoch 11, Loss: 1.172095\n",
            "Epoch 12, Loss: 1.095354\n",
            "Epoch 13, Loss: 1.024818\n",
            "Epoch 14, Loss: 1.004231\n",
            "Epoch 15, Loss: 0.897818\n",
            "Epoch 16, Loss: 0.832396\n",
            "Epoch 17, Loss: 0.810159\n",
            "Epoch 18, Loss: 0.745777\n",
            "Epoch 19, Loss: 0.639374\n",
            "Epoch 20, Loss: 0.621100\n",
            "Shadow model 47 trained.\n",
            "Epoch 1, Loss: 2.283787\n",
            "Epoch 2, Loss: 2.169653\n",
            "Epoch 3, Loss: 2.020294\n",
            "Epoch 4, Loss: 1.993295\n",
            "Epoch 5, Loss: 1.914742\n",
            "Epoch 6, Loss: 1.805286\n",
            "Epoch 7, Loss: 1.729495\n",
            "Epoch 8, Loss: 1.648698\n",
            "Epoch 9, Loss: 1.492503\n",
            "Epoch 10, Loss: 1.440142\n",
            "Epoch 11, Loss: 1.392712\n",
            "Epoch 12, Loss: 1.343817\n",
            "Epoch 13, Loss: 1.227222\n",
            "Epoch 14, Loss: 1.174806\n",
            "Epoch 15, Loss: 1.103928\n",
            "Epoch 16, Loss: 1.070985\n",
            "Epoch 17, Loss: 0.981758\n",
            "Epoch 18, Loss: 0.899701\n",
            "Epoch 19, Loss: 0.941365\n",
            "Epoch 20, Loss: 0.841487\n",
            "Shadow model 48 trained.\n",
            "Epoch 1, Loss: 2.270392\n",
            "Epoch 2, Loss: 2.115033\n",
            "Epoch 3, Loss: 2.013815\n",
            "Epoch 4, Loss: 1.901637\n",
            "Epoch 5, Loss: 1.798571\n",
            "Epoch 6, Loss: 1.654072\n",
            "Epoch 7, Loss: 1.545713\n",
            "Epoch 8, Loss: 1.471353\n",
            "Epoch 9, Loss: 1.338433\n",
            "Epoch 10, Loss: 1.260764\n",
            "Epoch 11, Loss: 1.251024\n",
            "Epoch 12, Loss: 1.112458\n",
            "Epoch 13, Loss: 1.009792\n",
            "Epoch 14, Loss: 0.937467\n",
            "Epoch 15, Loss: 0.861708\n",
            "Epoch 16, Loss: 0.780757\n",
            "Epoch 17, Loss: 0.760893\n",
            "Epoch 18, Loss: 0.702702\n",
            "Epoch 19, Loss: 0.673470\n",
            "Epoch 20, Loss: 0.611200\n",
            "Shadow model 49 trained.\n",
            "Epoch 1, Loss: 2.296800\n",
            "Epoch 2, Loss: 2.143664\n",
            "Epoch 3, Loss: 2.043491\n",
            "Epoch 4, Loss: 1.995521\n",
            "Epoch 5, Loss: 1.906836\n",
            "Epoch 6, Loss: 1.814184\n",
            "Epoch 7, Loss: 1.724091\n",
            "Epoch 8, Loss: 1.656717\n",
            "Epoch 9, Loss: 1.578468\n",
            "Epoch 10, Loss: 1.445276\n",
            "Epoch 11, Loss: 1.428086\n",
            "Epoch 12, Loss: 1.371531\n",
            "Epoch 13, Loss: 1.257948\n",
            "Epoch 14, Loss: 1.169512\n",
            "Epoch 15, Loss: 1.125246\n",
            "Epoch 16, Loss: 1.035281\n",
            "Epoch 17, Loss: 0.969912\n",
            "Epoch 18, Loss: 0.983963\n",
            "Epoch 19, Loss: 0.878639\n",
            "Epoch 20, Loss: 0.799906\n",
            "Shadow model 50 trained.\n",
            "Class 0, Training Accuracy: 0.9998\n",
            "Class 0, Training Precision: 0.9998\n",
            "Class 0, Training Recall: 1.0000\n",
            "Class 0, Training F1 Score: 0.9999\n",
            "Class 0, Training Confusion Matrix:\n",
            " [[1952    1]\n",
            " [   0 4047]]\n",
            "Attack model for class 0 trained.\n",
            "Class 1, Training Accuracy: 0.9998\n",
            "Class 1, Training Precision: 1.0000\n",
            "Class 1, Training Recall: 0.9997\n",
            "Class 1, Training F1 Score: 0.9999\n",
            "Class 1, Training Confusion Matrix:\n",
            " [[2036    0]\n",
            " [   1 3963]]\n",
            "Attack model for class 1 trained.\n",
            "Class 2, Training Accuracy: 0.9998\n",
            "Class 2, Training Precision: 0.9997\n",
            "Class 2, Training Recall: 1.0000\n",
            "Class 2, Training F1 Score: 0.9999\n",
            "Class 2, Training Confusion Matrix:\n",
            " [[2021    1]\n",
            " [   0 3978]]\n",
            "Attack model for class 2 trained.\n",
            "Class 3, Training Accuracy: 0.9993\n",
            "Class 3, Training Precision: 0.9993\n",
            "Class 3, Training Recall: 0.9998\n",
            "Class 3, Training F1 Score: 0.9995\n",
            "Class 3, Training Confusion Matrix:\n",
            " [[1989    3]\n",
            " [   1 4007]]\n",
            "Attack model for class 3 trained.\n",
            "Class 4, Training Accuracy: 0.9995\n",
            "Class 4, Training Precision: 0.9992\n",
            "Class 4, Training Recall: 1.0000\n",
            "Class 4, Training F1 Score: 0.9996\n",
            "Class 4, Training Confusion Matrix:\n",
            " [[2034    3]\n",
            " [   0 3963]]\n",
            "Attack model for class 4 trained.\n",
            "Class 5, Training Accuracy: 0.9997\n",
            "Class 5, Training Precision: 0.9995\n",
            "Class 5, Training Recall: 1.0000\n",
            "Class 5, Training F1 Score: 0.9998\n",
            "Class 5, Training Confusion Matrix:\n",
            " [[1997    2]\n",
            " [   0 4001]]\n",
            "Attack model for class 5 trained.\n",
            "Class 6, Training Accuracy: 0.9995\n",
            "Class 6, Training Precision: 0.9998\n",
            "Class 6, Training Recall: 0.9995\n",
            "Class 6, Training F1 Score: 0.9996\n",
            "Class 6, Training Confusion Matrix:\n",
            " [[1975    1]\n",
            " [   2 4022]]\n",
            "Attack model for class 6 trained.\n",
            "Class 7, Training Accuracy: 1.0000\n",
            "Class 7, Training Precision: 1.0000\n",
            "Class 7, Training Recall: 1.0000\n",
            "Class 7, Training F1 Score: 1.0000\n",
            "Class 7, Training Confusion Matrix:\n",
            " [[1996    0]\n",
            " [   0 4004]]\n",
            "Attack model for class 7 trained.\n",
            "Class 8, Training Accuracy: 0.9997\n",
            "Class 8, Training Precision: 0.9995\n",
            "Class 8, Training Recall: 1.0000\n",
            "Class 8, Training F1 Score: 0.9998\n",
            "Class 8, Training Confusion Matrix:\n",
            " [[1985    2]\n",
            " [   0 4013]]\n",
            "Attack model for class 8 trained.\n",
            "Class 9, Training Accuracy: 0.9998\n",
            "Class 9, Training Precision: 0.9997\n",
            "Class 9, Training Recall: 1.0000\n",
            "Class 9, Training F1 Score: 0.9999\n",
            "Class 9, Training Confusion Matrix:\n",
            " [[2001    1]\n",
            " [   0 3998]]\n",
            "Attack model for class 9 trained.\n",
            "Attack model for class 0 saved to attack_model_class_0.joblib\n",
            "Attack model for class 1 saved to attack_model_class_1.joblib\n",
            "Attack model for class 2 saved to attack_model_class_2.joblib\n",
            "Attack model for class 3 saved to attack_model_class_3.joblib\n",
            "Attack model for class 4 saved to attack_model_class_4.joblib\n",
            "Attack model for class 5 saved to attack_model_class_5.joblib\n",
            "Attack model for class 6 saved to attack_model_class_6.joblib\n",
            "Attack model for class 7 saved to attack_model_class_7.joblib\n",
            "Attack model for class 8 saved to attack_model_class_8.joblib\n",
            "Attack model for class 9 saved to attack_model_class_9.joblib\n",
            "Attack model for class 0 loaded from attack_model_class_0.joblib\n",
            "Attack model for class 1 loaded from attack_model_class_1.joblib\n",
            "Attack model for class 2 loaded from attack_model_class_2.joblib\n",
            "Attack model for class 3 loaded from attack_model_class_3.joblib\n",
            "Attack model for class 4 loaded from attack_model_class_4.joblib\n",
            "Attack model for class 5 loaded from attack_model_class_5.joblib\n",
            "Attack model for class 6 loaded from attack_model_class_6.joblib\n",
            "Attack model for class 7 loaded from attack_model_class_7.joblib\n",
            "Attack model for class 8 loaded from attack_model_class_8.joblib\n",
            "Attack model for class 9 loaded from attack_model_class_9.joblib\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Accuracies"
      ],
      "metadata": {
        "id": "gBb6gIaEbZp_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_base = CIFAR10Classifier()\n",
        "model_base.load_state_dict(torch.load('baseline_model.pth', map_location=device))\n",
        "model_base.to(device)\n",
        "\n",
        "accuracy_baseline = mia.evaluate_attack_model(train_loader_baseline, test_loader_baseline, model_base)\n",
        "print(f'Accuracy for Attacking to the Baseline Model :  {accuracy_baseline * 100:.2f}%')"
      ],
      "metadata": {
        "id": "oCyCn_vtbUYy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff86f55e-68e3-4ed5-9946-95b023f19ce7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for Attacking to the Baseline Model :  62.42%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_module_prefix(state_dict):\n",
        "    new_state_dict = {}\n",
        "    for k, v in state_dict.items():\n",
        "        if k.startswith('_module.'):\n",
        "            new_state_dict[k[8:]] = v  # remove '_module.' prefix\n",
        "        else:\n",
        "            new_state_dict[k] = v\n",
        "    return new_state_dict\n",
        "\n",
        "model_private = CIFAR10Classifier().to(device)\n",
        "private_state_dict = torch.load('modified_model.pth', map_location=device)\n",
        "model_private.load_state_dict(remove_module_prefix(private_state_dict))\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model_private.to(device)\n",
        "\n",
        "accuracy_private = mia.evaluate_attack_model(train_loader_modified, test_loader_modified, model_private)\n",
        "print(f'Accuracy for Attacking to the Private Model :  {accuracy_private * 100:.2f}%')"
      ],
      "metadata": {
        "id": "ssjrF1aNbZAL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47dc63be-6649-41ae-877d-ec790b97128a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for Attacking to the Private Model :  45.89%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Num of Shadow Models = 5 , Batch Size = 64 , Epochs = 20\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TS6131hO9z1m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
        "train_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Split training data into 80% and 20%\n",
        "train_size = int(0.8 * len(train_set))\n",
        "remaining_size = len(train_set) - train_size\n",
        "train_subset, remaining_subset = torch.utils.data.random_split(train_set, [train_size, remaining_size])\n",
        "\n",
        "# Create disjoint loaders for seen data from 80% training data\n",
        "num_shadow_models = 5\n",
        "seen_size_per_model = train_size // num_shadow_models\n",
        "seen_loaders = []\n",
        "\n",
        "for i in range(num_shadow_models):\n",
        "    start_idx = i * seen_size_per_model\n",
        "    end_idx = (i + 1) * seen_size_per_model\n",
        "    seen_indices = torch.arange(start_idx, end_idx)\n",
        "    seen_train_set = Subset(train_subset, seen_indices)\n",
        "    seen_loader = DataLoader(seen_train_set, batch_size=64, shuffle=True)\n",
        "    seen_loaders.append(seen_loader)\n",
        "\n",
        "# Create concatenated unseen data from the remaining 20% of training data and the entire test set\n",
        "unseen_dataset = ConcatDataset([remaining_subset, test_set])\n",
        "unseen_size_per_model = len(unseen_dataset) // num_shadow_models\n",
        "unseen_loaders = []\n",
        "\n",
        "for i in range(num_shadow_models):\n",
        "    start_idx = i * unseen_size_per_model\n",
        "    end_idx = (i + 1) * unseen_size_per_model\n",
        "    unseen_indices = torch.arange(start_idx, end_idx)\n",
        "    unseen_subset = Subset(unseen_dataset, unseen_indices)\n",
        "    unseen_loader = DataLoader(unseen_subset, batch_size=64, shuffle=False)\n",
        "    unseen_loaders.append(unseen_loader)\n",
        "\n",
        "test_loader = DataLoader(test_set, batch_size=64, shuffle=False)\n",
        "\n",
        "# Initialize MembershipInferenceAttackNoPrivacy\n",
        "mia = MembershipInferenceAttackNoPrivacy(CIFAR10Classifier, device)\n",
        "\n",
        "# Train shadow models without differential privacy\n",
        "mia.train_shadow_models(seen_loaders, num_epochs=10)\n",
        "\n",
        "# Collect outputs for attack model\n",
        "mia.collect_outputs(seen_loaders, unseen_loaders)\n",
        "\n",
        "# Train attack models\n",
        "mia.train_attack_models()\n",
        "\n",
        "# Save the attack models\n",
        "mia.save_attack_models('attack_model')\n",
        "\n",
        "# Load the attack models (for future use)\n",
        "mia.load_attack_models('attack_model')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPEdxFyw98hu",
        "outputId": "cf67a921-1c9d-42dc-b203-f4bba8430f68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch 1, Loss: 2.066963\n",
            "Epoch 2, Loss: 1.770589\n",
            "Epoch 3, Loss: 1.658656\n",
            "Epoch 4, Loss: 1.564891\n",
            "Epoch 5, Loss: 1.518469\n",
            "Epoch 6, Loss: 1.469298\n",
            "Epoch 7, Loss: 1.408042\n",
            "Epoch 8, Loss: 1.345974\n",
            "Epoch 9, Loss: 1.290851\n",
            "Epoch 10, Loss: 1.272630\n",
            "Shadow model 1 trained.\n",
            "Epoch 1, Loss: 2.029394\n",
            "Epoch 2, Loss: 1.748148\n",
            "Epoch 3, Loss: 1.617435\n",
            "Epoch 4, Loss: 1.516840\n",
            "Epoch 5, Loss: 1.457594\n",
            "Epoch 6, Loss: 1.386896\n",
            "Epoch 7, Loss: 1.325492\n",
            "Epoch 8, Loss: 1.268936\n",
            "Epoch 9, Loss: 1.223062\n",
            "Epoch 10, Loss: 1.175475\n",
            "Shadow model 2 trained.\n",
            "Epoch 1, Loss: 2.031841\n",
            "Epoch 2, Loss: 1.746687\n",
            "Epoch 3, Loss: 1.624366\n",
            "Epoch 4, Loss: 1.553309\n",
            "Epoch 5, Loss: 1.475473\n",
            "Epoch 6, Loss: 1.413871\n",
            "Epoch 7, Loss: 1.354392\n",
            "Epoch 8, Loss: 1.313959\n",
            "Epoch 9, Loss: 1.266630\n",
            "Epoch 10, Loss: 1.223101\n",
            "Shadow model 3 trained.\n",
            "Epoch 1, Loss: 2.068920\n",
            "Epoch 2, Loss: 1.783126\n",
            "Epoch 3, Loss: 1.637932\n",
            "Epoch 4, Loss: 1.534663\n",
            "Epoch 5, Loss: 1.478650\n",
            "Epoch 6, Loss: 1.393191\n",
            "Epoch 7, Loss: 1.347546\n",
            "Epoch 8, Loss: 1.318271\n",
            "Epoch 9, Loss: 1.254730\n",
            "Epoch 10, Loss: 1.184641\n",
            "Shadow model 4 trained.\n",
            "Epoch 1, Loss: 2.068176\n",
            "Epoch 2, Loss: 1.761016\n",
            "Epoch 3, Loss: 1.620246\n",
            "Epoch 4, Loss: 1.534467\n",
            "Epoch 5, Loss: 1.459533\n",
            "Epoch 6, Loss: 1.402055\n",
            "Epoch 7, Loss: 1.348388\n",
            "Epoch 8, Loss: 1.277864\n",
            "Epoch 9, Loss: 1.228350\n",
            "Epoch 10, Loss: 1.197483\n",
            "Shadow model 5 trained.\n",
            "Class 0, Training Accuracy: 0.9998\n",
            "Class 0, Training Precision: 0.9998\n",
            "Class 0, Training Recall: 1.0000\n",
            "Class 0, Training F1 Score: 0.9999\n",
            "Class 0, Training Confusion Matrix:\n",
            " [[1999    1]\n",
            " [   0 4000]]\n",
            "Attack model for class 0 trained.\n",
            "Class 1, Training Accuracy: 1.0000\n",
            "Class 1, Training Precision: 1.0000\n",
            "Class 1, Training Recall: 1.0000\n",
            "Class 1, Training F1 Score: 1.0000\n",
            "Class 1, Training Confusion Matrix:\n",
            " [[2040    0]\n",
            " [   0 3960]]\n",
            "Attack model for class 1 trained.\n",
            "Class 2, Training Accuracy: 0.9998\n",
            "Class 2, Training Precision: 0.9997\n",
            "Class 2, Training Recall: 1.0000\n",
            "Class 2, Training F1 Score: 0.9999\n",
            "Class 2, Training Confusion Matrix:\n",
            " [[2017    1]\n",
            " [   0 3982]]\n",
            "Attack model for class 2 trained.\n",
            "Class 3, Training Accuracy: 0.9997\n",
            "Class 3, Training Precision: 0.9995\n",
            "Class 3, Training Recall: 1.0000\n",
            "Class 3, Training F1 Score: 0.9998\n",
            "Class 3, Training Confusion Matrix:\n",
            " [[1974    2]\n",
            " [   0 4024]]\n",
            "Attack model for class 3 trained.\n",
            "Class 4, Training Accuracy: 1.0000\n",
            "Class 4, Training Precision: 1.0000\n",
            "Class 4, Training Recall: 1.0000\n",
            "Class 4, Training F1 Score: 1.0000\n",
            "Class 4, Training Confusion Matrix:\n",
            " [[2024    0]\n",
            " [   0 3976]]\n",
            "Attack model for class 4 trained.\n",
            "Class 5, Training Accuracy: 1.0000\n",
            "Class 5, Training Precision: 1.0000\n",
            "Class 5, Training Recall: 1.0000\n",
            "Class 5, Training F1 Score: 1.0000\n",
            "Class 5, Training Confusion Matrix:\n",
            " [[1947    0]\n",
            " [   0 4053]]\n",
            "Attack model for class 5 trained.\n",
            "Class 6, Training Accuracy: 1.0000\n",
            "Class 6, Training Precision: 1.0000\n",
            "Class 6, Training Recall: 1.0000\n",
            "Class 6, Training F1 Score: 1.0000\n",
            "Class 6, Training Confusion Matrix:\n",
            " [[2017    0]\n",
            " [   0 3983]]\n",
            "Attack model for class 6 trained.\n",
            "Class 7, Training Accuracy: 1.0000\n",
            "Class 7, Training Precision: 1.0000\n",
            "Class 7, Training Recall: 1.0000\n",
            "Class 7, Training F1 Score: 1.0000\n",
            "Class 7, Training Confusion Matrix:\n",
            " [[1999    0]\n",
            " [   0 4001]]\n",
            "Attack model for class 7 trained.\n",
            "Class 8, Training Accuracy: 1.0000\n",
            "Class 8, Training Precision: 1.0000\n",
            "Class 8, Training Recall: 1.0000\n",
            "Class 8, Training F1 Score: 1.0000\n",
            "Class 8, Training Confusion Matrix:\n",
            " [[1993    0]\n",
            " [   0 4007]]\n",
            "Attack model for class 8 trained.\n",
            "Class 9, Training Accuracy: 0.9997\n",
            "Class 9, Training Precision: 0.9995\n",
            "Class 9, Training Recall: 1.0000\n",
            "Class 9, Training F1 Score: 0.9998\n",
            "Class 9, Training Confusion Matrix:\n",
            " [[1984    2]\n",
            " [   0 4014]]\n",
            "Attack model for class 9 trained.\n",
            "Attack model for class 0 saved to attack_model_class_0.joblib\n",
            "Attack model for class 1 saved to attack_model_class_1.joblib\n",
            "Attack model for class 2 saved to attack_model_class_2.joblib\n",
            "Attack model for class 3 saved to attack_model_class_3.joblib\n",
            "Attack model for class 4 saved to attack_model_class_4.joblib\n",
            "Attack model for class 5 saved to attack_model_class_5.joblib\n",
            "Attack model for class 6 saved to attack_model_class_6.joblib\n",
            "Attack model for class 7 saved to attack_model_class_7.joblib\n",
            "Attack model for class 8 saved to attack_model_class_8.joblib\n",
            "Attack model for class 9 saved to attack_model_class_9.joblib\n",
            "Attack model for class 0 loaded from attack_model_class_0.joblib\n",
            "Attack model for class 1 loaded from attack_model_class_1.joblib\n",
            "Attack model for class 2 loaded from attack_model_class_2.joblib\n",
            "Attack model for class 3 loaded from attack_model_class_3.joblib\n",
            "Attack model for class 4 loaded from attack_model_class_4.joblib\n",
            "Attack model for class 5 loaded from attack_model_class_5.joblib\n",
            "Attack model for class 6 loaded from attack_model_class_6.joblib\n",
            "Attack model for class 7 loaded from attack_model_class_7.joblib\n",
            "Attack model for class 8 loaded from attack_model_class_8.joblib\n",
            "Attack model for class 9 loaded from attack_model_class_9.joblib\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Accuracies"
      ],
      "metadata": {
        "id": "3N4fduNR-FrZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_base = CIFAR10Classifier()\n",
        "model_base.load_state_dict(torch.load('baseline_model.pth', map_location=device))\n",
        "model_base.to(device)\n",
        "\n",
        "accuracy_baseline = mia.evaluate_attack_model(train_loader_baseline, test_loader_baseline, model_base)\n",
        "print(f'Accuracy for Attacking to the Baseline Model :  {accuracy_baseline * 100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XTNCHPB-Y42",
        "outputId": "dcf6b7a0-f8a9-44f0-eb77-bb6ae2467906"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for Attacking to the Baseline Model :  74.79%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_module_prefix(state_dict):\n",
        "    new_state_dict = {}\n",
        "    for k, v in state_dict.items():\n",
        "        if k.startswith('_module.'):\n",
        "            new_state_dict[k[8:]] = v  # remove '_module.' prefix\n",
        "        else:\n",
        "            new_state_dict[k] = v\n",
        "    return new_state_dict\n",
        "\n",
        "model_private = CIFAR10Classifier().to(device)\n",
        "private_state_dict = torch.load('modified_model.pth', map_location=device)\n",
        "model_private.load_state_dict(remove_module_prefix(private_state_dict))\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model_private.to(device)\n",
        "\n",
        "accuracy_private = mia.evaluate_attack_model(train_loader_modified, test_loader_modified, model_private)\n",
        "print(f'Accuracy for Attacking to the Private Model :  {accuracy_private * 100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDsZd1kZ-bOE",
        "outputId": "6e80d381-cf0f-4e2e-967d-97e6dd400e1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for Attacking to the Private Model :  60.89%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Num of Shadow Models = 3 , Batch Size = 64 , Epochs = 20"
      ],
      "metadata": {
        "id": "-JMa2RgGB3Wt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
        "train_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Split training data into 80% and 20%\n",
        "train_size = int(0.8 * len(train_set))\n",
        "remaining_size = len(train_set) - train_size\n",
        "train_subset, remaining_subset = torch.utils.data.random_split(train_set, [train_size, remaining_size])\n",
        "\n",
        "# Create disjoint loaders for seen data from 80% training data\n",
        "num_shadow_models = 3\n",
        "seen_size_per_model = train_size // num_shadow_models\n",
        "seen_loaders = []\n",
        "\n",
        "for i in range(num_shadow_models):\n",
        "    start_idx = i * seen_size_per_model\n",
        "    end_idx = (i + 1) * seen_size_per_model\n",
        "    seen_indices = torch.arange(start_idx, end_idx)\n",
        "    seen_train_set = Subset(train_subset, seen_indices)\n",
        "    seen_loader = DataLoader(seen_train_set, batch_size=64, shuffle=True)\n",
        "    seen_loaders.append(seen_loader)\n",
        "\n",
        "# Create concatenated unseen data from the remaining 20% of training data and the entire test set\n",
        "unseen_dataset = ConcatDataset([remaining_subset, test_set])\n",
        "unseen_size_per_model = len(unseen_dataset) // num_shadow_models\n",
        "unseen_loaders = []\n",
        "\n",
        "for i in range(num_shadow_models):\n",
        "    start_idx = i * unseen_size_per_model\n",
        "    end_idx = (i + 1) * unseen_size_per_model\n",
        "    unseen_indices = torch.arange(start_idx, end_idx)\n",
        "    unseen_subset = Subset(unseen_dataset, unseen_indices)\n",
        "    unseen_loader = DataLoader(unseen_subset, batch_size=64, shuffle=False)\n",
        "    unseen_loaders.append(unseen_loader)\n",
        "\n",
        "test_loader = DataLoader(test_set, batch_size=64, shuffle=False)\n",
        "\n",
        "# Initialize MembershipInferenceAttackNoPrivacy\n",
        "mia = MembershipInferenceAttackNoPrivacy(CIFAR10Classifier, device)\n",
        "\n",
        "# Train shadow models without differential privacy\n",
        "mia.train_shadow_models(seen_loaders, num_epochs=20)\n",
        "\n",
        "# Collect outputs for attack model\n",
        "mia.collect_outputs(seen_loaders, unseen_loaders)\n",
        "\n",
        "# Train attack models\n",
        "mia.train_attack_models()\n",
        "\n",
        "# Save the attack models\n",
        "mia.save_attack_models('attack_model')\n",
        "\n",
        "# Load the attack models (for future use)\n",
        "mia.load_attack_models('attack_model')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8I2mDK58B3mR",
        "outputId": "3aae5c57-7111-4db3-9a01-3512030551da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch 1, Loss: 1.962627\n",
            "Epoch 2, Loss: 1.683025\n",
            "Epoch 3, Loss: 1.571063\n",
            "Epoch 4, Loss: 1.476093\n",
            "Epoch 5, Loss: 1.411074\n",
            "Epoch 6, Loss: 1.352118\n",
            "Epoch 7, Loss: 1.300093\n",
            "Epoch 8, Loss: 1.237202\n",
            "Epoch 9, Loss: 1.189871\n",
            "Epoch 10, Loss: 1.153263\n",
            "Epoch 11, Loss: 1.093561\n",
            "Epoch 12, Loss: 1.060777\n",
            "Epoch 13, Loss: 1.043026\n",
            "Epoch 14, Loss: 1.002706\n",
            "Epoch 15, Loss: 0.974497\n",
            "Epoch 16, Loss: 0.935686\n",
            "Epoch 17, Loss: 0.917512\n",
            "Epoch 18, Loss: 0.902319\n",
            "Epoch 19, Loss: 0.882958\n",
            "Epoch 20, Loss: 0.845640\n",
            "Shadow model 1 trained.\n",
            "Epoch 1, Loss: 1.971920\n",
            "Epoch 2, Loss: 1.684230\n",
            "Epoch 3, Loss: 1.578052\n",
            "Epoch 4, Loss: 1.496143\n",
            "Epoch 5, Loss: 1.424258\n",
            "Epoch 6, Loss: 1.362241\n",
            "Epoch 7, Loss: 1.310539\n",
            "Epoch 8, Loss: 1.260077\n",
            "Epoch 9, Loss: 1.229977\n",
            "Epoch 10, Loss: 1.179939\n",
            "Epoch 11, Loss: 1.144884\n",
            "Epoch 12, Loss: 1.101795\n",
            "Epoch 13, Loss: 1.083806\n",
            "Epoch 14, Loss: 1.040788\n",
            "Epoch 15, Loss: 1.011365\n",
            "Epoch 16, Loss: 0.992831\n",
            "Epoch 17, Loss: 0.956410\n",
            "Epoch 18, Loss: 0.937298\n",
            "Epoch 19, Loss: 0.901061\n",
            "Epoch 20, Loss: 0.881776\n",
            "Shadow model 2 trained.\n",
            "Epoch 1, Loss: 1.915023\n",
            "Epoch 2, Loss: 1.618026\n",
            "Epoch 3, Loss: 1.514719\n",
            "Epoch 4, Loss: 1.417507\n",
            "Epoch 5, Loss: 1.332357\n",
            "Epoch 6, Loss: 1.272699\n",
            "Epoch 7, Loss: 1.210948\n",
            "Epoch 8, Loss: 1.180894\n",
            "Epoch 9, Loss: 1.121752\n",
            "Epoch 10, Loss: 1.082327\n",
            "Epoch 11, Loss: 1.044341\n",
            "Epoch 12, Loss: 1.009139\n",
            "Epoch 13, Loss: 0.988599\n",
            "Epoch 14, Loss: 0.949273\n",
            "Epoch 15, Loss: 0.911027\n",
            "Epoch 16, Loss: 0.874113\n",
            "Epoch 17, Loss: 0.863262\n",
            "Epoch 18, Loss: 0.832996\n",
            "Epoch 19, Loss: 0.817126\n",
            "Epoch 20, Loss: 0.803732\n",
            "Shadow model 3 trained.\n",
            "Class 0, Training Accuracy: 0.9998\n",
            "Class 0, Training Precision: 0.9997\n",
            "Class 0, Training Recall: 1.0000\n",
            "Class 0, Training F1 Score: 0.9999\n",
            "Class 0, Training Confusion Matrix:\n",
            " [[2050    1]\n",
            " [   0 3949]]\n",
            "Attack model for class 0 trained.\n",
            "Class 1, Training Accuracy: 0.9993\n",
            "Class 1, Training Precision: 0.9990\n",
            "Class 1, Training Recall: 1.0000\n",
            "Class 1, Training F1 Score: 0.9995\n",
            "Class 1, Training Confusion Matrix:\n",
            " [[1935    4]\n",
            " [   0 4060]]\n",
            "Attack model for class 1 trained.\n",
            "Class 2, Training Accuracy: 0.9995\n",
            "Class 2, Training Precision: 0.9995\n",
            "Class 2, Training Recall: 0.9998\n",
            "Class 2, Training F1 Score: 0.9996\n",
            "Class 2, Training Confusion Matrix:\n",
            " [[1978    2]\n",
            " [   1 4019]]\n",
            "Attack model for class 2 trained.\n",
            "Class 3, Training Accuracy: 0.9995\n",
            "Class 3, Training Precision: 0.9998\n",
            "Class 3, Training Recall: 0.9995\n",
            "Class 3, Training F1 Score: 0.9996\n",
            "Class 3, Training Confusion Matrix:\n",
            " [[1989    1]\n",
            " [   2 4008]]\n",
            "Attack model for class 3 trained.\n",
            "Class 4, Training Accuracy: 0.9997\n",
            "Class 4, Training Precision: 0.9995\n",
            "Class 4, Training Recall: 1.0000\n",
            "Class 4, Training F1 Score: 0.9998\n",
            "Class 4, Training Confusion Matrix:\n",
            " [[1938    2]\n",
            " [   0 4060]]\n",
            "Attack model for class 4 trained.\n",
            "Class 5, Training Accuracy: 0.9998\n",
            "Class 5, Training Precision: 1.0000\n",
            "Class 5, Training Recall: 0.9997\n",
            "Class 5, Training F1 Score: 0.9999\n",
            "Class 5, Training Confusion Matrix:\n",
            " [[2079    0]\n",
            " [   1 3920]]\n",
            "Attack model for class 5 trained.\n",
            "Class 6, Training Accuracy: 1.0000\n",
            "Class 6, Training Precision: 1.0000\n",
            "Class 6, Training Recall: 1.0000\n",
            "Class 6, Training F1 Score: 1.0000\n",
            "Class 6, Training Confusion Matrix:\n",
            " [[2003    0]\n",
            " [   0 3996]]\n",
            "Attack model for class 6 trained.\n",
            "Class 7, Training Accuracy: 0.9993\n",
            "Class 7, Training Precision: 0.9990\n",
            "Class 7, Training Recall: 1.0000\n",
            "Class 7, Training F1 Score: 0.9995\n",
            "Class 7, Training Confusion Matrix:\n",
            " [[2016    4]\n",
            " [   0 3979]]\n",
            "Attack model for class 7 trained.\n",
            "Class 8, Training Accuracy: 0.9997\n",
            "Class 8, Training Precision: 0.9995\n",
            "Class 8, Training Recall: 1.0000\n",
            "Class 8, Training F1 Score: 0.9997\n",
            "Class 8, Training Confusion Matrix:\n",
            " [[2021    2]\n",
            " [   0 3977]]\n",
            "Attack model for class 8 trained.\n",
            "Class 9, Training Accuracy: 0.9997\n",
            "Class 9, Training Precision: 0.9995\n",
            "Class 9, Training Recall: 1.0000\n",
            "Class 9, Training F1 Score: 0.9998\n",
            "Class 9, Training Confusion Matrix:\n",
            " [[1971    2]\n",
            " [   0 4027]]\n",
            "Attack model for class 9 trained.\n",
            "Attack model for class 0 saved to attack_model_class_0.joblib\n",
            "Attack model for class 1 saved to attack_model_class_1.joblib\n",
            "Attack model for class 2 saved to attack_model_class_2.joblib\n",
            "Attack model for class 3 saved to attack_model_class_3.joblib\n",
            "Attack model for class 4 saved to attack_model_class_4.joblib\n",
            "Attack model for class 5 saved to attack_model_class_5.joblib\n",
            "Attack model for class 6 saved to attack_model_class_6.joblib\n",
            "Attack model for class 7 saved to attack_model_class_7.joblib\n",
            "Attack model for class 8 saved to attack_model_class_8.joblib\n",
            "Attack model for class 9 saved to attack_model_class_9.joblib\n",
            "Attack model for class 0 loaded from attack_model_class_0.joblib\n",
            "Attack model for class 1 loaded from attack_model_class_1.joblib\n",
            "Attack model for class 2 loaded from attack_model_class_2.joblib\n",
            "Attack model for class 3 loaded from attack_model_class_3.joblib\n",
            "Attack model for class 4 loaded from attack_model_class_4.joblib\n",
            "Attack model for class 5 loaded from attack_model_class_5.joblib\n",
            "Attack model for class 6 loaded from attack_model_class_6.joblib\n",
            "Attack model for class 7 loaded from attack_model_class_7.joblib\n",
            "Attack model for class 8 loaded from attack_model_class_8.joblib\n",
            "Attack model for class 9 loaded from attack_model_class_9.joblib\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Accuracies"
      ],
      "metadata": {
        "id": "ruW8fZx5CUBz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_base = CIFAR10Classifier()\n",
        "model_base.load_state_dict(torch.load('baseline_model.pth', map_location=device))\n",
        "model_base.to(device)\n",
        "\n",
        "accuracy_baseline = mia.evaluate_attack_model(train_loader_baseline, test_loader_baseline, model_base)\n",
        "print(f'Accuracy for Attacking to the Baseline Model :  {accuracy_baseline * 100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_13JkAxCPvF",
        "outputId": "27cd9241-1b59-4e99-9d90-8ff8c92ddb3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for Attacking to the Baseline Model :  71.41%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_module_prefix(state_dict):\n",
        "    new_state_dict = {}\n",
        "    for k, v in state_dict.items():\n",
        "        if k.startswith('_module.'):\n",
        "            new_state_dict[k[8:]] = v  # remove '_module.' prefix\n",
        "        else:\n",
        "            new_state_dict[k] = v\n",
        "    return new_state_dict\n",
        "\n",
        "model_private = CIFAR10Classifier().to(device)\n",
        "private_state_dict = torch.load('modified_model.pth', map_location=device)\n",
        "model_private.load_state_dict(remove_module_prefix(private_state_dict))\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model_private.to(device)\n",
        "\n",
        "accuracy_private = mia.evaluate_attack_model(train_loader_modified, test_loader_modified, model_private)\n",
        "print(f'Accuracy for Attacking to the Private Model :  {accuracy_private * 100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2C01PimCQpq",
        "outputId": "7b12c5cd-4990-4120-8298-792309424ad5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for Attacking to the Private Model :  55.62%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Num of Shadow Models = 2 , Batch Size = 64 , Epochs = 15"
      ],
      "metadata": {
        "id": "svXhd6gBDzLH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
        "train_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Split training data into 80% and 20%\n",
        "train_size = int(0.8 * len(train_set))\n",
        "remaining_size = len(train_set) - train_size\n",
        "train_subset, remaining_subset = torch.utils.data.random_split(train_set, [train_size, remaining_size])\n",
        "\n",
        "# Create disjoint loaders for seen data from 80% training data\n",
        "num_shadow_models = 2\n",
        "seen_size_per_model = train_size // num_shadow_models\n",
        "seen_loaders = []\n",
        "\n",
        "for i in range(num_shadow_models):\n",
        "    start_idx = i * seen_size_per_model\n",
        "    end_idx = (i + 1) * seen_size_per_model\n",
        "    seen_indices = torch.arange(start_idx, end_idx)\n",
        "    seen_train_set = Subset(train_subset, seen_indices)\n",
        "    seen_loader = DataLoader(seen_train_set, batch_size=64, shuffle=True)\n",
        "    seen_loaders.append(seen_loader)\n",
        "\n",
        "# Create concatenated unseen data from the remaining 20% of training data and the entire test set\n",
        "unseen_dataset = ConcatDataset([remaining_subset, test_set])\n",
        "unseen_size_per_model = len(unseen_dataset) // num_shadow_models\n",
        "unseen_loaders = []\n",
        "\n",
        "for i in range(num_shadow_models):\n",
        "    start_idx = i * unseen_size_per_model\n",
        "    end_idx = (i + 1) * unseen_size_per_model\n",
        "    unseen_indices = torch.arange(start_idx, end_idx)\n",
        "    unseen_subset = Subset(unseen_dataset, unseen_indices)\n",
        "    unseen_loader = DataLoader(unseen_subset, batch_size=64, shuffle=False)\n",
        "    unseen_loaders.append(unseen_loader)\n",
        "\n",
        "test_loader = DataLoader(test_set, batch_size=64, shuffle=False)\n",
        "\n",
        "# Initialize MembershipInferenceAttackNoPrivacy\n",
        "mia = MembershipInferenceAttackNoPrivacy(CIFAR10Classifier, device)\n",
        "\n",
        "# Train shadow models without differential privacy\n",
        "mia.train_shadow_models(seen_loaders, num_epochs=15)\n",
        "\n",
        "# Collect outputs for attack model\n",
        "mia.collect_outputs(seen_loaders, unseen_loaders)\n",
        "\n",
        "# Train attack models\n",
        "mia.train_attack_models()\n",
        "\n",
        "# Save the attack models\n",
        "mia.save_attack_models('attack_model')\n",
        "\n",
        "# Load the attack models (for future use)\n",
        "mia.load_attack_models('attack_model')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7Vxef11DytL",
        "outputId": "966b1f91-331f-4681-925b-30b642d58965"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch 1, Loss: 1.827725\n",
            "Epoch 2, Loss: 1.576761\n",
            "Epoch 3, Loss: 1.465654\n",
            "Epoch 4, Loss: 1.385781\n",
            "Epoch 5, Loss: 1.321715\n",
            "Epoch 6, Loss: 1.274589\n",
            "Epoch 7, Loss: 1.220363\n",
            "Epoch 8, Loss: 1.183660\n",
            "Epoch 9, Loss: 1.144996\n",
            "Epoch 10, Loss: 1.105306\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Accuracies"
      ],
      "metadata": {
        "id": "6aLZ86tZEAzy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_base = CIFAR10Classifier()\n",
        "model_base.load_state_dict(torch.load('baseline_model.pth', map_location=device))\n",
        "model_base.to(device)\n",
        "\n",
        "accuracy_baseline = mia.evaluate_attack_model(train_loader_baseline, test_loader_baseline, model_base)\n",
        "print(f'Accuracy for Attacking to the Baseline Model :  {accuracy_baseline * 100:.2f}%')"
      ],
      "metadata": {
        "id": "IuAf5cg3EA85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_module_prefix(state_dict):\n",
        "    new_state_dict = {}\n",
        "    for k, v in state_dict.items():\n",
        "        if k.startswith('_module.'):\n",
        "            new_state_dict[k[8:]] = v  # remove '_module.' prefix\n",
        "        else:\n",
        "            new_state_dict[k] = v\n",
        "    return new_state_dict\n",
        "\n",
        "model_private = CIFAR10Classifier().to(device)\n",
        "private_state_dict = torch.load('modified_model.pth', map_location=device)\n",
        "model_private.load_state_dict(remove_module_prefix(private_state_dict))\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model_private.to(device)\n",
        "\n",
        "accuracy_private = mia.evaluate_attack_model(train_loader_modified, test_loader_modified, model_private)\n",
        "print(f'Accuracy for Attacking to the Private Model :  {accuracy_private * 100:.2f}%')"
      ],
      "metadata": {
        "id": "y9o9rW2dEDhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Max Depth"
      ],
      "metadata": {
        "id": "g_dtF-7OL72a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MembershipInferenceAttackNoPrivacy:\n",
        "    def __init__(self, shadow_model_class, device='cpu'):\n",
        "        self.shadow_model_class = shadow_model_class\n",
        "        self.device = device\n",
        "        self.attack_models = {}\n",
        "\n",
        "    def train_shadow_models(self, seen_loaders, num_epochs=10, lr=1e-3):\n",
        "        self.shadow_models = [self.shadow_model_class().to(self.device) for _ in range(len(seen_loaders))]\n",
        "\n",
        "        for i, (shadow_model, seen_loader) in enumerate(zip(self.shadow_models, seen_loaders)):\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "            optimizer = optim.Adam(shadow_model.parameters(), lr=lr)\n",
        "            self.train_model(shadow_model, seen_loader, criterion, optimizer, num_epochs)\n",
        "            print(f'Shadow model {i+1} trained.')\n",
        "\n",
        "    def train_model(self, model, dataloader, criterion, optimizer, num_epochs):\n",
        "        model.train()\n",
        "        for epoch in range(num_epochs):\n",
        "            running_loss = 0.0\n",
        "            for inputs, labels in dataloader:\n",
        "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                running_loss += loss.item()\n",
        "            print(f'Epoch {epoch+1}, Loss: {running_loss/len(dataloader):.6f}')\n",
        "\n",
        "    def collect_outputs(self, seen_loaders, unseen_loaders):\n",
        "        self.attack_data = []\n",
        "        self.attack_labels = []\n",
        "\n",
        "        for shadow_model, seen_loader, unseen_loader in zip(self.shadow_models, seen_loaders, unseen_loaders):\n",
        "            self.collect_shadow_model_outputs(shadow_model, seen_loader, label=1)  # in\n",
        "            self.collect_shadow_model_outputs(shadow_model, unseen_loader, label=0)  # out\n",
        "\n",
        "        self.attack_data = torch.cat(self.attack_data).cpu().numpy()\n",
        "        self.attack_labels = torch.cat(self.attack_labels).cpu().numpy()\n",
        "\n",
        "    def collect_shadow_model_outputs(self, model, dataloader, label):\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in dataloader:\n",
        "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
        "                outputs = model(inputs)\n",
        "                probabilities = F.softmax(outputs, dim=1)\n",
        "                self.attack_data.append(torch.cat([probabilities, labels.unsqueeze(1).float()], dim=1))\n",
        "                self.attack_labels.append(torch.full((outputs.size(0),), label, dtype=torch.float))\n",
        "\n",
        "    def train_attack_models(self):\n",
        "        for class_label in range(10):  # Assuming 10 classes\n",
        "            class_indices = (self.attack_data[:, -1] == class_label)\n",
        "            class_data = self.attack_data[class_indices][:, :-1]  # Exclude the last column (class label)\n",
        "            class_labels = self.attack_labels[class_indices]  # Binary labels (in or out)\n",
        "\n",
        "            attack_model = RandomForestClassifier( n_estimators=50,  # Number of trees\n",
        "                                                   max_depth=12,     # Maximum depth of each tree\n",
        "                                                   random_state=42   # Setting a random state for reproducibility (optional)\n",
        "                                                   )\n",
        "\n",
        "            attack_model.fit(class_data, class_labels)\n",
        "\n",
        "            # Evaluate performance\n",
        "            train_predictions = attack_model.predict(class_data)\n",
        "            train_accuracy = accuracy_score(class_labels, train_predictions)\n",
        "            train_precision = precision_score(class_labels, train_predictions)\n",
        "            train_recall = recall_score(class_labels, train_predictions)\n",
        "            train_f1 = f1_score(class_labels, train_predictions)\n",
        "            train_confusion_matrix = confusion_matrix(class_labels, train_predictions)\n",
        "\n",
        "            print(f'Class {class_label}, Training Accuracy: {train_accuracy:.4f}')\n",
        "            print(f'Class {class_label}, Training Precision: {train_precision:.4f}')\n",
        "            print(f'Class {class_label}, Training Recall: {train_recall:.4f}')\n",
        "            print(f'Class {class_label}, Training F1 Score: {train_f1:.4f}')\n",
        "            print(f'Class {class_label}, Training Confusion Matrix:\\n {train_confusion_matrix}')\n",
        "\n",
        "            self.attack_models[class_label] = attack_model\n",
        "            print(f'Attack model for class {class_label} trained.')\n",
        "\n",
        "    def save_attack_models(self, path):\n",
        "        for class_label, model in self.attack_models.items():\n",
        "            joblib.dump(model, f'{path}_class_{class_label}.joblib')\n",
        "            print(f'Attack model for class {class_label} saved to {path}_class_{class_label}.joblib')\n",
        "\n",
        "    def load_attack_models(self, path):\n",
        "        for class_label in range(10):  # Assuming 10 classes\n",
        "            model = joblib.load(f'{path}_class_{class_label}.joblib')\n",
        "            self.attack_models[class_label] = model\n",
        "            print(f'Attack model for class {class_label} loaded from {path}_class_{class_label}.joblib')\n",
        "\n",
        "    def infer_membership(self, model, seen_loader, unseen_loader, seen_outputs, unseen_outputs, labels):\n",
        "        model_outputs = torch.cat([seen_outputs, unseen_outputs]).cpu().numpy()\n",
        "        labels = labels.cpu().numpy()\n",
        "\n",
        "        # print(\"In Infer : \")\n",
        "        # print(f\"model_outputs size: {model_outputs.shape}\")\n",
        "        # print(f\"labels size: {len(labels)}\")\n",
        "\n",
        "        memberships = []\n",
        "        for output, label in zip(model_outputs, labels):\n",
        "            class_label = int(label)\n",
        "            attack_model = self.attack_models[class_label]\n",
        "            membership_pred = attack_model.predict(output.reshape(1, -1))[0]\n",
        "            memberships.append(membership_pred)\n",
        "\n",
        "        # print(f\"memberships size: {len(memberships)}\")\n",
        "        return torch.tensor(memberships, device=self.device)\n",
        "\n",
        "    def evaluate_attack_model(self, seen_loader, unseen_loader, target_model):\n",
        "        seen_outputs, labels_seen = self.get_model_outputs(target_model, seen_loader)\n",
        "        unseen_outputs, labels_unseen = self.get_model_outputs(target_model, unseen_loader)\n",
        "\n",
        "        # print(f\"Seen outputs size: {seen_outputs.size()}\")\n",
        "        # print(f\"Unseen outputs size: {unseen_outputs.size()}\")\n",
        "\n",
        "        attack_data = torch.cat([seen_outputs, unseen_outputs]).to(self.device)\n",
        "        attack_labels = torch.cat([torch.ones(len(seen_outputs)), torch.zeros(len(unseen_outputs))]).to(self.device)\n",
        "        labels = torch.cat([labels_seen, labels_unseen]).to(self.device)\n",
        "\n",
        "        # print(f\"Attack data size: {attack_data.size()}\")\n",
        "        # print(f\"Attack labels size: {attack_labels.size()}\")\n",
        "\n",
        "        memberships = self.infer_membership(target_model, seen_loader, unseen_loader, seen_outputs, unseen_outputs, labels)\n",
        "        membership_preds = torch.tensor(memberships).float()\n",
        "        accuracy = (membership_preds == attack_labels).float().mean().item()\n",
        "        return accuracy\n",
        "\n",
        "    def get_model_outputs(self, model, dataloader):\n",
        "        model.eval()\n",
        "        outputs_list = []\n",
        "        labels_list = []\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in dataloader:\n",
        "                inputs = inputs.to(self.device)\n",
        "                outputs = model(inputs)\n",
        "                probabilities = F.softmax(outputs, dim=1)\n",
        "                outputs_list.append(probabilities)\n",
        "                labels_list.append(labels)\n",
        "        return torch.cat(outputs_list), torch.cat(labels_list)\n"
      ],
      "metadata": {
        "id": "ri999CVuL-DB"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
        "train_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Split training data into 80% and 20%\n",
        "train_size = int(0.8 * len(train_set))\n",
        "remaining_size = len(train_set) - train_size\n",
        "train_subset, remaining_subset = torch.utils.data.random_split(train_set, [train_size, remaining_size])\n",
        "\n",
        "# Create disjoint loaders for seen data from 80% training data\n",
        "num_shadow_models = 5\n",
        "seen_size_per_model = train_size // num_shadow_models\n",
        "seen_loaders = []\n",
        "\n",
        "for i in range(num_shadow_models):\n",
        "    start_idx = i * seen_size_per_model\n",
        "    end_idx = (i + 1) * seen_size_per_model\n",
        "    seen_indices = torch.arange(start_idx, end_idx)\n",
        "    seen_train_set = Subset(train_subset, seen_indices)\n",
        "    seen_loader = DataLoader(seen_train_set, batch_size=64, shuffle=True)\n",
        "    seen_loaders.append(seen_loader)\n",
        "\n",
        "# Create concatenated unseen data from the remaining 20% of training data and the entire test set\n",
        "unseen_dataset = ConcatDataset([remaining_subset, test_set])\n",
        "unseen_size_per_model = len(unseen_dataset) // num_shadow_models\n",
        "unseen_loaders = []\n",
        "\n",
        "for i in range(num_shadow_models):\n",
        "    start_idx = i * unseen_size_per_model\n",
        "    end_idx = (i + 1) * unseen_size_per_model\n",
        "    unseen_indices = torch.arange(start_idx, end_idx)\n",
        "    unseen_subset = Subset(unseen_dataset, unseen_indices)\n",
        "    unseen_loader = DataLoader(unseen_subset, batch_size=64, shuffle=False)\n",
        "    unseen_loaders.append(unseen_loader)\n",
        "\n",
        "test_loader = DataLoader(test_set, batch_size=64, shuffle=False)\n",
        "\n",
        "# Initialize MembershipInferenceAttackNoPrivacy\n",
        "mia = MembershipInferenceAttackNoPrivacy(CIFAR10Classifier, device)\n",
        "\n",
        "# Train shadow models without differential privacy\n",
        "mia.train_shadow_models(seen_loaders, num_epochs=15)\n",
        "\n",
        "# Collect outputs for attack model\n",
        "mia.collect_outputs(seen_loaders, unseen_loaders)\n",
        "\n",
        "# Train attack models\n",
        "mia.train_attack_models()\n",
        "\n",
        "# Save the attack models\n",
        "mia.save_attack_models('attack_model')\n",
        "\n",
        "# Load the attack models (for future use)\n",
        "mia.load_attack_models('attack_model')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8ZbmGJpMIc6",
        "outputId": "beb650bf-9d59-4e24-b298-9d67b21e6484"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch 1, Loss: 2.045591\n",
            "Epoch 2, Loss: 1.743839\n",
            "Epoch 3, Loss: 1.622105\n",
            "Epoch 4, Loss: 1.521956\n",
            "Epoch 5, Loss: 1.447738\n",
            "Epoch 6, Loss: 1.365872\n",
            "Epoch 7, Loss: 1.309938\n",
            "Epoch 8, Loss: 1.258959\n",
            "Epoch 9, Loss: 1.200750\n",
            "Epoch 10, Loss: 1.153068\n",
            "Epoch 11, Loss: 1.118268\n",
            "Epoch 12, Loss: 1.068444\n",
            "Epoch 13, Loss: 1.032273\n",
            "Epoch 14, Loss: 0.997757\n",
            "Epoch 15, Loss: 0.963482\n",
            "Shadow model 1 trained.\n",
            "Epoch 1, Loss: 2.024667\n",
            "Epoch 2, Loss: 1.750804\n",
            "Epoch 3, Loss: 1.632855\n",
            "Epoch 4, Loss: 1.541699\n",
            "Epoch 5, Loss: 1.453559\n",
            "Epoch 6, Loss: 1.388812\n",
            "Epoch 7, Loss: 1.339692\n",
            "Epoch 8, Loss: 1.298659\n",
            "Epoch 9, Loss: 1.248379\n",
            "Epoch 10, Loss: 1.197547\n",
            "Epoch 11, Loss: 1.146959\n",
            "Epoch 12, Loss: 1.127301\n",
            "Epoch 13, Loss: 1.096213\n",
            "Epoch 14, Loss: 1.034080\n",
            "Epoch 15, Loss: 1.014374\n",
            "Shadow model 2 trained.\n",
            "Epoch 1, Loss: 2.056166\n",
            "Epoch 2, Loss: 1.795268\n",
            "Epoch 3, Loss: 1.688032\n",
            "Epoch 4, Loss: 1.618346\n",
            "Epoch 5, Loss: 1.533881\n",
            "Epoch 6, Loss: 1.482674\n",
            "Epoch 7, Loss: 1.425089\n",
            "Epoch 8, Loss: 1.366458\n",
            "Epoch 9, Loss: 1.317120\n",
            "Epoch 10, Loss: 1.280949\n",
            "Epoch 11, Loss: 1.236255\n",
            "Epoch 12, Loss: 1.205113\n",
            "Epoch 13, Loss: 1.162740\n",
            "Epoch 14, Loss: 1.133906\n",
            "Epoch 15, Loss: 1.096796\n",
            "Shadow model 3 trained.\n",
            "Epoch 1, Loss: 2.065464\n",
            "Epoch 2, Loss: 1.812290\n",
            "Epoch 3, Loss: 1.675625\n",
            "Epoch 4, Loss: 1.580308\n",
            "Epoch 5, Loss: 1.513722\n",
            "Epoch 6, Loss: 1.454067\n",
            "Epoch 7, Loss: 1.401819\n",
            "Epoch 8, Loss: 1.351828\n",
            "Epoch 9, Loss: 1.312486\n",
            "Epoch 10, Loss: 1.264182\n",
            "Epoch 11, Loss: 1.203002\n",
            "Epoch 12, Loss: 1.167186\n",
            "Epoch 13, Loss: 1.128042\n",
            "Epoch 14, Loss: 1.088711\n",
            "Epoch 15, Loss: 1.045768\n",
            "Shadow model 4 trained.\n",
            "Epoch 1, Loss: 2.010705\n",
            "Epoch 2, Loss: 1.708348\n",
            "Epoch 3, Loss: 1.570163\n",
            "Epoch 4, Loss: 1.483627\n",
            "Epoch 5, Loss: 1.414856\n",
            "Epoch 6, Loss: 1.331063\n",
            "Epoch 7, Loss: 1.274246\n",
            "Epoch 8, Loss: 1.221871\n",
            "Epoch 9, Loss: 1.172521\n",
            "Epoch 10, Loss: 1.119217\n",
            "Epoch 11, Loss: 1.062938\n",
            "Epoch 12, Loss: 1.021768\n",
            "Epoch 13, Loss: 0.998547\n",
            "Epoch 14, Loss: 0.957524\n",
            "Epoch 15, Loss: 0.919606\n",
            "Shadow model 5 trained.\n",
            "Class 0, Training Accuracy: 0.8152\n",
            "Class 0, Training Precision: 0.7835\n",
            "Class 0, Training Recall: 1.0000\n",
            "Class 0, Training F1 Score: 0.8786\n",
            "Class 0, Training Confusion Matrix:\n",
            " [[ 878 1109]\n",
            " [   0 4013]]\n",
            "Attack model for class 0 trained.\n",
            "Class 1, Training Accuracy: 0.8250\n",
            "Class 1, Training Precision: 0.7937\n",
            "Class 1, Training Recall: 0.9993\n",
            "Class 1, Training F1 Score: 0.8847\n",
            "Class 1, Training Confusion Matrix:\n",
            " [[ 923 1047]\n",
            " [   3 4027]]\n",
            "Attack model for class 1 trained.\n",
            "Class 2, Training Accuracy: 0.8475\n",
            "Class 2, Training Precision: 0.8145\n",
            "Class 2, Training Recall: 0.9949\n",
            "Class 2, Training F1 Score: 0.8957\n",
            "Class 2, Training Confusion Matrix:\n",
            " [[1155  895]\n",
            " [  20 3930]]\n",
            "Attack model for class 2 trained.\n",
            "Class 3, Training Accuracy: 0.8903\n",
            "Class 3, Training Precision: 0.8594\n",
            "Class 3, Training Recall: 0.9980\n",
            "Class 3, Training F1 Score: 0.9235\n",
            "Class 3, Training Confusion Matrix:\n",
            " [[1370  650]\n",
            " [   8 3972]]\n",
            "Attack model for class 3 trained.\n",
            "Class 4, Training Accuracy: 0.8580\n",
            "Class 4, Training Precision: 0.8225\n",
            "Class 4, Training Recall: 0.9995\n",
            "Class 4, Training F1 Score: 0.9024\n",
            "Class 4, Training Confusion Matrix:\n",
            " [[1208  850]\n",
            " [   2 3940]]\n",
            "Attack model for class 4 trained.\n",
            "Class 5, Training Accuracy: 0.8560\n",
            "Class 5, Training Precision: 0.8233\n",
            "Class 5, Training Recall: 0.9993\n",
            "Class 5, Training F1 Score: 0.9028\n",
            "Class 5, Training Confusion Matrix:\n",
            " [[1123  861]\n",
            " [   3 4013]]\n",
            "Attack model for class 5 trained.\n",
            "Class 6, Training Accuracy: 0.8393\n",
            "Class 6, Training Precision: 0.8067\n",
            "Class 6, Training Recall: 0.9995\n",
            "Class 6, Training F1 Score: 0.8928\n",
            "Class 6, Training Confusion Matrix:\n",
            " [[1021  962]\n",
            " [   2 4015]]\n",
            "Attack model for class 6 trained.\n",
            "Class 7, Training Accuracy: 0.8272\n",
            "Class 7, Training Precision: 0.7950\n",
            "Class 7, Training Recall: 0.9998\n",
            "Class 7, Training F1 Score: 0.8857\n",
            "Class 7, Training Confusion Matrix:\n",
            " [[ 945 1036]\n",
            " [   1 4018]]\n",
            "Attack model for class 7 trained.\n",
            "Class 8, Training Accuracy: 0.8147\n",
            "Class 8, Training Precision: 0.7836\n",
            "Class 8, Training Recall: 0.9998\n",
            "Class 8, Training F1 Score: 0.8786\n",
            "Class 8, Training Confusion Matrix:\n",
            " [[ 865 1111]\n",
            " [   1 4023]]\n",
            "Attack model for class 8 trained.\n",
            "Class 9, Training Accuracy: 0.8558\n",
            "Class 9, Training Precision: 0.8236\n",
            "Class 9, Training Recall: 0.9980\n",
            "Class 9, Training F1 Score: 0.9024\n",
            "Class 9, Training Confusion Matrix:\n",
            " [[1134  857]\n",
            " [   8 4001]]\n",
            "Attack model for class 9 trained.\n",
            "Attack model for class 0 saved to attack_model_class_0.joblib\n",
            "Attack model for class 1 saved to attack_model_class_1.joblib\n",
            "Attack model for class 2 saved to attack_model_class_2.joblib\n",
            "Attack model for class 3 saved to attack_model_class_3.joblib\n",
            "Attack model for class 4 saved to attack_model_class_4.joblib\n",
            "Attack model for class 5 saved to attack_model_class_5.joblib\n",
            "Attack model for class 6 saved to attack_model_class_6.joblib\n",
            "Attack model for class 7 saved to attack_model_class_7.joblib\n",
            "Attack model for class 8 saved to attack_model_class_8.joblib\n",
            "Attack model for class 9 saved to attack_model_class_9.joblib\n",
            "Attack model for class 0 loaded from attack_model_class_0.joblib\n",
            "Attack model for class 1 loaded from attack_model_class_1.joblib\n",
            "Attack model for class 2 loaded from attack_model_class_2.joblib\n",
            "Attack model for class 3 loaded from attack_model_class_3.joblib\n",
            "Attack model for class 4 loaded from attack_model_class_4.joblib\n",
            "Attack model for class 5 loaded from attack_model_class_5.joblib\n",
            "Attack model for class 6 loaded from attack_model_class_6.joblib\n",
            "Attack model for class 7 loaded from attack_model_class_7.joblib\n",
            "Attack model for class 8 loaded from attack_model_class_8.joblib\n",
            "Attack model for class 9 loaded from attack_model_class_9.joblib\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Accuracies"
      ],
      "metadata": {
        "id": "ZAPKL7SKOTbl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_base = CIFAR10Classifier()\n",
        "model_base.load_state_dict(torch.load('baseline_model.pth', map_location=device))\n",
        "model_base.to(device)\n",
        "\n",
        "accuracy_baseline = mia.evaluate_attack_model(train_loader_baseline, test_loader_baseline, model_base)\n",
        "print(f'Accuracy for Attacking to the Baseline Model :  {accuracy_baseline * 100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8W1eC2rbOV1X",
        "outputId": "13d8cd14-0672-4487-9ffe-a38731da533d"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for Attacking to the Baseline Model :  75.88%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_module_prefix(state_dict):\n",
        "    new_state_dict = {}\n",
        "    for k, v in state_dict.items():\n",
        "        if k.startswith('_module.'):\n",
        "            new_state_dict[k[8:]] = v  # remove '_module.' prefix\n",
        "        else:\n",
        "            new_state_dict[k] = v\n",
        "    return new_state_dict\n",
        "\n",
        "model_private = CIFAR10Classifier().to(device)\n",
        "private_state_dict = torch.load('modified_model.pth', map_location=device)\n",
        "model_private.load_state_dict(remove_module_prefix(private_state_dict))\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model_private.to(device)\n",
        "\n",
        "accuracy_private = mia.evaluate_attack_model(train_loader_modified, test_loader_modified, model_private)\n",
        "print(f'Accuracy for Attacking to the Private Model :  {accuracy_private * 100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BowLZI7OWmV",
        "outputId": "0d8684c7-a2b8-41c0-dde7-a5b1f17b4f7d"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for Attacking to the Private Model :  60.02%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attack Model with privacy"
      ],
      "metadata": {
        "id": "1IHLYg4KM0VZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opacus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "YWXPTmytONGC",
        "outputId": "4b0b64ff-0da5-44b8-98b6-a5639c62b33c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opacus in /usr/local/lib/python3.10/dist-packages (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.10/dist-packages (from opacus) (1.25.2)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.10/dist-packages (from opacus) (2.3.0+cu121)\n",
            "Requirement already satisfied: scipy>=1.2 in /usr/local/lib/python3.10/dist-packages (from opacus) (1.11.4)\n",
            "Requirement already satisfied: opt-einsum>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from opacus) (3.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0->opacus) (12.5.82)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0->opacus) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.0->opacus) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Subset, ConcatDataset\n",
        "from model import CIFAR10Classifier\n",
        "import torch.nn.functional as F\n",
        "from opacus import PrivacyEngine\n",
        "from opacus.utils.batch_memory_manager import BatchMemoryManager\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
        "\n",
        "class MembershipInferenceAttackWithDP:\n",
        "    def __init__(self, shadow_model_class, device='cpu'):\n",
        "        self.shadow_model_class = shadow_model_class\n",
        "        self.device = device\n",
        "        self.attack_models = {}\n",
        "\n",
        "    def train_shadow_models(self, seen_loaders, num_epochs=10, lr=1e-3, max_grad_norm=8, epsilon=50, delta=1e-5):\n",
        "        self.shadow_models = [self.shadow_model_class().to(self.device) for _ in range(len(seen_loaders))]\n",
        "\n",
        "        for i, (shadow_model, seen_loader) in enumerate(zip(self.shadow_models, seen_loaders)):\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "            optimizer = optim.RMSprop(shadow_model.parameters(), lr=lr)\n",
        "            privacy_engine = PrivacyEngine()\n",
        "            shadow_model, optimizer, seen_loader = privacy_engine.make_private_with_epsilon(\n",
        "                module=shadow_model,\n",
        "                optimizer=optimizer,\n",
        "                data_loader=seen_loader,\n",
        "                epochs=num_epochs,\n",
        "                target_epsilon=epsilon,\n",
        "                target_delta=delta,\n",
        "                max_grad_norm=max_grad_norm,\n",
        "            )\n",
        "            self._train_private_model(shadow_model, seen_loader, criterion, optimizer, privacy_engine, num_epochs)\n",
        "            print(f'Shadow model {i+1} trained.')\n",
        "\n",
        "    def _train_private_model(self, model, dataloader, criterion, optimizer, privacy_engine, num_epochs):\n",
        "        model.train()\n",
        "        for epoch in range(num_epochs):\n",
        "            running_loss = 0.0\n",
        "            top1_acc = []\n",
        "            with BatchMemoryManager(data_loader=dataloader, max_physical_batch_size=128, optimizer=optimizer) as memory_safe_data_loader:\n",
        "                for inputs, labels in memory_safe_data_loader:\n",
        "                    optimizer.zero_grad()\n",
        "                    inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
        "                    outputs = model(inputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    preds = np.argmax(outputs.detach().cpu().numpy(), axis=1)\n",
        "                    labels_np = labels.detach().cpu().numpy()\n",
        "                    acc = (preds == labels_np).mean()\n",
        "\n",
        "                    running_loss += loss.item()\n",
        "                    top1_acc.append(acc)\n",
        "\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                epsilon = privacy_engine.get_epsilon(1e-5)\n",
        "                print(f'Epoch {epoch+1}, Loss: {running_loss/len(dataloader):.6f}, ε: {epsilon:.2f}')\n",
        "\n",
        "    def collect_outputs(self, seen_loaders, unseen_loaders):\n",
        "        self.attack_data = []\n",
        "        self.attack_labels = []\n",
        "\n",
        "        for shadow_model, seen_loader, unseen_loader in zip(self.shadow_models, seen_loaders, unseen_loaders):\n",
        "            self._collect_shadow_model_outputs(shadow_model, seen_loader, label=1)  # in\n",
        "            self._collect_shadow_model_outputs(shadow_model, unseen_loader, label=0)  # out\n",
        "\n",
        "        self.attack_data = torch.cat(self.attack_data).to(self.device)\n",
        "        self.attack_labels = torch.cat(self.attack_labels).to(self.device)\n",
        "\n",
        "    def _collect_shadow_model_outputs(self, model, dataloader, label):\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in dataloader:\n",
        "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
        "                outputs = model(inputs)\n",
        "                probabilities = F.softmax(outputs, dim=1)\n",
        "                self.attack_data.append(torch.cat([probabilities, labels.unsqueeze(1).float()], dim=1))\n",
        "                self.attack_labels.append(torch.full((outputs.size(0),), label, dtype=torch.float).to(self.device))\n",
        "\n",
        "    def train_attack_models(self):\n",
        "        attack_dataset = torch.utils.data.TensorDataset(self.attack_data, self.attack_labels)\n",
        "        for class_label in range(10):  # Assuming 10 classes\n",
        "            class_indices = (self.attack_data[:, -1] == class_label).nonzero().squeeze()\n",
        "            class_data = self.attack_data[class_indices][:, :-1]  # Exclude the last column (class label)\n",
        "            class_labels = self.attack_labels[class_indices]  # Ensure labels are the same shape as outputs\n",
        "\n",
        "            attack_model = RandomForestClassifier(n_estimators=100)\n",
        "            attack_model.fit(class_data.cpu().numpy(), class_labels.cpu().numpy().ravel())\n",
        "\n",
        "            self.attack_models[class_label] = attack_model\n",
        "            print(f'Attack model for class {class_label} trained.')\n",
        "\n",
        "    def save_attack_models(self, path):\n",
        "        import joblib\n",
        "        for class_label, model in self.attack_models.items():\n",
        "            joblib.dump(model, f'{path}_class_{class_label}.pkl')\n",
        "            print(f'Attack model for class {class_label} saved to {path}_class_{class_label}.pkl')\n",
        "\n",
        "    def load_attack_models(self, path):\n",
        "        import joblib\n",
        "        for class_label in range(10):  # Assuming 10 classes\n",
        "            model = joblib.load(f'{path}_class_{class_label}.pkl')\n",
        "            self.attack_models[class_label] = model\n",
        "            print(f'Attack model for class {class_label} loaded from {path}_class_{class_label}.pkl')\n",
        "\n",
        "    def infer_membership(self, model, dataloader):\n",
        "        model_outputs, labels = self._get_model_outputs(model, dataloader)\n",
        "        memberships = []\n",
        "\n",
        "        for output, label in zip(model_outputs, labels):\n",
        "            class_label = label.item()\n",
        "            attack_model = self.attack_models[class_label]\n",
        "            membership_pred = attack_model.predict(output.unsqueeze(0).cpu().numpy())\n",
        "            memberships.append(membership_pred.item())\n",
        "\n",
        "        return torch.tensor(memberships, device=self.device)\n",
        "\n",
        "    def evaluate_attack_model(self, seen_loader, unseen_loader, target_model):\n",
        "        seen_outputs, labels_seen = self._get_model_outputs(target_model, seen_loader)\n",
        "        unseen_outputs, labels_unseen = self._get_model_outputs(target_model, unseen_loader)\n",
        "        labels = torch.cat([labels_seen, labels_unseen]).to(self.device)\n",
        "\n",
        "        attack_data = torch.cat([seen_outputs, unseen_outputs]).to(self.device)\n",
        "        attack_labels = torch.cat([torch.ones(len(seen_outputs)), torch.zeros(len(unseen_outputs))]).to(self.device)\n",
        "\n",
        "        memberships = self.infer_membership(target_model, DataLoader(ConcatDataset([seen_loader.dataset, unseen_loader.dataset]), batch_size=64))\n",
        "        membership_preds = (memberships > 0.5).float()\n",
        "        accuracy = (membership_preds == attack_labels).float().mean().item()\n",
        "        return accuracy\n",
        "\n",
        "    def _get_model_outputs(self, model, dataloader):\n",
        "        model.eval()\n",
        "        outputs_list = []\n",
        "        labels_list = []\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in dataloader:\n",
        "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
        "                outputs = model(inputs)\n",
        "                probabilities = F.softmax(outputs, dim=1)\n",
        "                outputs_list.append(probabilities)\n",
        "                labels_list.append(labels)\n",
        "        return torch.cat(outputs_list), torch.cat(labels_list)\n",
        "\n",
        "# Usage example\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
        "train_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Split training data into 80% and 20%\n",
        "train_size = int(0.8 * len(train_set))\n",
        "remaining_size = len(train_set) - train_size\n",
        "train_subset, remaining_subset = torch.utils.data.random_split(train_set, [train_size, remaining_size])\n",
        "\n",
        "# Create disjoint loaders for seen data from 80% training data\n",
        "num_shadow_models = 2\n",
        "seen_size_per_model = train_size // num_shadow_models\n",
        "seen_loaders = []\n",
        "\n",
        "for i in range(num_shadow_models):\n",
        "    start_idx = i * seen_size_per_model\n",
        "    end_idx = (i + 1) * seen_size_per_model\n",
        "    seen_indices = torch.arange(start_idx, end_idx)\n",
        "    seen_train_set = Subset(train_subset, seen_indices)\n",
        "    seen_loader = DataLoader(seen_train_set, batch_size=10, shuffle=True)\n",
        "    seen_loaders.append(seen_loader)\n",
        "\n",
        "# Create concatenated unseen data from the remaining 20% of training data and the entire test set\n",
        "unseen_dataset = ConcatDataset([remaining_subset, test_set])\n",
        "unseen_size_per_model = len(unseen_dataset) // num_shadow_models\n",
        "unseen_loaders = []\n",
        "\n",
        "for i in range(num_shadow_models):\n",
        "    start_idx = i * unseen_size_per_model\n",
        "    end_idx = (i + 1) * unseen_size_per_model\n",
        "    unseen_indices = torch.arange(start_idx, end_idx)\n",
        "    unseen_subset = Subset(unseen_dataset, unseen_indices)\n",
        "    unseen_loader = DataLoader(unseen_subset, batch_size=10, shuffle=False)\n",
        "    unseen_loaders.append(unseen_loader)\n",
        "\n",
        "test_loader = DataLoader(test_set, batch_size=10, shuffle=False)\n",
        "\n",
        "# Initialize MembershipInferenceAttackWithDP\n",
        "mia_private = MembershipInferenceAttackWithDP(CIFAR10Classifier, device)\n",
        "\n",
        "# Train shadow models with differential privacy\n",
        "mia_private.train_shadow_models(seen_loaders, num_epochs=10)\n",
        "\n",
        "# Collect outputs for attack model\n",
        "mia_private.collect_outputs(seen_loaders, unseen_loaders)\n",
        "\n",
        "# Train attack models\n",
        "mia_private.train_attack_models()\n",
        "\n",
        "# Save the attack models\n",
        "mia_private.save_attack_models('attack_model_private')\n",
        "\n",
        "# Load the attack models (for future use)\n",
        "mia_private.load_attack_models('attack_model_private')\n",
        "\n",
        "# Evaluate the attack model\n",
        "print(\"Accuracy for Attacking to the Private Model : \")\n",
        "accuracy_private = mia_private.evaluate_attack_model(test_loader, test_loader, mia_private.shadow_models[0])\n",
        "print(f'Accuracy: {accuracy_private}')\n"
      ],
      "metadata": {
        "id": "J8bbhtvhRBfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_module_prefix(state_dict):\n",
        "    new_state_dict = {}\n",
        "    for k, v in state_dict.items():\n",
        "        if k.startswith('_module.'):\n",
        "            new_state_dict[k[8:]] = v  # remove '_module.' prefix\n",
        "        else:\n",
        "            new_state_dict[k] = v\n",
        "    return new_state_dict\n",
        "\n",
        "model_private = CIFAR10Classifier().to(device)\n",
        "private_state_dict = torch.load('modified_model.pth', map_location=device)\n",
        "model_private.load_state_dict(remove_module_prefix(private_state_dict))\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model_private.to(device)\n",
        "\n",
        "accuracy_private = mia_private.evaluate_attack_model(train_loader_modified, test_loader_modified, model_private)\n",
        "print(f'Accuracy for Attacking to the Private Model :  {accuracy_private * 100:.2f}%')"
      ],
      "metadata": {
        "id": "C6V_63qKRLmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Presentation"
      ],
      "metadata": {
        "id": "GnKBFT-sV_r9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import models\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class CIFAR10Classifier(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CIFAR10Classifier, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(3, 16, 3, 1)\n",
        "    self.conv2 = nn.Conv2d(16, 32, 3, 1)\n",
        "    self.dropout1 = nn.Dropout2d(0.25)\n",
        "    self.dropout2 = nn.Dropout2d(0.5)\n",
        "    self.fc1 = nn.Linear(6272, 64)\n",
        "    self.fc2 = nn.Linear(64, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.conv2(x)\n",
        "    x = F.relu(x)\n",
        "    x = F.max_pool2d(x, 2)\n",
        "    x = self.dropout1(x)\n",
        "    x = torch.flatten(x, 1)\n",
        "    x = self.fc1(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.dropout2(x)\n",
        "    x = self.fc2(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "pNppxCWAfGmF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Subset, DataLoader, TensorDataset\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = CIFAR10Classifier()\n",
        "state_dict = torch.load(\"model_state_dict.pth\", map_location=device)\n",
        "new_state_dict = {key.replace('_module.', ''): value for key, value in state_dict.items()}\n",
        "model.load_state_dict(new_state_dict)\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "DATA_ROOT = '../cifar10'\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Load the indices from list.txt\n",
        "indices_file = 'list.txt'\n",
        "with open(indices_file, 'r') as f:\n",
        "    indices = [int(line.strip()) for line in f]\n",
        "\n",
        "full_train_dataset = CIFAR10(root=DATA_ROOT, train=True, download=True, transform=transform)\n",
        "test_dataset = CIFAR10(root=DATA_ROOT, train=False, download=True, transform=transform)\n",
        "\n",
        "train_indices_set = set(indices)\n",
        "all_indices = set(range(len(full_train_dataset)))\n",
        "other_indices = list(all_indices - train_indices_set)\n",
        "\n",
        "train_dataset = Subset(full_train_dataset, indices[:len(indices)//2])\n",
        "other_dataset = Subset(full_train_dataset, other_indices)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "other_loader = DataLoader(other_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# Create labels\n",
        "train_labels = torch.ones(len(train_dataset)).to(device)\n",
        "other_labels = torch.zeros(len(other_dataset)).to(device)\n",
        "test_labels = torch.zeros(len(test_dataset)).to(device)\n",
        "\n",
        "def extract_features_and_labels(model, dataloader, device):\n",
        "    model.eval()\n",
        "    features = []\n",
        "    labels = []\n",
        "    with torch.no_grad():\n",
        "        for data in dataloader:\n",
        "            inputs, targets = data\n",
        "            inputs = inputs.to(device)\n",
        "            outputs = model(inputs)\n",
        "            probabilities = F.softmax(outputs, dim=1)\n",
        "            features.append(probabilities.cpu())  # Keep features on CPU for concatenation\n",
        "            labels.append(targets.cpu())\n",
        "    return torch.cat(features), torch.cat(labels)\n",
        "\n",
        "\n",
        "train_features, train_labels = extract_features_and_labels(model, train_loader, device)\n",
        "other_features, other_labels = extract_features_and_labels(model, other_loader, device)\n",
        "test_features, test_labels = extract_features_and_labels(model, test_loader, device)\n",
        "\n",
        "combined_features = torch.cat((train_features, other_features, test_features))\n",
        "combined_labels = torch.cat((train_labels, other_labels, test_labels))\n",
        "combined_membership_labels = torch.cat((torch.ones(len(train_features)), torch.zeros(len(other_features)+ len(test_features) ))).to(device)\n",
        "\n",
        "new_dataset = TensorDataset(combined_features, combined_labels, combined_membership_labels)\n",
        "new_loader = DataLoader(new_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# attacker_models = {}\n",
        "# for class_label in range(10):\n",
        "#     attacker = LogisticRegression()  # or the attack model you trained\n",
        "#     attacker.load_state_dict(torch.load(f'attacker_model_class_{class_label}.pth', map_location=device))\n",
        "#     attacker_models[class_label] = attacker\n",
        "\n",
        "attacker_models = mia.attack_models\n",
        "\n",
        "# Calculate training accuracy, confusion matrix, precision, and recall\n",
        "all_labels = []\n",
        "all_predicted = []\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        " with torch.no_grad():\n",
        "    for features, labels, membership_labels in new_loader:\n",
        "        features, labels, membership_labels = features.to(device), labels.to(device), membership_labels.to(device)\n",
        "        for feature, label, membership_label in zip(features, labels, membership_labels):\n",
        "            class_label = int(label.item())\n",
        "            attacker = attacker_models[class_label]\n",
        "            output = attacker.predict(feature.unsqueeze(0).cpu().numpy())[0]\n",
        "            predicted = 1 if output > 0.5 else 0\n",
        "            total += 1\n",
        "            correct += (predicted == membership_label.item())\n",
        "            all_labels.append(membership_label.cpu().numpy())\n",
        "            all_predicted.append(predicted)\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f'Training Accuracy: {accuracy:.4f}')\n",
        "\n",
        "cm = confusion_matrix(all_labels, all_predicted)\n",
        "precision = precision_score(all_labels, all_predicted)\n",
        "recall = recall_score(all_labels, all_predicted)\n",
        "f1 = f1_score(all_labels, all_predicted)\n",
        "\n",
        "print(f'Confusion Matrix:\\n{cm}')\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")"
      ],
      "metadata": {
        "id": "xfOCx4yHfKLe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "go-xW3Q2V_u7"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wq8p7Ze_YnPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CapSNyWI2ga1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DbGSdTM92gdM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}